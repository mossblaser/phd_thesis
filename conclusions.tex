\chapter{Conclusions and future research}
	
	In this thesis I have explored the challenges posed by SpiNNaker's unusual
	topology, network architecture and scale. In this concluding chapter I begin
	by summarising the answers found to the research questions posed back in
	chapter~\ref{sec:introduction} before considering the implications of the
	work and the new avenues of research it has uncovered.
	
	\section{Answers to research questions}
		
		\textbf{1. Can the hexagonal torus topology be deployed and used in real,
		large scale systems?}
		
		In chapter~\ref{sec:building}, I introduced a cabling scheme and assembly
		technique which has been used successfully to build a prototype SpiNNaker
		system with over half a million processor cores using the hexagonal torus
		topology. The techniques shown are expected to scale up to machines of
		double this size, filling a six metre long row of machine room cabinets.
		
		Though SpiNNaker's processor count places it amongst some of the world's
		largest supercomputers (see figure \ref{fig:top500-num-processors} on page
		\pageref{fig:top500-num-processors}), it is comparatively compact,
		consuming a rows of cabinets compared with the warehouse-scale
		installations found in commercial systems. In spite of this, the folding
		and interleaving techniques described allow hexagonal torus topologies to
		scale to arbitrarily large installations without cable lengths increasing.
		
		Chapter~\ref{sec:shortestPaths} described an efficient and general purpose
		technique for finding, and enumerating shortest path vectors in hexagonal
		torus topologies. These improvements bring the hexagonal torus topology in
		line with other network topologies by enabling routing algorithms to
		exploit all possible paths in the network. Further
		chapter~\ref{sec:placement} demonstrated that placement algorithms are also
		easily adapted to hexagonal torus topologies thanks to their similarity to
		2D toruses.
		
		Though, as this thesis highlights, hexagonal toruses lack many of the
		intuitive properties enjoyed by other torus topologies, it is still
		possible to reason about it with only limited computational effort.
		Further, now that the practicality of scaling the topology up to large
		system sizes has been demonstrated in practice, this topology represents a
		credible option for future network architectures.
		
		\vspace*{1.0em}
		\noindent%
		\textbf{2. Does SpiNNaker's router architecture help, or hinder fault
		tolerance?}
		
		SpiNNaker's unconventional use of packet dropping to avoid deadlocks
		greatly simplifies its implementation, part of the motivation for its
		original use. In chapter~\ref{sec:routing} this feature is also used to the
		advantage of PGS repair, a post processing scheme used to add fault
		tolerance to SpiNNaker's existing routing algorithms. Compared with the
		often complex and wasteful methods used to tolerate faults in other
		networks, PGS repair incurs very little performance overhead in the
		presence of static faults.
		
		Routing table usage does increase in the presence of faults, however, which
		may be a concern for applications for applications which require many
		routing table entries. Routing table usage, as well as other overheads, was
		most significantly increased in the presence of contiguous groups of
		network faults. This results from the tendency of the PGS repair algorithm
		to produce routes which tightly skirt around the edges of these faults,
		resulting in higher concentrations of routing table entries.  Though the
		symptoms of this problem can be attributed to the design of SpiNNaker's
		multicast routing mechanism, is cause lies with the behaviour of the PGS
		repair algorithm. Potential improvements to the PGS repair algorithm which
		could reduce the quantity of routing table entries surrounding contiguous
		faults.
		
		The overall answer to this research question, therefore, is that the
		flexibility provided to routing algorithms in SpiNNaker's architecture is
		of great benefit, enabling arbitrary fault patterns to be inexpensively
		avoided.
		
		\vspace*{1.0em}
		\noindent%
		\textbf{3. How can the parts of a neural simulation be placed onto a large
		hexagonal torus topology to reduce network load?}
		
		In chapter~\ref{sec:placement}, I explored a number of contemporary
		approaches to the problem of placing applications with irregular
		communication patterns into network topologies. I observed that researchers
		working on circuit placement for chips and FPGAs are tackling very similar
		problems, and working at scales larger than those faced in application
		placement. I developed a simulated annealing based placement algorithm
		inspired by the techniques used in circuit placement, with specific
		adaptions for use in application placement and SpiNNaker's network
		topology.
		
		The simulated annealing based placement algorithm outperformed all of the
		pre-existing placement algorithms considered in terms of placement quality.
		In the case of one benchmark, simulated annealing based placement made it
		possible to run the neural model in real time for the first time. At larger
		scales, simulated annealing was also found to be able to produce good
		quality placements in benchmarks of over one million processes.
		
		The major shortcoming of simulated annealing based placement is its runtime
		cost. Though its runtime grows in proportion to the size of the problem,
		the implementation used took over 12~hours to place a problem for the
		largest planned SpiNNaker machine. Though this runtime is tractable,
		particularly given the relative output quality compared with the previous
		state-of-the-art, the algorithm is unlikely to scale comfortably as-is to
		larger problems.
		
		The conclusion to be drawn from this result, however, is not just that
		simulated annealing is a good solution for today's placement problems but
		rather that circuit placement techniques can be successfully adapted to
		fulfil this role. Since the placement problems faced by chip designers are
		growing at roughly the same exponential rate as the size of super
		computers, it is likely that as approaches are retired by chip placement
		researchers they will find new life as application placement algorithms.
	
	\subsection{Implications}
		
		One of the primary implications of these findings is that a number of the
		key practical challenges faced in scaling up the SpiNNaker architecture
		have been addressed. In particular, the development of an effective
		placement algorithm for SpiNNaker applications has been shown to be
		critical to some applications, in particular networks based on the Nengo
		modelling framework. The construction of larger SpiNNaker machines also
		paves the way for future large scale modelling work with more substantial
		models such as SPAUN currently under active development at the time of
		writing.
		
		Outside of the SpiNNaker project the hexagonal torus topology has also been
		validated as a scalable and practical candidate for future network
		architectures. As super computers become ever larger, the physical
		scalability afforded by the 2D nature of the hexagonal torus topology may
		become more appealing.
		
		Finally, my experiences with circuit placement techniques for application
		placement in neural simulations indicate that similar results might be
		obtained for other types of applications. Indeed, if this is the case,
		circuit placement may offer a long-term source of placement algorithms
		ready to handle the demands of future application placement problems.
		
	\section{Future research}
		
		During the undertaking of the research presented in this thesis, a number
		of new questions have been uncovered warranting future attention. In
		addition, though the goals of this study have largely been met, there are a
		some important limitations which future research may hope to address.
		
		\subsection{Cabinets in rows and columns}
			
			In chapter~\ref{sec:building} I developed and implemented a number of
			cabling schemes for the SpiNNaker architecture with the largest system
			spanning a six metre row of machine room cabinets -- a relatively small
			installation by conventional standards. In SpiNNaker, the cabling exists
			in a 2D plane (i.e. the face of the cabinets) but as the system is scaled
			up, the row of cabinets tends towards approximating a 1D line.
			
			\begin{figure}
				\center
				\buildfig{figures/multi-row-cabling.tex}
				
				\caption{Multiple rows of interconnected cabinets.}
				\label{fig:multi-row-cabling}
			\end{figure}
			
			In conventional large scale super computer installations, nodes are
			installed in multiple rows of cabinets as illustrated in
			figure~\ref{fig:multi-row-cabling}.  From a `birds-eye' view, the system
			approximates a 2D space, spread across floor of a machine room. This
			means that the same folding and interleaving techniques used in SpiNNaker
			are still applicable.
			
			Unfortunately for SpiNNaker, cables connecting between rows in this
			layout will be longer than the one metre limit imposed by its hardware
			because of the depth of the cabinets and spaces between them. Future
			SpiNNaker systems will need to consider alternative link technologies.
			For example, a hybrid system could be used in which inter-cabinet
			connections continue to use the current HSS link technology while
			inter-cabinet links use optical connections. This type of architecture
			could be supported by the use of pluggable `SFP+' transceiver modules
			\cite{sff01}.
		
		\subsection{Cabling assistance for other architectures}
			
			One of the secondary results of assembling the prototype SpiNNaker
			systems in chapter~\ref{sec:building} was the use of realtime guidance
			and feedback to assist cable installation. I am not aware of this
			technique being used in existing architectures and following the success
			experienced in this project, it is possible that the technique may be
			useful in conventional systems. Future work may wish to explore the
			possibility of expanding this technique to other super computer
			architectures and other large scale networks, such as data centres.
		
		\subsection{Congestion mitigation}
			
			\label{sec:wiggly-board-allocations}
			
			In chapter~\ref{sec:routing} I found that contiguous network faults cause
			hot-spots of congestion and routing table depletion where the PGS repair
			algorithm routes many paths tightly around the edges of faults.  However,
			it is not just faults which can cause contiguous blockages in the network
			topology. In reality, researchers do not always need a full-sized
			SpiNNaker system to perform their experiments. As a consequence, large
			SpiNNaker systems are soft-partitioned on demand into many smaller
			machines \cite{spalloc16}. To ensure isolation between partitioned
			sub-machines, HSS links between boards in different sub-machines are
			disabled. Because of SpiNNaker's `wrapped triple' partitioning scheme,
			the resulting sub-machines have hexagonal \emph{mesh} topologies (i.e.
			without wrap-around links) with irregular edges as in
			figure~\ref{fig:spalloc-mesh}.
			
			\begin{figure}
				\center
				\buildfig{figures/spalloc-mesh.tex}
				
				\caption{Irregular edges in a SpiNNaker system comprised of 24~boards
				partitioned from a larger machine by disabling board-to-board links.
				Each hexagon represents a SpiNNaker chip. No wrap-around connections
				are present.}
				\label{fig:spalloc-mesh}
			\end{figure}
			
			In these partitioned systems, the `tooth'-like gaps on the periphery of
			the network cause similar issues to the HSS link failures considered in
			chapter~\ref{fig:routing}. When a route is generated between nodes on
			opposite sides of a tooth, the PGS repair process will produce a
			shortest-path route around the tooth. Since many routes may be blocked by
			a single tooth a hot-spot will potentially be created around the tip of
			the tooth.
			
			In chapter~\ref{sec:placement}, the `CConv' benchmark application was
			found to run correctly the majority of the time when placed by the
			simulated annealing based placement algorithm but occasionally fail.
			Preliminary experiments suggest these occasional failures are caused by
			placement solutions which place heavily communicating parts of the
			application on opposite sides of teeth on the perimeter of the network.
			Future research should attempt to find methods to minimise these issues.
			
			\subsubsection{Avoiding hotspots with PGS repair}
				
				One possible approach to reducing network congestion around faults
				would be to force the PGS repair process to take more varied routes
				around faults. For example, in circuit routing algorithms, routers
				avoid congestion by adding an additional cost to routes which pass
				through congested areas \cite{kahng11}. A similar technique could be
				used in PGS repair.
			
			\subsubsection{Fault and irregularity aware placement}
				
				One of the shortcomings of the simulated annealing based placer
				developed in chapter~\ref{sec:placement} is that it does not account
				for network faults, or irregularities, in its placement solutions.
				Future work may further exploit techniques used in circuit placement
				where congestion-aware placement techniques \cite{viswanathan07} could
				be adapted to support application placement.
		
		\subsection{Improving placement performance}
			
			The simulated annealing based placer demonstrated in
			chapter~\ref{sec:placement} produced good quality placements but its
			execution time make it less useful beyond one million vertex placement
			problems. Future work should explore possibilities for improving the
			performance and scalability of this technique.
			
			In addition to considering alternative placement algorithms not based on
			simulated annealing there are a number of potential improvements which
			could be made to the placement algorithm proposed in this work.
			
			One possible approach is to attempt to reduce the application graph being
			placed. For example, graph clustering \cite{schaeffer07} may be used to
			group together vertices which are strongly connected which would then be
			placed as a single unit.  Unfortunately graph clustering can suffer from
			the same problems as graph partitioning based placement: vertices may be
			grouped together which in practice cannot be packed together into a given
			portion of a SpiNNaker machine.  Possible solutions to this problem
			include using a two-phase placement algorithm with a `global' phase and a
			`detailed' phase \cite{kahng11}. In the global placement phase, solutions
			are permitted which slightly over-allocate resources but which achieve
			good placement quality. In the detailed placement phase which follows,
			the solution is `legalised' by making small changes to the placement to
			ensure resources are not over allocated.
			
			An alternative approach well suited to SpiNNaker is to limit the
			clustering process to clusters which fit on a single SpiNNaker chip. In
			typical SpiNNaker application graphs, clustering to this level may reduce
			placement problem sizes by an order of magnitude, and consequently reduce
			runtimes by the same ratio. Preliminary experiments suggest that this
			approach may result in similar placement quality for very large placement
			problems while substantially reducing overall runtime.
		
		\subsection{Benchmarking}
			
			Finally, one of the most significant limitations of this study has been
			the lack of available large-scale SpiNNaker applications for use as
			benchmarks. As a consequence, much of the scalability experimentation
			performed in this thesis has relied on simple synthetic benchmarks based
			on projections of future SpiNNaker applications.
			
			In the short term, more sophisticated synthetic benchmark generation
			techniques used by the circuit placement community \cite{nam07} may offer
			an insight into alternative benchmarks in future work.
			
			In the longer term, it is hoped that with the availability of large
			SpiNNaker systems and placement and routing algorithms better suited to
			exploit them will lead to large scale applications being developed. These
			applications will hopefully lead to more interesting and representative
			benchmarks for use in future work.
	
	\section{Closing remarks}
		
		This thesis has explored and tackled a number of the challenges posed in
		scaling up the unconventional SpiNNaker architecture. Along the way I have
		demonstrated that the hexagonal torus topology may be a practical choice in
		future applications which can scale up to the physical dimensions expected
		of future super computers. I have also developed new efficient and
		effective methods of placing and routing neural simulation software on
		SpiNNaker which -- it is hoped -- will enable a new generation of large
		scale neural simulations on spinnaker.
		
		Although this work has fallen short of demonstrating truly large scale
		neuroscientific simulations running at the full scale of SpiNNaker's
		available hardware, a number smaller scale simulations have been made
		possible for the first time. With SpiNNaker's hardware architecture now
		operating at truly large scales, it is hoped that neural models will follow
		leading to a better understanding of the world's most important yet least
		understood computer: the brain.
