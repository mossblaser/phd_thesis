\chapter{Routing packets in large SpiNNaker machines}
	
	\label{sec:routing}
	
	So far, this thesis has focused on tackling the practical challenges
	resulting from SpiNNaker's hexagonal torus network topology. In this chapter,
	I adjust my focus towards the practical challenges resulting from SpiNNaker's
	large scale. Faults in large systems are inevitable and in the half-million
	core, \num{28800} chip SpiNNaker machine recently completed at the University
	of Manchester, around \SI{1}{\percent} of chips exhibited faults\footnote{Of
	the faulty chips discovered, the vast majority of faults are attributed to a
	currently unknown SDRAM failure}. These faults result in gaps and broken
	links in the network topology which routing algorithms must avoid in order to
	ensure correct system operation.
	
	In this chapter I tackle the problem of extending existing routing algorithms
	for SpiNNaker's network to enable them to route-around known, static faults.
	Though dynamic or transient faults may also occur, in this work such faults
	are ignored and other techniques, such as protocol-level fault tolerance, are
	relied on instead.
	
	Numerous heuristic-based fault-tolerant routing algorithms exist which target
	different network topologies and router architectures. Unfortunately as I
	will show, these algorithms are not portable and rely on or attempt to work
	around specific features of their target network architecture. In particular,
	existing work is dominated by the challenge of developing routing schemes
	which avoid or defuse network deadlocks. Due to SpiNNaker's unconventional
	use of timeout-based flow-control, it is not subject to the routing
	restrictions present in other architectures intended to cope with deadlocks.
	
	In this chapter I introduce a graph-search based post-processing step for
	non-fault-tolerant routing algorithms which guarantees routability in
	SpiNNaker systems without disconnected subregions. I also demonstrate that
	this technique introduces both negligible computational overhead to the
	routing algorithm runtime and resulting network performance.
	
	TODO: NOTE THE FAULT RATES ENCOUNTERED IN PRACTICE...
	
	\section{Related work}
		
		Existing work on routing in SpiNNaker's network has ignored the challenge
		of avoiding faults and instead focused on producing efficient multicast
		routes. As a result this section is broken into two halves. In the first
		half I survey the existing non-fault-tolerant approaches to routing used in
		SpiNNaker to-date. In the second I discuss the approaches to fault tolerant
		routing taken in other systems.
		
		\subsection{Multicast routing in SpiNNaker}
			
			Various fault-intolerant multicast routing algorithms exist for many
			networks and a number have been proposed and evaluated specifically in the
			context of SpiNNaker.
			
			In 2012, Davies \emph{et al.} evaluated the use of three common torus
			routing algorithms in SpiNNaker and found that simple oblivious routing is
			suitable for typical neural applications \cite{davies12}. The three
			routing techniques are:
			
			\begin{description}
				
				\item[Dimension Order Routing (DOR)] Packets are routed along each
				dimension (e.g. $X$, $Y$ and $Z$) in turn until no further hops are
				available in that direction.  The order in which the dimensions are
				traversed is fixed.
				
				\item[Right Turn Only Routing (RTOR)] As in DOR except the dimension
				order is chosen such that routes only contain right-turns.
				
				\item[Longest Dimension First Routing (LDFR)] As in DOR except the
				dimension order is chosen in descending order of number of hops in each
				dimension.
				
			\end{description}
			
			These unicast routing techniques are converted into a multicast routing
			algorithm by merging together the routes produced between the source node
			and each destination node as illustrated in figure
			\ref{fig:simple-routers}.
			
			\begin{figure}
				\center
				\begin{subfigure}{0.3\linewidth}
					\center
					\buildfig{figures/simple-routers-dor.tex}
					
					\caption{DOR}
					\label{fig:simple-routers-dor}
				\end{subfigure}
				\begin{subfigure}{0.3\linewidth}
					\center
					\buildfig{figures/simple-routers-rtor.tex}
					
					\caption{RTOR}
					\label{fig:simple-routers-dor}
				\end{subfigure}
				\begin{subfigure}{0.3\linewidth}
					\center
					\buildfig{figures/simple-routers-ldfr.tex}
					
					\caption{LDFR}
					\label{fig:simple-routers-dor}
				\end{subfigure}
				
				\caption{Example multicast routes produced by merging together unicast
				routes from a central source node to each destination node.}
				\label{fig:simple-routers}
			\end{figure}
			
			In 2014, Navaridas \emph{et al.} introduced two new algorithms, `Enhanced
			Shortest Path Routing' (ESPR) and `Neighbourhood Exploring Routing' (NER)
			which produce multicast routing trees with fewer total hops
			\cite{navaridas14}. In both algorithms, routes are generated sequentially
			for each of the destinations of a route using LDFR. Unlike LDFR, however,
			these algorithms search a limited area of the network for other,
			already-connected destination nodes to which LDFR routes may be
			constructed. If no suitable destination is found, a LDFR route is
			constructed to the source node. Figure \ref{fig:search-regions} illustrates
			the shape of the searched regions of each algorithm. ESPR searches the
			trapezoidal region between the source and destination nodes while NER
			searches a fixed radius out from the destination node.
			
			\begin{figure}
				\center
				\begin{subfigure}{0.45\linewidth}
					\center
					\buildfig{figures/search-regions-espr.tex}
					
					\caption{ESPR}
					\label{fig:search-regions-espr}
				\end{subfigure}
				\begin{subfigure}{0.45\linewidth}
					\center
					\buildfig{figures/search-regions-ner.tex}
					
					\caption{NER}
					\label{fig:search-regions-espr}
				\end{subfigure}
				
				\caption{The ESPR and NER algorithms attempt to connect the node marked
				`D' to the closest node in the shaded region which is connected to the
				source node, `S'. If no connected node is found in the shaded region, the
				LDFR route is taken to `S'. The dotted line indicates the route chosen
				from `D'.}
				\label{fig:search-regions}
			\end{figure}
			
			Unfortunately none of these routing algorithms make any allowance for the
			avoidance of network faults. As a result their utility in real-world
			systems is limited.
		
		\subsection{Fault-tolerant routing}
			
			Numerous fault-tolerant routing algorithms have been proposed for
			super-computer networks. These algorithms are largely constrained by the
			need to maintain deadlock freedom. Since SpiNNaker's routers employ a
			timeout based deadlock-breaking strategy, much of this effort is
			unnecessary in SpiNNaker. As described below, this frequently renders
			existing fault-tolerant routing algorithms unnecessarily complex and
			inflexible.
			
			Deadlocks occur in a network if a cyclic dependency exists on any limited
			resource in the network. For example, as illustrated in figure
			\ref{fig:ring-deadlock}, in a ring network a deadlock may form when every
			node is waiting on the next node to accept a packet before accepting new
			packets from the previous node.
			
			\begin{figure}
				\center
				\buildfig{figures/ring-deadlock.tex}
				
				\caption{A deadlock in a ring network where each node is waiting for
				the next to accept a packet before accepting any further packets.}
				\label{fig:ring-deadlock}
			\end{figure}
			
			To prevent deadlocks, combinations of router microarchitectural features
			and routing restrictions may be employed. For example, a simple
			deadlock-free routing algorithm for mesh and torus networks mandates the
			use of DOR \cite{dally93}. Packets travelling in a -ve direction along
			each axis take priority over those travelling in a +ve direction. Packets
			travelling along the Y axis take priority over those travelling along the
			X dimension. Given these rules it is possible to define a total ordering
			on all hops in the network. Figure \ref{fig:deadlock-free-dor}
			illustrates a $3\times3$ mesh network whose hops have been numbered
			according to the total ordering defined above.  Any `X-then-Y' DOR route
			through this network results in the use of hops labelled with strictly
			increasing numbers. As a result, no cyclic dependencies (and thus no
			deadlocks) may occur.
			
			\begin{figure}
				\center
				\buildfig{figures/deadlock-free-dor.tex}
			
				\caption{Deadlock-free routing of two example routes using DOR in a 2D
				mesh topology. The numbers of the hops taken by each route are given on
				the right.}
				\label{fig:deadlock-free-dor}
			\end{figure}
			
			Unfortunately, the routing restrictions imposed to ensure deadlock
			freedom can result in fault-intolerant routing. In the example above, if
			the node at the bottom-right corner of the figure was faulty, the dotted
			example route would be blocked as no alternative routes are allowed.
			
			In practice, the routing rules used may be more relaxed, for example
			requiring that any route whose length is equal to a DOR must exist to
			guarantee routability \cite{rodrigo09}.
			
			Alternative routing strategies take a hybrid approach whereby an
			efficient but fault-intollerant routing algorithm is used where possible
			and in the presence of faults a less efficient but more robust strategy
			is employed. For example, the Immucube network architecture employs three
			virtual networks which operate independently over the same physical links
			\cite{puente07}. Initially messages are routed using a high-performance
			but potentially-deadlockable routing scheme in the first virtual network.
			If a deadlock is occurs, the deadlocked packet is dropped into the second
			virtual network in which packets are routed using a less efficient but
			deadlock-free but fault-intolerant routing algorithm. Finally, upon
			encountering a fault, packets are dropped onto the third virtual network
			which forms a ring network routing packets to every node in the network.
			
			Releated approaches \cite{mejia06,boppana95} divide the network into
			regions in which different routing rules are enforced to ensure deadlock
			freedom and, when required, fault tolerance.
			
			TODO FIGURE?
			
			The BlueGene/L supercomputer \cite{adiga02} uses DOR for its torus
			network and implements fault-tolerance by sacrificing otherwise
			functioning `lamb' nodes to ensure no route passes through a known dead
			link \cite{ho04}. In figure \ref{fig:lamb-nodes} an example scenario is
			shown where a single dead node is present and all nodes in the same row
			or column as the dead node have been made into lamb nodes. The lamb nodes
			may not be used in an application except as a through-route for other
			traffic. This pattern of lamb nodes guarantees that all dimension-order
			routes between all pairs of non-lamb nodes are not obstructed by the
			faulty node. This approach trades use of higher performance routing
			logic for wasted resources. This type of approach is most appropriate
			when algorithmic routing is used and routing rules are inflexible.
			
			\begin{figure}
				\center
				\buildfig{figures/lamb-nodes.tex}
				
				\caption{`Lamb' nodes may be disabled to ensure DOR will never
				encounter a fault.}
				\label{fig:lamb-nodes}
			\end{figure}
			
			Other algorithms proposed for the BlueGene architecture attempt to avoid
			the need for lamb nodes by generating routes which reach their destination
			via a `proxy' node \cite{gomez04}. By appropriately selecting the location
			of such a proxy, the existing routing algorithm used by the system can be
			guaranteed to select a route free of faults.
			
			TODO: EXAMPLE OF PROXY ROUTING TO AVOID FAULT
			
			Finally, many algorithms in in the field are distributed and use only local
			information along with limited information from their peers to generate
			routes \cite{fick09b}. In SpiNNaker, route generation is conventionally
			carried out centrally since no special on-chip hardware facilities exist
			for route generation. Centralised route generation also enables the routing
			algorithm to consider all available routes. As a result, there is little
			incentive for the use of distributed routing algorithms on SpiNNaker since
			global system information could be compactly shared for one-off routing
			passes.
			
			Algorithms for other architectures such as IP networks tend to be poor fits
			for static, regular network topologies since they use expensive graph-based
			algorithms for route discovery which aren't necessary here. They also tend
			to heavily feature graph topology discovery etc. which aren't needed here.
			
			Work on fault-tolerance in data centre networks does exploit the regularity
			of the network topology in routing algorithms \cite{guo08,liao12}.
			Unfortunately, the approaches used are not general enough to be applied to
			mesh-like topologies such as the one in SpiNNaker.
			
			Outside the field of computer networks, routing algorithms used to route
			wires across the surfaces of chips are required to solve similar problems
			to fault-tolerant network routing problems in mesh networks. Like mesh
			networks, the routes are defined within a regular Manhattan geometry and
			congested areas, rather than faults must be avoided by the algorithms
			\cite{kahng11}.  Unfortunately, these algorithms are designed for
			occasional batch operation prior to the multi-month process of chip
			manufacturing and so runtimes of hours or days are commonplace
			\cite{nam08}. As such these algorithms would be inappropriate for use
			with applications such as SpiNNaker where users' applications tend to be
			short-lived and thus routing should not be allowed to dominate runtime.
	
	\section{Partial graph search repair}
		
		In this section I introduce a novel post-processing algorithm, Partial
		Graph Search (PGS) repair, for routes produced by non-fault-tolerant
		routing algorithms.
		
		PGS repair guarantees routability for networks with no disconnected
		subregions by using a graph search algorithm to route around faults in the
		original route.  General-purpose graph search algorithms such as Breadth
		First Search (BFS), Dijkstra's Algorithm and A* are guaranteed to find
		shortest-path routes between pairs of points in arbitrary graphs. Such
		algorithms are generally a poor choice in highly regular network topologies
		such as meshes and toruses due to their high computational cost. In PGS
		repair, graph searching is only used for \emph{part} of the routing
		problem: to repair gaps in routes generated by more efficient routing
		algorithms.
		
		Real world super computer architectures are designed to ensure that faults
		are isolated \cite{gara05,alverson12} and thus tend to only impact a
		localised region of the network. Since PGS repair is only needed to route
		around these isolated faults, the space searched by the graph search
		algorithm should be very small in practice resulting in only short
		runtimes. In addition since faults are rare in real-world systems, the
		graph search process will only rarely be invoked.
		
		The PGS repair post-processing technique starts with a route produced by a
		non-fault-tolerant routing algorithm such as ESPR or NER. If this route is
		not obstructed by a fault, the algorithm terminates immediately without
		modifying the route. If the route attempts to use a faulty link, the
		algorithm proceeds as follows.
		
		The routing tree produced by the underlying routing algorithm is broken
		into subtrees wherever it attempts to route through a broken link and
		each subtree is assigned a unique colour, as illustrated in figure
		\ref{fig:pgs-repair-colouring}. From each disconnected subtree's root
		node in turn, a graph search is performed to find a short, fault-free
		route to a subtree node of a different colour. The subtree is then
		attached to the tree discovered by the graph search and re-coloured to
		match the tree it is connected to.
		
		\begin{figure}
			\center
			\begin{subfigure}{0.32\linewidth}
				\hspace*{-1.5em}
				\buildfig{figures/pgs-repair-colouring.tex}
				
				\caption{}
				\label{fig:pgs-repair-colouring}
			\end{subfigure}
			\begin{subfigure}{0.32\linewidth}
				\hspace*{-1.5em}
				\buildfig{figures/pgs-repair-colouring-fix1.tex}
				
				\caption{}
				\label{fig:pgs-repair-colouring-fix1}
			\end{subfigure}
			\begin{subfigure}{0.32\linewidth}
				\hspace*{-1.5em}
				\buildfig{figures/pgs-repair-colouring-fix2.tex}
				
				\caption{}
				\label{fig:pgs-repair-colouring-fix2}
			\end{subfigure}
			
			\caption{PGS repair process example showing a disconnected multicast
			route from A to B, C, D, E and F. $\times$ indicates a broken link.}
			\label{fig:pgs-repair-colouring-steps}
		\end{figure}
		
		For example in figure \ref{fig:pgs-repair-colouring-fix1} a path from the
		root of the subtree containing nodes E and F is found which connects it to
		the subtree rooted at A. Similarly in figure
		\ref{fig:pgs-repair-colouring-fix2} a path is also found connecting the
		subtree containing nodes C and D back to the subtree rooted at node A.
		
		If the routing tree was broken into $N+1$ subtrees by faults there will be
		$N$ subtrees disconnected from the root node. Each of the $N$ iterations of
		the algorithm connect a disconnected subtree to another subtree reducing
		the number of subtrees by $1$ each time. After $N$ iterations, therefore,
		exactly $1$ subtree remains which connects every node in the original
		routing tree without traversing faulty links.
		
		TODO: EXPLAIN THE FIDDLINESS HERE TO ENSURE WE DON'T CREATE LOOPS.
		
	\section{Evaluation \& Results}
		
		The PGS repair technique, by design, is able to work around all possible
		fault patterns which don't completely disconnect parts of the network. This
		result this evaluation focuses on the impact on performance PGS repair
		imposes. The metrics of interest in this evaluation are:
		
		\begin{itemize}
			\item Algorithm runtime
			\item Network congestion
			\item Routing table utilisation
		\end{itemize}
		
		\subsection{Traffic Patterns}
			
			In this evaluation, two standard benchmark multicast traffic patterns are
			used which have been used in previous research into SpiNNaker's network:
			
			\begin{figure}
				\center
				\buildfig{figures/traffic-distribution-centroids.tex}
				
				\caption{An example 4-centroid distribution with four centroids. The
				$\times$ marks the location of the origin node. Lighter colours
				indicate greater likelihood of a connection.}
				\label{fig:traffic-distribution-centroids}
			\end{figure}
			
			\begin{description}
				
				\item[Uniform] Destinations are chosen with uniform probability
				anywhere in the machine.
				
				\item[$N$-Centroids] Destinations are clustered around one of $N$
				randomly chosen `centroids' as illustrated in figure
				\ref{fig:traffic-distribution-centroids}.
				
			\end{description}
			
			The uniform traffic pattern is widely used in networks research
			\cite{dally04,davies12} while the centroids model was developed
			specifically to reproduce the traffic patterns found in the neural
			applications SpiNNaker is designed for \cite{navaridas14}. In this work
			we consider 3 centroids.
		
		\subsection{Fault model}
			
			In addition two different fault models are used which are representative of
			the faults found in real SpiNNaker systems:
			
			\begin{figure}
				\center
				\begin{subfigure}{0.48\linewidth}
					\hspace*{-1.5cm}
					\buildfig{figures/fault-example-uniform.tex}
					
					\caption{Uniform}
					\label{fig:fault-example-uniform}
				\end{subfigure}
				\begin{subfigure}{0.48\linewidth}
					\hspace*{-1.5cm}
					\buildfig{figures/fault-example-hss.tex}
					
					\caption{HSS Link}
					\label{fig:fault-example-hss}
				\end{subfigure}
				
				\caption{The two link fault models considered.}
				\label{fig:fault-example}
			\end{figure}
			
			\begin{description}
				
				\item[Uniform] Links are selected and disabled at random (figure
				\ref{fig:fault-example-uniform}).
				
				\item[HSS Link] Groups of links corresponding with randomly selected
				single High-Speed Serial (HSS) link between SpiNNaker boards are disabled
				together (figure \ref{fig:fault-example-uniform}).
				
			\end{description}
			
			The uniform link failure model models isolated failures resulting from
			isolated manufacturing defects in individual links. The HSS Link failure
			model models faults arising from failing or disconnected board-to-board
			links which carry several chip-to-chip traffic flows via a single cable in
			SpiNNaker systems. Though SpiNNaker-specific, the later fault model is
			analogous to failure modes arising in other architectures where a single
			fault may render several links impassable in a single area.
			
			A range of failure rates are explored in this section. My measurements of
			current large-scale SpiNNaker installations the link failure rate is about
			\SI{0.03}{\percent} with failures due to both individual chip-to-chip links
			and board-to-board HSS links. Exact link failure statistics for commercial
			super computer installations are not widely available, however, published
			Mean-Time-Between-Failure (MTBF) statistics place an upper bound on link
			failure rates at a similar \SI{0.03}{\percent} in one-year-old BlueGene/Q
			systems \cite{chiu11}.
			
			Unfortunately presently undiagnosed problem with the SDRAM packaged with
			approximately \SI{1}{\percent} of SpiNNaker chips has rendered these chips
			unusable for most applications. The gaps in the network resulting from the
			loss of these chips currently dominate true link failures leaving just over
			\SI{1}{\percent} of links inoperable.
			
			Surprisingly, research into fault tolerant routing in super computers
			appears to focus on benchmarks with even higher fault rates ranging from
			\SI{3}{\percent} to as high as \SI{7}{\percent}
			\cite{ho04,gomez04,mejia06}.
			
			In this evaluation, fault rates ranging from \SI{0.01}{\percent} to
			\SI{5}{\percent} are considered to cover both realistic fault levels
			along with the more extreme cases considered in related work.
		
		\subsection{Base routing algorithm}
			
			Since the PGS repair process is routing algorithm agnostic all
			experiments use the NER algorithm which has been found to be appropriate
			for SpiNNaker applications \cite{navaridas14}.
		
		\subsection{Algorithm runtime}
			
			To assess the impact of the PGS repair process on routing algorithm
			runtime, the algorithm was used to process a large number of randomly
			generated routing problems and the runtime recorded.
			
			\num{10000} one-to-sixteen multicast routing problems were generated in a
			$256\times256$ hexagonal torus topology, the largest size possible for a
			SpiNNaker system. Other quantities of multicast destinations were also
			evaluated but are omitted for brevity since the pattern of results are
			similar to those outlined here.
			
			TODO: APPENDIX WITH OTHER RUNS?
			
			The NER and PGS repair algorithms were written in C and compiled with GCC
			4.8.3 with \verb|-O2| level optimisations and executed on a cluster of
			idle workstations with 3.10 GHz Intel Core-i5-2400 CPUs.
			
			\begin{figure}
				\center
				\buildrplot{figures/routing-runtimes.R}
				
				\caption{Mean runtime of routing and PGS repair overhead. PGS repair
				overhead is stacked above the routing runtime (i.e. bars do not
				overlap). Error bars indicate 95\% confidence interval. Note different
				Y-scale for HSS link and uniform fault models.}
				\label{fig:routing-runtimes}
			\end{figure}
			
			Figure \ref{fig:routing-runtimes} shows the average runtimes recorded for
			both the NER and PGS repair algorithms. In fault-free networks the
			PGS-repair post-processing step is not required and incurs no penalty
			while the runtime of the algorithm grows with the fault rate for both
			fault and traffic models.
			
			Notably the HSS fault model results in longer runtimes for the PGS repair
			process compared with an equivalent fault-density of uniform faults.
			Because the HSS fault model produces contiguous lines of faults the PGS
			repair algorithm must construct a longer path to avoid the fault.  Since
			the space explored by a graph algorithm typically grows with $O(H^2)$
			with respect to the hops in the discovered route, $H$, this increase in
			search distance has a large impact on the runtime of the PGS repair
			process.
			
			The runtime of the PGS repair algorithm remains roughly in proportion to
			the runtime of the underlying routing algorithm with respect to different
			traffic models. The centroid traffic pattern tends to result in routes
			with fewer hops than a uniform traffic pattern with the same number of
			destination nodes as segments of routes are often shared between
			destination nodes. Since the NER algorithm's runtime is strongly related
			to the number of hops in the output route the runtime of the algorithm is
			greater for uniform traffic. Likewise the probability of PGS repair being
			required increases with the number of hops in route and hence the runtime
			of the PGS repair algorithm increases roughly in proportion.
		
		\subsection{Routing table usage}
			
			In order to gain a realistic measure of routing table usage it is
			necessary to determine the effect of many routes being generated for a
			single set of faults. To enable a sufficiently large number of sample to
			be collected the experimental setup considered previously is reduced to a
			network containing $48\times48$ nodes.
			
			\num{1000} $48\times48$ node network models are produced according to the
			HSS link and uniform fault models. For each of these models
			$48\times48\times16=$~\num{36864} one-to-sixteen routes are generated using
			the centroid and uniform traffic models. This corresponds to one
			multicast route per application core. As is convention in SpiNNaker,
			routing table entries are inserted for each route at the source of the
			route, at each destination and at each corner or fork. The number of
			routing table entries at each node in the model is counted and the
			maximum number of entries in a single node is reported for each network
			model.  The \emph{maximum} number of routing entries of any router was
			chosen since the number of entries available per SpiNNaker router is
			bounded by hardware.
			
			\begin{figure}
				\center
				\buildrplot{figures/routing-entries.R}
				
				\caption{Violin plot showing the distribution of maximum table sizes
				for \num{1000} random networks. The red line at \num{1024} entries
				indicates the size of SpiNNaker's routing tables.}
				\label{fig:routing-entries}
			\end{figure}
			
			
			Figure \ref{fig:routing-entries} shows the distributions of the largest
			routing table sizes for each fault and traffic model.
			
			\begin{figure}
				\center
				\begin{subfigure}{0.48\linewidth}
					\center
					\buildfig{figures/hss-link-routing-table-usage.tex}
					
					\caption{Routing table entries}
					\label{fig:hss-link-routing-table-usage}
				\end{subfigure}
				\begin{subfigure}{0.48\linewidth}
					\center
					\buildfig{figures/hss-link-resource-usage.tex}
					
					\caption{Routes passing through chip}
					\label{fig:hss-link-resource-usage}
				\end{subfigure}
				
				\caption{The impact of a HSS link fault on routing table usage and
				congestion. Each hexagon represents a single chip, the red line
				indicates the chip-to-chip connections broken by the HSS link fault.}
				\label{fig:hss-link-usage}
			\end{figure}
			
			The HSS link failure model has a much greater impact on peak routing
			table resource usage than uniform link failures for a given fault rate.
			This is because HSS link faults result in a large concentration of routes
			being disrupted and then re-routed around the same obstacle in a single
			location. Figure \ref{fig:hss-link-routing-table-usage} shows how routing
			table usage varies around a HSS link fault in one instance of the
			experiment. There are clear peaks in routing table usage around the ends
			of the line of faults which result from routes produced by PGS repair
			finding shortest paths around the edge of the faults.
		
		\subsection{Network congestion}
			
			To measure the impact of PGS repair on network congestion, two
			experiments were performed, one using the same model used to measure
			routing table usage and one based on tests run on SpiNNaker hardware.
			
			For each of the network fault and traffic pattern described previously,
			the paths taken for the \num{36864} one-to-sixteen multicast routes
			generated are used to compute the number of times each link in the
			network is used. The number of routes passing through the most-used link
			is then recorded, giving an indication of the level of congestion in the
			network.
			
			\begin{figure}
				\center
				\buildrplot{figures/routing-resource.R}
				
				\caption{Violin plot showing the distribution of maximum
				routes-per-chip for \num{1000} random networks.}
				\label{fig:routing-resource}
			\end{figure}
			
			The results are presented in figure \ref{fig:routing-resource} and follow
			the same trends as the results previously shown for routing table usage.
			Again, HSS link faults result in routes with the greatest congestion due
			to the concentration of routes finding shortest paths around an obstacle
			(see \ref{fig:hss-link-resource-usage}).
			
			To verify that the results above, an additional experiment has been
			carried out which attempts to mimic the model used previously in actual
			SpiNNaker hardware. In these experiments a large SpiNNaker machine is
			divided into independent 48-board (2304-chip) sections. Because the
			48-board systems used in these experiments are cut out of a larger
			machine, they lack wrap-around links and thus form hexagonal mesh
			topologies, rather than hexagonal toruses.
			
			Due to the SDRAM issue described above, fault rates below
			\SI{1}{\percent} cannot be modelled.  To simulate higher fault rates,
			additional links are disabled in software according to the fault models
			described used previously. Since some faults are due to genuine hardware
			faults, these faults cannot be placed randomly in each experiment. To
			reduce, bias each combination of fault rate, fault model and traffic
			pattern is repeated XXX times across randomly chosen physical machines.
			
			XXX 1-to-XXX routes are generated in both uniform and XXX-centroid
			distributions as used throughout this evaluation. Synthetic network
			traffic is generated at the source of each route following a Bernoulli
			distribution. Traffic consumers running on all destination nodes accept
			packets as quickly as possible from the network and log their arrival.
			The Bernoulli probability is set the same for every route's traffic
			generator and increased in steps of XXX and the number of packets dropped
			in an XXX second period logged. The network is considered saturated once
			less than \SI{99}{\percent} of packets successfully arrive at their
			destination.
			
			Figure \ref{XXX} shows the distributions of the saturation points for
			each experimental configuration.
			
			TODO: ANALYSIS
		
	\section{Conclusions}
		
		In this chapter I described how SpiNNaker's unconventional network and
		router architecture render existing fault tolerant routing algorithms
		unsuitable. I introduced PGS repair, a post-processing technique for
		existing non-fault tolerant routing algorithms designed for SpiNNaker such
		as NER.
		
		Unlike some other fault tolerant routing algorithms for other
		architectures, PGS repair is able to work-around arbitrary fault patterns
		by exploiting SpiNNaker's inbuilt deadlock avoidance mechanisms. In the
		presence of realistic failure rates of up to \SI{1}{\percent}, only small
		overheads of up to XXX, XXX and XXX for in algorithm runtime, routing table
		usage and network performance are incurred respectively. This low
		performance overhead makes PGS repair appropriate for use in real
		applications. At the time of writing the algorithm has been successfully
		used in a number of neural and non-neural SpiNNaker applications.
		
		At more extreme fault rates not expected in real-world systems, the
		algorithm still functions correctly but the results incur much greater
		routing table and congestion overheads, particularly when faults are
		concentrated. Future extensions to this algorithm might aim to reduce this
		overhead by producing longer and more varied routes around faults to even
		out the load.
