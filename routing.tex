\chapter{Fault tolerant routing for SpiNNaker}
	
	\label{sec:routing}
	
	In the previous chapter I developed an efficient technique for finding
	shortest path vectors in hexagonal torus topologies. These vectors are used
	as the basis for many routing algorithms, notably including all published
	SpiNNaker-specific algorithms. These routing algorithms, however, do not
	tolerate faults which are a typical hazard in any large-scale system. In this
	chapter, I introduce a post-processing step for existing routing algorithms
	which ensures that the generated routes avoid faulty links and nodes.
	
	In SpiNNaker, the vast majority of network faults are the result of
	manufacturing defects which are static and known before of routing time.
	Though transient faults can occur and new faults may develop as the hardware
	ages, informal experiments suggest that such faults are unlikely, even during
	application runs lasting on the order of a day at a time. As a consequence,
	for many short and medium duration applications, static fault tolerance
	measures such as those described in this chapter are expected to be
	sufficient.
	
	Numerous heuristic-based fault tolerant routing algorithms exist which target
	different network topologies and router architectures. Unfortunately, as I
	will show, these algorithms are not portable and rely on, or attempt to work
	around, specific features of their target network architecture. In
	particular, existing work is dominated by the challenge of developing routing
	schemes which avoid or resolve network deadlocks~\cite[chapter~14]{dally04}.
	Due to SpiNNaker's unconventional use of timeout-based flow-control, it is
	not subject to the routing restrictions present in other architectures
	intended to cope with deadlocks. The post-processing technique proposed in
	this chapter utilises SpiNNaker's built-in deadlock avoidance capability
	allowing it to generate arbitrary routes to avoid faults.
	
	To assess the performance of the proposed routing technique, a number of
	experiments have been performed using both software models and SpiNNaker
	hardware. For realistic fault rates, the proposed fault tolerant routing
	algorithm incurs negligible execution time overhead during route generation
	and only a small increase in network congestion and routing table usage.
	
	\section{Related work}
		
		In this section I discuss the techniques used by the non-fault-tolerant
		multicast routing algorithms developed for SpiNNaker to date. I then
		explore the techniques employed by other architectures to achieve fault
		tolerance and consider their suitability for use with SpiNNaker.
		
		\subsection{Multicast routing in SpiNNaker}
			
			Various non-fault-tolerant multicast routing algorithms have been
			proposed and evaluated specifically in the context of SpiNNaker's
			hexagonal torus network.
			
			In 2012, Davies \emph{et al.} evaluated the use of three common torus
			routing algorithms in SpiNNaker~\cite{davies12}. In each of the
			algorithms considered, multicast routes are constructed by merging
			together unicast routes from the source to each sink. The following
			underlying unicast routing algorithms were considered:
			
			\begin{description}
				
				\item[Dimension Order Routing (DOR)] Packets are routed along each
				dimension (e.g. $X$, $Y$ and $Z$) in turn until no further hops are
				available in that direction.  The order in which the dimensions are
				traversed is fixed.
				
				\item[Right Turn Only Routing (RTOR)] As in DOR except the dimension
				order is chosen such that routes only contain right-turns.
				
				\item[Longest Dimension First Routing (LDFR)] As in DOR except the
				dimension order is chosen in descending order of number of hops in each
				dimension.
				
			\end{description}
			
			\begin{figure}
				\center
				\begin{subfigure}{0.3\linewidth}
					\center
					\buildfig{figures/simple-routers-dor.tex}
					
					\caption{DOR}
					\label{fig:simple-routers-dor}
				\end{subfigure}
				\begin{subfigure}{0.3\linewidth}
					\center
					\buildfig{figures/simple-routers-rtor.tex}
					
					\caption{RTOR}
					\label{fig:simple-routers-dor}
				\end{subfigure}
				\begin{subfigure}{0.3\linewidth}
					\center
					\buildfig{figures/simple-routers-ldfr.tex}
					
					\caption{LDFR}
					\label{fig:simple-routers-dor}
				\end{subfigure}
				
				\caption[Multicast routes produced by merging unicast routes.]%
				{Example multicast routes produced by merging together unicast
				routes from a central source node to each destination node.}
				\label{fig:simple-routers}
			\end{figure}
			
			Figure~\ref{fig:simple-routers} illustrates an example multicast route
			from a central source point to a number of sinks located along a
			hexagonal perimeter. The authors concluded that, of the three algorithms,
			multicast routes built using LDFR deliver the most balanced and efficient
			use of network resources.
			
			In 2014, Navaridas \emph{et al.} introduced two new algorithms, `Enhanced
			Shortest Path Routing' (ESPR) and `Neighbourhood Exploring Routing' (NER)
			which are explicitly designed to generate multicast
			routes~\cite{navaridas14}.
			
			In both ESPR and NER, the algorithm considers each sink vertex in turn,
			starting with those nearest to the source. For each vertex the algorithm
			attempts to find a nearby vertex to which a route already exists. If a
			connected vertex is found, an LDFR route from that vertex is added. If no
			nearby vertices are found the algorithm inserts a LDFR route from the
			source vertex. By producing routes which connect to already-connected
			nearby vertices, ESPR and NER hope to reduce the number of long-distance
			routes from the source.
			
			\begin{figure}
				\center
				\begin{subfigure}{0.45\linewidth}
					\center
					\buildfig{figures/search-regions-espr.tex}
					
					\caption{ESPR}
					\label{fig:search-regions-espr}
				\end{subfigure}
				\begin{subfigure}{0.45\linewidth}
					\center
					\buildfig{figures/search-regions-ner.tex}
					
					\caption{NER}
					\label{fig:search-regions-ner}
				\end{subfigure}
				
				\caption[The ESPR and NER multicast routing algorithms.]%
				{The ESPR and NER algorithms attempt to connect the node marked
				`D' to the closest node in the shaded region which is connected to the
				source node, `S'. If no connected node is found in the shaded region, the
				LDFR route is taken to `S'. The dotted line indicates the route chosen
				from `D'.}
				\label{fig:search-regions}
			\end{figure}
			
			ESPR and NER differ in the way they search for nearby vertices. In ESPR,
			the algorithm searches within a parallelogram drawn to enclose the
			current and source vertices as illustrated in
			figure~\ref{fig:search-regions-espr}. The NER algorithm searches for
			vertices within a fixed distance of the current vertex as illustrated in
			figure~\ref{fig:search-regions-ner}.
			
			The NER algorithm was found to produce higher quality multicast routes
			for larger SpiNNaker machines than both ESPR and the simpler algorithms
			described in previous work. Unfortunately, as in previous work, this
			algorithm cannot tolerate faults.
			
		\subsection{Fault tolerant routing}
			
			Numerous fault tolerant routing algorithms have been proposed for
			super-computer networks but they are largely constrained by the need to
			maintain deadlock freedom. Since SpiNNaker's routers employ a timeout
			based deadlock-breaking strategy, much of this effort is unnecessary in
			SpiNNaker. As we see below, this frequently renders existing fault
			tolerant routing algorithms unnecessarily complex and inflexible making
			them unable to take advantage of the efficiency savings achieved by
			algorithms such as NER.
			
			\begin{figure}
				\center
				\buildfig{figures/ring-deadlock.tex}
				
				\caption[A deadlock in a ring network.]%
				{A deadlock in a ring network where each node is waiting for
				the next to accept a packet before accepting any further packets.}
				\label{fig:ring-deadlock}
			\end{figure}
			
			Deadlocks occur in a network when a cyclic dependency on any resource in
			the network develops. For example, as illustrated in
			figure~\ref{fig:ring-deadlock}, in a ring network a deadlock may occur
			when every node is waiting for the next node to accept a packet before
			accepting new packets from the previous node.
			
			To prevent deadlocks, combinations of router microarchitectural features
			and routing restrictions are often employed. For example, a simple
			deadlock-free (unicast) routing algorithm for mesh and torus networks
			mandates the use of DOR~\cite[chapter~14]{dally04}. Packets travelling in
			a positive direction along each axis take priority over those travelling
			in a negative direction. Packets travelling along the Y axis take
			priority over those travelling along the X dimension. Given these rules
			it is possible to define a total ordering on all hops in the network.
			Figure~\ref{fig:deadlock-free-dor} illustrates a $3\times3$ mesh network
			whose hops have been numbered according to the total ordering defined
			above.  Any `X-then-Y' DOR route through this network results in the use
			of hops labelled with strictly increasing numbers. As a result, no cyclic
			dependencies (and thus no deadlocks) may occur.
			
			\begin{figure}
				\center
				\buildfig{figures/deadlock-free-dor.tex}
			
				\caption[Deadlock-free routing using DOR in a 2D mesh topology.]%
				{Deadlock-free routing of two example routes using DOR in a 2D mesh
				topology. The numbers of the hops taken by each route are given on the
				right. (Figure based on~\cite[figure~14.12]{dally04}.)}
				\label{fig:deadlock-free-dor}
			\end{figure}
			
			Unfortunately, the routing restrictions imposed to ensure deadlock
			freedom can result in fault-intolerant routing. In the case of DOR
			example, if the node at the bottom-right corner of the figure was faulty,
			the dotted example route would be blocked as no alternative routes are
			allowed. Although some routing schemes may feature more `relaxed' routing
			rules~\cite{rodrigo09}, these may still ultimately not tolerate all
			faults.
			
			\begin{figure}
				\center
				\buildfig{figures/lamb-nodes.tex}
				
				\caption[Avoiding faults using `lamb' nodes and DOR.]%
				{`Lamb' nodes may be disabled to ensure DOR will never
				encounter a fault.}
				\label{fig:lamb-nodes}
			\end{figure}
			
			The 3D torus based BlueGene/L supercomputer~\cite{adiga02} notably makes
			use of DOR to ensure deadlock freedom while implementing fault tolerance
			by disabling so-called `lamb' nodes. Lamb nodes are nodes which, though
			otherwise healthy, could potentially have their DOR routes blocked by a
			fault. This principle is illustrated in the 2D topology shown in
			figure~\ref{fig:lamb-nodes} where a single node has died. Nodes in the
			same row and column as the dead node are turned into `lamb' nodes and are
			configured to forward packets but otherwise take no part in running
			applications. DOR routes between the remaining nodes will never pass
			through the dead node thus avoiding the fault while maintaining normal
			performance. This approach trades off wasted compute capacity for
			maintaining network performance in the presence of faults. When faults
			are rare and easily repaired this trade-off can be worthwhile. In
			SpiNNaker, such fine-grained replacement of nodes is impractical since
			each chip constitutes a node and therefore a whole board of 48 chips must
			be replaced to repair a single fault.
			
			Other routing algorithms proposed for BlueGene attempt to avoid the need
			for lamb nodes by generating routes which reach their destination via a
			`proxy' node~\cite{gomez04}. In this scheme, a pair of nodes DOR route
			their messages via a `proxy' node.  This node is chosen such that routes
			to and from the proxy are not obstructed by the fault.  This approach
			relieves the need to sacrifice `lamb' nodes at the expense of adding to
			the workload of the proxy nodes and increasing routing complexity.
			
			Alternative routing strategies may take a hybrid approach whereby an
			efficient but fault intolerant routing algorithm is used which falls back
			on a less efficient, fault tolerant routing algorithm when faults are
			encountered. For example, the Immucube network architecture employs three
			virtual networks which operate independently over the same physical
			links~\cite{puente07}. Initially messages are routed in the first virtual
			network using a high-performance routing scheme which may deadlock or be
			blocked by faults.  If a deadlock occurs, the deadlocked packet is moved
			into the second virtual network in which packets are routed using a less
			efficient deadlock-free algorithm. Finally if packets in the first and
			second virtual networks encounter a fault they are moved onto the third
			virtual network. The third virtual network forms a ring network which
			eventually reaches every node in the network, though potentially with a
			high performance penalty.
			
			Instead of providing multiple routing rules which apply globally, some
			hybrid routing schemes divide the system up into regions in which
			different routing rules are used which avoid the particular patterns of
			faults in those areas~\cite{mejia06,boppana95}. As in other hybrid
			approaches, fault tolerance is usually achieved through the use of a
			relatively inefficient routing algorithm, though their use is constrained
			to a single region of the system.
			
			Other routing algorithms are distributed and use only local information
			along with limited information from their peers to generate
			routes~\cite{fick09b}. In SpiNNaker applications, route generation is
			typically carried out centrally, allowing algorithms such as ESPR and NER
			to take advantage of global information when producing multicast routes.
			Since routes in SpiNNaker applications are often static, using a
			centralised routing algorithm to produce high quality routes `up front'
			may be favourable over potentially faster but lower quality distributed
			algorithms.
			
			Algorithms for very different network architectures also tend to be a
			poor fit for SpiNNaker. IP networks, for example, must contend with a
			dynamic and irregular network topology and so the trade-offs made in
			their routing algorithms are a poor match for the applications considered
			here.  Similarly work focusing on other architectures whose topology
			differs significantly from a torus -- for example data centre
			networks~\cite{guo08,liao12} -- are not general enough be of use.
			
			%% XXX: Include remark about circuit routing? Probably not since there
			%%      are an awful lot of routing-like problems and just covering one
			%%      seems silly.
			% Outside the field of computer networks, routing algorithms used to route
			% wires across the surfaces of chips are required to solve similar problems
			% to those considered here. Like mesh networks, the routes are defined
			% within a regular 2D Manhattan geometry and congested areas rather than
			% faults must be avoided by the router \cite{kahng11}. These algorithms are
			% designed for occasional batch operation prior to the multi-month process
			% of chip manufacturing and so execution times of hours or days are
			% commonplace \cite{nam08}. In addition, instances of congestion in
			% circuit designs are far more common than dead links in networks which
			% has an impact on the approach taken by these algorithms. Consequently,
			% these algorithms are a poor fit for network routing.
	
	\section{Partial graph search repair}
		
		As we have seen, existing multicast routing algorithms for SpiNNaker are
		designed to make efficient use of the system's network but are not able to
		avoid faults. The hybrid routing approach used by Immucube allows an
		existing, efficient routing algorithm to be used while falling back on
		other routes when faults are encountered.  In this section I describe a
		novel hybrid routing algorithm based on a post-processing scheme called
		Partial Graph Search (PGS) Repair.  By using SpiNNaker's timeout-based
		deadlock avoidance scheme, PGS repair is not hampered by the routing
		restrictions imposed by other architectures. For example, expensive router
		architectural features, such as the multiple virtual networks used by
		Immucube, are not required. In addition, the routes taken to avoid faults
		are not constrained to inefficient paths, such as system-wide ring
		networks, to ensure deadlock freedom.
		
		The PGS repair technique uses a graph search algorithm to find alternative
		routes around faulty nodes and to repair fault-afflicted routes generated
		by a non-fault-tolerant routing algorithm such as NER. Though graph search
		algorithms are generally shunned in network routing due to their high
		computational complexity, their execution time is often bounded by $O(H^D)$
		where $H$ is the number of hops in the generated route and $D$ is the
		number of dimensions in the network.  Since $H$ is likely to be small
		because faults are rare and isolated in real
		systems~\cite{gara05,alverson12}, the execution time of the graph search
		will also be small in practice.
		
		The PGS repair technique proceeds as follows. A route is initially
		generated by a fault intolerant routing algorithm such as NER. If no faults
		are encountered by this route, the algorithm terminates immediately.
		Otherwise the algorithm proceeds to post-process this route.
		
		\begin{figure}
			\center
			\begin{subfigure}{0.32\linewidth}
				\hspace*{-1.5em}
				\buildfig{figures/pgs-repair-colouring.tex}
				
				\caption{}
				\label{fig:pgs-repair-colouring}
			\end{subfigure}
			\begin{subfigure}{0.32\linewidth}
				\hspace*{-1.5em}
				\buildfig{figures/pgs-repair-colouring-fix1.tex}
				
				\caption{}
				\label{fig:pgs-repair-colouring-fix1}
			\end{subfigure}
			\begin{subfigure}{0.32\linewidth}
				\hspace*{-1.5em}
				\buildfig{figures/pgs-repair-colouring-fix2.tex}
				
				\caption{}
				\label{fig:pgs-repair-colouring-fix2}
			\end{subfigure}
			
			\caption[PGS repair example.]%
			{PGS repair example showing a disconnected multicast route from A
			to B, C, D, E and F. {\color{red}$\times$} indicates a broken link.}
			\label{fig:pgs-repair-colouring-steps}
		\end{figure}
		
		The routing tree produced by the underlying routing algorithm is broken
		into subtrees wherever it attempts to route through a broken link or node.
		Each subtree is then assigned a unique `colour', as illustrated in
		figure~\ref{fig:pgs-repair-colouring}. From each disconnected subtree's
		root in turn, a graph search is performed to find a short, fault-free route
		to a subtree of any other colour. Once a subtree has been found, the route
		discovered by the graph search is added and the subtree coloured to match
		the tree it has been connected to.
		
		For example in figure~\ref{fig:pgs-repair-colouring-fix1}, a path from the
		root of the subtree containing nodes E and F is found which connects it to
		the subtree rooted at A. In figure~\ref{fig:pgs-repair-colouring-fix2} a
		path is also found connecting the subtree containing nodes C and D back to
		the subtree rooted at node A. At this point only one tree remains -- the
		fully connected route.
		
		If the routing tree was broken into $N+1$ subtrees by faults there will be
		$N$ subtrees disconnected from the root node. Each of the $N$ graph
		searches performed connects two disconnected subtrees reducing the number
		of subtrees by $1$ each time. After $N$ iterations, therefore, exactly one
		tree remains which connects every node in the original routing tree
		resulting in a multicast route not obstructed by faults.
		
		Because SpiNNaker's network architecture ensures deadlock freedom for any
		routing pattern, the graph search algorithm is free to use any route to
		avoid a dead link, reducing the overhead introduced by avoiding faults.
		
	\section{Evaluation \& Results}
		
		\label{sec:routing-evaluation}
		
		The PGS repair technique, by design, is able to work around all possible
		fault patterns (excluding those which completely disconnect entire regions
		of the network). As a consequence, my evaluation focuses on the impact on
		performance PGS repair imposes rather than its ability to avoid faults.
		Specifically I concentrate on the impact of PGS repair on three figures of
		merit: algorithm execution time, routing table utilisation, and network
		throughput.
		
		\subsection{Traffic Patterns}
			
			Two standard benchmark multicast traffic patterns are considered which
			have been used in previous research into SpiNNaker's network:
			
			\begin{figure}
				\center
				\buildfig{figures/traffic-distribution-centroids.tex}
				
				\caption[An example of a 3-centroid traffic distribution.]%
				{An example of a 3-centroid traffic distribution. Traffic is
				sent by the node at the position marked with an {\color{red}$\times$}
				to nodes chosen randomly with nodes in lighter areas being selected
				with a higher probability.}
				\label{fig:traffic-distribution-centroids}
			\end{figure}
			
			\begin{description}
				
				\item[Uniform] Destinations are chosen with uniform probability
				anywhere in the machine. This traffic pattern is widely used in
				networks research~\cite[chapter~23]{dally04} and is representative of
				applications with either dense communication requirements or poor
				placement quality (see chapter~\ref{sec:placement}).
				
				\item[3-centroids] Destinations are clustered around the sending node
				with a 75\% probability or one of 3 randomly chosen `centroids' with a
				5\% probability each as illustrated in
				figure~\ref{fig:traffic-distribution-centroids}.  Destinations are
				distributed around the sender or centroid according to a geometric
				distribution with more distant locations less likely to be selected.
				This pattern was developed specifically to reproduce the traffic
				patterns found in the neural applications SpiNNaker is designed
				for~\cite{navaridas14}.
				
			\end{description}
			
			There is little consensus on the `fan out' of routes in real SpiNNaker
			applications. As we will see in the next chapter
			(\S\ref{sec:existing-applications}), fan outs in existing applications
			vary between unicast routes and multicast routes with almost one hundred
			sinks. Since informal experimentation has suggested that fan out does not
			have a significant impact on the performance of PGS repair and I have
			simplified this evaluation by only considering routes with a single
			source node and exactly sixteen sink nodes.
		
		\subsection{Fault model}
			
			\begin{figure}
				\center
				\begin{subfigure}{0.48\linewidth}
					\hspace*{-0.5cm}
					\buildfig{figures/fault-example-uniform.tex}
					
					\caption{Uniform}
					\label{fig:fault-example-uniform}
				\end{subfigure}
				\begin{subfigure}{0.48\linewidth}
					\hspace*{-0.5cm}
					\buildfig{figures/fault-example-hss.tex}
					
					\caption{HSS Link}
					\label{fig:fault-example-hss}
				\end{subfigure}
				
				\caption{The two link fault models considered.}
				\label{fig:fault-example}
			\end{figure}
			
			Two different fault models are considered which are chosen to be
			representative of the faults found in real SpiNNaker systems:
			
			\begin{description}
				
				\item[Uniform] Links are selected and disabled at random
				(figure~\ref{fig:fault-example-uniform}). This models failures
				resulting from isolated manufacturing defects in individual links.
				
				\item[HSS Link] Groups of links corresponding with randomly selected
				High-Speed Serial (HSS) links between SpiNNaker boards are disabled
				(figure~\ref{fig:fault-example-uniform}). Each HSS link carries several
				chip-to-chip traffic flows via a single cable; problems result in a
				contiguous line of failures.
				
			\end{description}
			
			Though SpiNNaker-specific, the later fault model is analogous to failure
			modes arising in other architectures where a single fault may render
			several links impassable in a single area.
			
			Informal measurements of current large-scale SpiNNaker installations
			suggest that the link failure rate is approximately \SI{0.03}{\percent}
			with failures due to both individual chip-to-chip links and
			board-to-board HSS links. Exact link failure statistics for commercial
			super computer installations are not widely available, however, published
			Mean-Time-Between-Failure (MTBF) figures coincidentally place an upper
			bound on link failure rates at a similar \SI{0.03}{\percent} in
			one-year-old BlueGene/Q systems~\cite{chiu11}.
			
			In addition to link faults, there is currently an undiagnosed problem
			with the SDRAM packaged with around \SI{1}{\percent} of SpiNNaker chips
			which renders them unusable. These faulty chips leave gaps in the network
			which dominate `true' link faults making the effective link fault rate
			rise to \SI{1}{\percent}.
			
			Surprisingly, research into fault tolerant routing in super computers
			appears to focus on benchmarks with even higher fault rates which range
			from \SI{3}{\percent} to as high as
			\SI{7}{\percent}~\cite{ho04,gomez04,mejia06}.
			
			In this evaluation, fault rates ranging from \SI{0.01}{\percent} to
			\SI{5}{\percent} are considered to cover both realistic fault levels
			along with the more extreme cases considered in the literature.
		
		\subsection{Base routing algorithm}
			
			Since the PGS repair process may be used with any routing algorithm, all
			experiments have been conducted against routes initially generated by the
			NER routing algorithm since this algorithm is known to produce high
			quality routes for SpiNNaker's network~\cite{navaridas14}.
			
		\subsection{Algorithm execution time}
			
			To assess the execution time overhead introduced by the PGS repair
			process, routing problems were generated for a $256\times256$ hexagonal
			torus topology with randomly generated faults. For each combination of
			fault pattern and traffic pattern \num{10000} route and fault sets were
			generated and the execution time of the NER and PGS repair algorithms
			recorded.
			
			The NER and PGS repair algorithms were written in C and compiled with GCC
			4.8.3 with \verb|-O2| level optimisations and executed on a cluster of
			idle workstations with 3.10 GHz Intel Core-i5-2400 CPUs.
			
			\begin{figure}
				\center
				\buildrplot{figures/routing-runtimes.R}
				
				\caption[Mean execution time of NER routing and PGS repair.]%
				{Mean execution time of NER routing and PGS repair. Bars are stacked
				and do not overlap. Error bars indicate 95\% confidence interval.}
				\label{fig:routing-runtimes}
			\end{figure}
			
			Figure~\ref{fig:routing-runtimes} shows the average execution times
			recorded for both the NER and PGS repair algorithms. In fault-free
			networks the PGS repair post-processing step is not required and incurs
			no penalty. As the fault rate increases the execution time of the
			algorithm grows proportionally for both fault and traffic models.
			
			The HSS fault model results in longer execution times for the PGS repair
			process compared with an equivalent fault-density of uniform faults.
			Because the HSS fault model produces contiguous lines of faults the PGS
			repair algorithm must construct a longer path to avoid the fault.  Since
			the space explored by a graph algorithm grows with $O(H^2)$ in a 2D
			hexagonal torus topology, this increase in search distance has a
			significant impact on the execution time of the PGS repair process.
			
			The centroid traffic pattern tends to result in routes with fewer hops
			than uniform traffic since route segments are often shared between
			destination nodes in the same centroid.  Since the NER algorithm's
			execution time is strongly related to the number of hops in the output
			route the execution time of the algorithm is greater for uniform traffic.
			Likewise the probability of a route encountering a fault and thus PGS
			repair being invoked, increases with the number of hops in the route,
			hence PGS repair execution times are reduced under centroid traffic.
		
		\subsection{Routing table usage}
			
			SpiNNaker uses a table based router with a fixed quantity of table
			entries in its multicast router. Algorithms such as NER attempt to
			exploit `default routing' which allows routing table entries to be
			omitted on chips where a packet is forwarded in a straight line by
			generating routes which avoid turning corners.  PGS repair, however,
			makes no such effort and thus may be liable to increase the number of
			routing table entries required.
			
			To gain a realistic measure of routing table usage it is necessary to
			produce a range of routes for a single example network and generate
			routing tables accordingly.  To enable a sufficiently large number of
			samples to be collected smaller networks with $48\times48$ nodes are
			considered in this experiment.
			
			One thousand $48\times48$ node networks are produced according to the HSS
			link and uniform fault models. For each of these networks two sets of
			$48\times48\times16=$~\num{36864} routes are generated: one using the
			centroid model and the other the uniform traffic model. This number of
			routes corresponds to one multicast route per application core in a
			$48\times48$ SpiNNaker system.
			
			Routing table entries are inserted at the source node of each route, at
			each destination node and at every corner or fork, exploiting default
			routing to route packets along other parts of routes. This table
			generation scheme is widely used in SpiNNaker applications due to its
			simplicity and efficient use of routing table entries. Though more
			effective techniques exist for compressing routing tables for use in
			SpiNNaker's network~\cite{mundy16}, their performance is strongly
			influenced by other factors such as routing key selection and so are not
			considered here.
			
			For each network model and traffic pattern the number of routing table
			entries required on each node is counted. The maximum count is then
			recorded since even if only one routing table is too large to fit in
			SpiNNaker's router the application cannot run.
			
			\begin{figure}
				\center
				\buildrplot{figures/routing-entries.R}
				
				\caption[Routing table sizes for \num{1000} random networks.]%
				{Violin plot showing the distribution of maximum routing table
				sizes for \num{1000} random networks. The red line at \num{1024}
				entries indicates the size of SpiNNaker's routing tables.}
				\label{fig:routing-entries}
			\end{figure}
			
			Figure~\ref{fig:routing-entries} shows the distributions of the largest
			routing table sizes for each fault and traffic model. For realistic fault
			rates (under \SI{0.1}{\percent}), routing table sizes are not
			significantly impacted.
			
			At around the \SI{1}{\percent} fault rate, routing table sizes grow
			little in the presence of uniform faults though begin to exceed routing
			table size limits for the HSS link faults. Since SpiNNaker's
			\SI{1}{\percent} fault rate can be attributed to isolated, uniformly
			distributed SDRAM faults, the HSS link fault model is not representative
			of real faults at this fault rate. As such, I conclude that for the fault
			rates and fault patterns experienced by real SpiNNaker machines PGS
			repair has no significant impact on routing table usage.
			
			\begin{figure}
				\center
				\begin{subfigure}{0.48\linewidth}
					\center
					\buildfig{figures/hss-link-routing-table-usage.tex}
					
					\caption{Routing table entries}
					\label{fig:hss-link-routing-table-usage}
				\end{subfigure}
				\begin{subfigure}{0.48\linewidth}
					\center
					\buildfig{figures/hss-link-resource-usage.tex}
					
					\caption{Routes passing through chip}
					\label{fig:hss-link-resource-usage}
				\end{subfigure}
				
				\caption[The impact of a HSS link fault.]%
				{The impact of a HSS link fault on routing table usage and
				congestion. Each hexagon represents a single chip, the red line
				indicates the chip-to-chip connections broken by the HSS link fault.}
				\label{fig:hss-link-usage}
			\end{figure}
			
			As the fault rates climb to more extreme levels, routing table sizes grow
			more quickly under the HSS link failure model. This is because HSS link
			faults result in a large concentration of routes being re-routed around
			the same obstacle. Figure~\ref{fig:hss-link-routing-table-usage} shows
			how routing table sizes have doubled around a HSS link fault in one
			instance of the experiment. The peaks in routing table use are
			concentrated around the tips of the HSS link fault as a result of PGS
			repair always taking shortest path routes around obstacles.
		
		\subsection{Network congestion}
			
			To measure the impact of PGS repair on network congestion, two
			experiments were performed. In the first, the same model is used as in
			the routing table usage experiments, using a simple metric to estimate
			network congestion, and thus throughput. The second experiment attempts
			to confirm the validity of the first using SpiNNaker hardware. A
			synthetic traffic generator running on SpiNNaker was used to determine
			network throughput in the presence of simulated faulty links.  Because of
			the baseline fault rate of \SI{1}{\percent} and limited routing table
			size in real SpiNNaker hardware, these experiments are more limited in
			scope and sample size.
			
			\subsubsection{Software model}
			
				For each of the network fault and traffic patterns described in the
				previous experiment, the number of times each link is used is counted.
				The number of routes passing through the most-used link is recorded,
				giving an indication of the peak congestion in the network.  In
				general, network throughput is bounded by the slowest, or in this case,
				most congested link in the system~\cite[chapter~1]{dally04} making this
				metric a suitable proxy for peak throughput.
				
				\begin{figure}
					\center
					\buildrplot{figures/routing-resource.R}
					
					\caption[Distribution of routes-per-chip for \num{1000} random networks.]%
					{Violin plots showing the distribution of maximum
					routes-per-chip for \num{1000} random networks.}
					\label{fig:routing-resource}
				\end{figure}
				
				The results are presented in figure~\ref{fig:routing-resource} and
				follow the same trends as the results shown for routing table usage.
				Again, HSS link faults result in routes with the greatest congestion
				due to the concentration of routes finding shortest paths around an
				obstacle as seen in figure~\ref{fig:hss-link-resource-usage}.
			
			\subsubsection{SpiNNaker experiments}
			
				To verify the results produced by the software model, an additional
				experiment has been carried out using SpiNNaker hardware. In these
				experiments a synthetic network traffic generator is loaded onto a
				$8\times8$ chip SpiNNaker machine partitioned from a larger SpiNNaker
				installation.  Because these machines are subsections of larger torus
				topologies, they lack wrap-around links and form hexagonal mesh
				topologies. It was not possible to use larger topologies because, at
				some of the fault rates and distributions considered, the \num{1024}
				routing table entries available in SpiNNaker are insufficient.
				
				The traffic generation software running on SpiNNaker is configured to
				produce uniform and 3-centroid traffic patterns with routes generated
				by the NER and PGS repair algorithms. Simulated network faults are
				inserted according to the HSS link and uniform fault models in addition
				to those already present. Due to the baseline fault rate of
				\SI{1}{\percent} these experiments only consider fault rates above
				this.
				
				Packets are generated according to a Bernoulli distribution. The rate
				of packet injection is gradually increased across all routes until
				SpiNNaker's network becomes saturated and packets start being dropped.
				Once fewer than \SI{99}{\percent} of packets arrive at their intended
				destinations, the network is deemed to have saturated and the peak
				throughput achieved prior to saturation is recorded. This experiment is
				repeated 100 times, each time with a different randomly generated
				network and on a randomly partitioned block of a larger SpiNNaker
				system.
				
				\begin{figure}
					\center
					\buildrplot{figures/routing-hardware.R}
					
					\caption[Maximum throughput achieved on SpiNNaker.]%
					{Maximum throughput achieved by \num{100} random traffic
					patterns running on SpiNNaker. The dots show the recorded result
					(jittered in the `X' direction for clarity) and the lines show the
					mean.}
					\label{fig:routing-hardware}
				\end{figure}
				
				Figure~\ref{fig:routing-hardware} confirms that, as suggested by the
				software model, increased fault rates result in reduced network
				throughput. HSS link faults also continue to result in greater
				performance segregation than uniform faults. Increasing the fault rate
				from 1\% to 3\% halves throughput for HSS link faults while in the
				presence of uniform faults throughput is only reduced by 10\%.
				
				In the experiments performed on SpiNNaker, the uniform and centroid
				traffic patterns differ little. This similarity is attributed to the
				small size of the network under test resulting in there being little
				difference between the shape of the traffic patterns at that scale.
				
				Overall, the results gathered on SpiNNaker confirm the trends suggested
				by the software model.
		
	\section{Conclusions}
		
		Fault tolerance is a necessity in large-scale systems such as SpiNNaker and
		other super computers. Thanks to SpiNNaker's unconventional use of
		timeout-based deadlock avoidance, the expensive trade-offs made by routing
		algorithms for other architectures need not be made. The PGS repair
		algorithm introduced in this chapter is able to exploit the high-quality
		routes generated by existing non-fault-tolerant routing algorithms while
		handling faults with minimal routing and network overhead.
		
		Unlike some other fault tolerant routing algorithms, PGS repair is able to
		work around arbitrary fault patterns when a route exists. In the presence
		of fault rates of up to \SI{0.1}{\percent}, representative of real-world
		systems, the execution time, routing table use and network performance
		overhead introduced were found to be negligible.  In the presence of more
		extreme fault rates of up to \SI{1}{\percent} representative of current
		SpiNNaker machines suffering from an unknown SDRAM issue, the overhead
		remains manageable. Overhead increased by \SI{30}{\percent},
		\SI{11}{\percent} and \SI{44}{\percent} for algorithm execution time,
		routing table usage and network performance respectively.  As a result of
		these performance characteristics, PGS repair is a viable option for
		SpiNNaker applications.
		
		At more extreme fault rates, not anticipated in real-world systems, PGS
		repair produces routes which are valid but incur much greater congestion
		and routing table overhead, particularly when faults are concentrated in
		one area. Future extensions to this algorithm might aim to reduce this
		overhead by producing longer and more varied routes around faults to even
		out load.
		
		The success of the PGS repair algorithm demonstrates that SpiNNaker's
		unconventional deadlock avoidance scheme may warrant greater attention in
		future, larger scale systems in which more faults may be present. It may
		also be applicable to low-cost systems with less stringent quality
		controls.
