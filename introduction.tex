\chapter{Introduction}

\label{sec:introduction}

%Problem area
%
%* Network construction and exploitation
%  * Cabling: Build it cheaply in terms of tech cost and install cost
%  * Routing: Get around it cheaply and reliably
%  * Placement: Use it efficiently

The Spiking Neural Network Architecture (SpiNNaker) is a novel super computer
architecture designed to simulate biologically realistic models of brains in
real time \cite{furber07}. Though neurons, the building blocks of the brain,
are relatively well understood, their complex interactions remain mysterious.
Just as understanding the workings of a transistor is insufficient to
understand a modern microprocessor, neuroscientists believe that understanding
the neurons in isolation cannot explain the brain and that understanding their
connectivity is key \cite{eliasmith13,eliasmith14}. Experiments on real brains,
however, are fraught with difficulty. Variations between individuals can be
significant and it is only possible to record tens or hundreds of the trillions
of signals in the brain, and even then only with limited control over which
signals are recorded. Computer simulations of models of large neural networks,
however, enable researchers to develop repeatable experiments and gain complete
visibility of any signal and any neuron. Models such as SPAUN
\cite{eliasmith12}, built from millions of simulated neurons, have shown great
promise in expanding our understanding of higher level brain functions such as
memory and simple problem solving.  Unfortunately these neural models are
expensive to simulate, requiring hours of compute time to simulate each second
of neural activity. As well as being inconvenient, this precludes the use of
robotics to immerse these models in real world environments and also limits
studies of long-term behaviours such as learning.

SpiNNaker is designed to enable the real time simulation of models containing
up to one billion neurons -- approximately \SI{1}{\percent} of a human brain or
ten mouse brains \cite{furber06}. To achieve this goal, the largest planned
SpiNNaker machine will contain over one million low-powered computer processors
interconnected by a bespoke network architecture.

SpiNNaker's large processor count matches the current trend in super computers
where processor counts are growing exponentially \cite{meuer16j}, mimicking the
growth of the number of components in the processors themselves predicted by
Gordon Moore's famous `law' \cite{moore75}. As a result of this growth, the
interconnection networks which enable these processors to work together have
grown in importance \cite{dally04}.  Network designers must carefully balance
performance against practicality and financial cost.  SpiNNaker's network is no
exception to this rule and, as the systems scale up from desktop prototypes to
machine-room scale installations, the reality of building and exploiting these
machines presents an array of challenges.

As in all super computers, SpiNNaker's network interconnects its processors in
a particular network topology which defines how different processors may
communicate with each other. Unlike the tree and $N$-dimensional torus
topologies found in contemporary super computers \cite{dally04}, SpiNNaker
employs a `hexagonal torus topology'. In this topology, nodes in SpiNNaker's
network fit together in a honeycomb-like pattern where messages may `hop' from
node to node to reach their destination. As we will see in
chapter~\ref{sec:background}, the hexagonal torus topology, in theory, sits at
a `sweet spot' in terms of network performance and practicality. As the first
known large-scale installation of the hexagonal torus topology, however, there
remain a number of practical challenges for large spinnaker machines arising
from this choice.

As super computer networks have grown in scale to millions of processors the
task of dealing with previously rare faults has grown.  Though fault rates in
networks remain consistently low, architectures such as SpiNNaker may have
hundreds of thousands of links meaning even fault rates of a fraction of a
percent will impact tens or hundreds of links. To enable reliable operation,
networks must be able to adapt the routes taken by messages through the network
to avoid faulty links and nodes. The techniques employed are often closely tied
to a particular network architecture and consequently SpiNNaker's novel network
architecture demands its own approach.

Another challenge introduced by the growing scale of super computers is making
\emph{efficient} use of network resources. Communicating processes should be
located on logically `nearby' nodes to reduce network load. The neural models
for which SpiNNaker is designed are often described abstractly, rather than
geometrically, using modelling languages such as PyNN~\cite{davison08} and
Nengo~\cite{eliasmith04}.  Because of this, the communication requirements of
simulations can be highly irregular making an efficient placement of processes
onto processors in the machine non-trivial.

%Contributions
%
%* Cabling scheme for hexagonal toruses without long cables
%* Efficient installation technique for dense systems
%* Exhaustive and efficient route calculation in hex toruses
%* Fault tolerant routing scheme exploiting SpiNNaker's odd router
%* Placement based on SA a: works very well and b: suggests circuit placement is
%  a good source of inspiration.

This thesis addresses the practical challenges of scaling up the SpiNNaker
architecture in a real-world setting summarised by these research questions:

\begin{enumerate}
	
	\item Can the hexagonal torus topology be deployed and used in real, large
	scale systems?
	
	\item Does SpiNNaker's router architecture help, or hinder fault tolerance?
	
	\item How can the parts of a neural simulation be placed onto a large
	hexagonal torus topology to reduce network load?
	
\end{enumerate}

%Structure
%
%* Chapter 2: Background: detailed dive into what's in SpiNNaker, why its
%  really so unusual. Also looks at what applications run on SpiNNaker and how
%  they work.
%* Chapter 3: How to build a really big SpiNNaker machine.
%* Chapter 4: How to find your way around that machine.
%* Chapter 5: How to find your way around that machine even when its broken.
%* Chapter 6: Now you can walk, time to run.
%* Chapter 7: Wrapping up.
%* Appendices: Hard-to-come-by theoretical and practical details useful if
%  you're about to continue where this research left off but be useful but
%  otherwise hard to come by, especially in one place.

Chapter~\ref{sec:background} introduces the SpiNNaker architecture and, in
particular, describes its hexagonal torus topology and network architecture.

In chapter~\ref{sec:building}, I develop a cabling scheme for large hexagonal
torus topologies which enables arbitrarily large networks to be constructed
using only short, inexpensive cables. This theoretical work is then evaluated
through the construction of a range of prototype SpiNNaker systems. The largest
of these prototypes contains over half a million processor cores and spans
several machine room cabinets. In addition, I propose the use of built-in
diagnostic facilities to assist technicians performing network installation and
maintenance. This technique is found to greatly reduce the effort required and
the number of mistakes made.

In chapters~\ref{sec:shortestPaths}~and~\ref{sec:routing} I develop new routing
techniques for SpiNNaker's network. Chapter~\ref{sec:shortestPaths} develops a
new approach to finding the shortest paths through hexagonal torus topologies,
an integral part of many routing algorithms. This newly proposed approach is
cheaper to compute than the state of the art and, unlike previous efforts, is
able to discover all valid short paths through the topology. This theoretical
advance brings hexagonal torus topologies in line with conventional topologies
by providing routing algorithms with complete information about the paths
available to them. In chapter \ref{sec:routing} I propose a fault tolerant
routing algorithm for SpiNNaker which is able to avoid arbitrary static fault
patterns with minimal performance overhead. A key finding of this chapter is
that the flexibility afforded to fault tolerant routing algorithms by
SpiNNaker's unconventional router architecture is what facilities the low
overheads reported in this chapter.

Finally, in chapter~\ref{sec:placement}, I explore the problem of application
placement in SpiNNaker's network. As in other networks and applications, neural
simulations should be arranged such that communication occurs primarily between
processors close together in the network to control network load. Due to the
irregular connectivity and large scale of the neural models expected to run on
SpiNNaker, an automated approach is necessary. I develop a novel placement
algorithm based on algorithms used for circuit layout in computer chips. My
algorithm is found to allow some larger neural models to run on SpiNNaker for
the first time while enabling other applications to run at greater speeds. In
addition, synthetic benchmarks containing over one million processes indicate
that this algorithm should handle the anticipated demands of the neural models
expected to run on large-scale SpiNNaker installations.
