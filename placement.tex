\chapter{Placing applications in large SpiNNaker machines}
	
	In the previous chapter I tackled the problem of scale in generating routes
	for very large networks such as SpiNNaker. In this work the centroid traffic
	pattern was used as an approximation of the expected network traffic
	generated by `well behaved' neural network simulation software running on
	SpiNNaker. The traffic produced largely exhibits strong locality, that is
	most communication occurs between either nearby nodes or clusters of nodes.
	In reality, neural simulation applications are not specified geometrically
	but rather as abstract graphs of communicating neurons
	\cite{davison08,eliasmith13}. Applications must then \emph{place} these
	neurons onto nodes in a SpiNNaker system, attempting maximise communication
	locality.
	
	In this chapter I re-evaluate the suitability of simulated annealing as a
	technique for finding high quality placements for large parallel
	applications. Though this technique had fallen out of fashion in the field of
	application placement by the early 1990s, it has found wide use for placing
	components in computer chip and FPGA designs. In the intervening years,
	placement problems in super computers have grown in size from tens or
	hundreds of nodes to millions, a scale at which chip placement techniques
	were operating in the mid 1990s. I adapt the simulated annealing algorithm
	used by the VPR academic circuit placement software to produce placements for
	applications running on SpiNNaker. In that in a range of real and synthetic
	benchmarks simulated annealing produces high quality placements enabling
	efficient use of SpiNNaker's network resources.
	
	
	%In the field of chip design, Moore's `Law' \cite{moore65,moore75} observes a
	%similar exponential growth in the number of components within a single chip.
	%Today modern processors contain billions of components and an analogous
	%placement problem exists in attempting to place interconnected components
	%near to eachother. In this chapter I explore the techniques used for circuit
	%placement and adapt one such technique, Simulated Annealing (SA)
	%\cite{kirkpatrick83}, for use in application placement. Despite some early
	%interest in SA for application placement in the 1980s and early 1990s, the
	%technique has since fallen out of favour. I find that at the scales of modern
	%placement problems SA-based placement is able to produce solutions of
	%superiour quality to contemporary methods.
	%
	%TODO: SUMMARISE RESULTS...
	
	\section{Related work}
		
		The placement problem has been tackled independently in the literature by
		researchers in both the application and chip placement communities. In this
		survey I cover application and chip placement separately as these two
		communities have remained largely isolated from one another. First I
		explore the techniques applied to application placement before moving on to
		contrast this with the techniques used in circuit placement.
		
		\subsection{Application placement algorithms}
			
			TODO: GENERAL INTRO
			
			In the application placement literature, the placement problem is often
			referred under the umbrella term `mapping'. Unfortunately term is often
			used more broadly to include other tasks such as routing and application
			partitioning. To avoid ambiguity I use the term `placement', as preferred
			by the chip and FPGA design communities, to refer specifically to the
			problem of assigning nodes in an application's communication graph to nodes
			in a machine's connectivity graph.
			
			\subsubsection{Application-specific approaches (manual placement)}
				
				In the case of some applications such as finite element modelling
				\cite{bermejo13}, the structure of the problem itself leads to a
				natural placement of the computation on nodes in a machine. For example
				when simulating a 3D volume in an node super computer with a $3 \times
				4 \times 2$ 3D torus or mesh topology network, the modelled volume
				might be divided into as in figure \ref{fig:fem-partitioning}. Each
				cuboid in the model is then assigned to the corresponding node in the
				network topology.
				
				\begin{figure}
					\center
					\buildfig{figures/fem-partitioning.tex}
					
					\caption{Example partitioning of a 3D space to fit into a super
					computer with a $3\times4\times2$ torus or mesh topology.}
					\label{fig:fem-partitioning}
				\end{figure}
				
				When the number of dimensions in a problem do not match that of the
				underlying network architecture, the common solution is to either
				divide only along a subset of the axes or to divide into additional
				pieces on the existing axes \cite{gilge14}.
			
			\subsubsection{Sequential placement}
				
				In the case where a placement solution is non-obvious one of the
				simplest and most popular strategies is to apply a simple sequential
				placement algorithm. Sequential placement algorithms function by
				iterating over the vertices in the application's communication graph
				and assigning them to a free node in the target machine. Sequential
				placement algorithms are differentiated by the order in which they
				iterate over vertices in the communication graph and fill nodes in the
				target machine. A number of widely used orderings are described below.
				
				\begin{figure}
					\center
					\begin{subfigure}{0.32\linewidth}
						\center
						\buildfig{figures/sequential-row-order.tex}
						\caption{Row-order}
						\label{fig:sequential-row-order}
					\end{subfigure}
					\begin{subfigure}{0.32\linewidth}
						\center
						\buildfig{figures/sequential-alternating.tex}
						\caption{Alternating}
						\label{fig:sequential-alternating}
					\end{subfigure}
					\begin{subfigure}{0.32\linewidth}
						\center
						\buildfig{figures/sequential-hilbert.tex}
						\caption{Hilbert curve}
						\label{fig:sequential-hilbert}
					\end{subfigure}
					
					\caption{Space-filling curves in 2D mesh and torus topologies.}
					\label{fig:sequential}
				\end{figure}
				
				Super computer management software such as SLURM \cite{yoo03} and Blue
				Gene's system software \cite{gilge14} by default na\"ively iterate over
				vertices in an application communication graph in the order they are
				provided. The nodes in the target machine are then iterated over in a
				simple space-filling curve through the network topology. Figure
				\ref{fig:hilbert-placement} illustrates the default patterns available
				in these software packages. The row-order (figure
				\ref{fig:sequential-row-order}) and alternating (figure
				\ref{fig:sequential-alternating}) curves correspond with 2D versions of
				the default node assignment orders used in SLURM and BlueGene systems.
				
				\begin{figure}
					\center
					\buildfig{figures/hilbert-placement.tex}
					
					\caption{A Hilbert curve, coloured from blue to red.}
					\label{fig:hilbert-placement}
				\end{figure}
				
				The Cray extensions to SLURM software provide a Hilbert curve
				\cite{hilbert91} (figure \ref{fig:sequential-hilbert}) node assignment
				order. Unlike the row-order and alternating space filling curves the
				Hilbert curve ensures that pairs of vertices close together in the node
				iteration order are also close together in the target machine's network
				\cite{moon01, zumbusch99}. Figure \ref{fig:hilbert-placement} shows a
				5$^\textrm{th}$-order Hilbert curve where each point in the curve is
				coloured according to its position along the curve. In this figure it
				is possible to see that nearby positions in the curve (which share
				similar colours) are also close in 2D space.
				
				When the proximity of vertices in the vertex-ordering supplied by an
				application is a good estimator of those vertices communication
				requirements, the sequential assignment schemes discussed above can be
				very effective. These techniques have also proven adequate in
				small-scale and densely connected applications such as early neural
				simulations running on prototype SpiNNaker machines with tens of nodes
				\cite{galluppi10} but growing beyond this scale has proven problematic.
				
				\begin{figure}
					\center
					\begin{subfigure}{0.45\linewidth}
						\center
						\buildfig{figures/rcm-initial.tex}
						
						\caption{Original permutation}
						\label{fig:rcm-initial}
					\end{subfigure}
					\begin{subfigure}{0.45\linewidth}
						\center
						\buildfig{figures/rcm-sorted.tex}
						
						\caption{RCM permutation}
						\label{fig:rcm-sorted}
					\end{subfigure}
					
					\caption{Adjacency matrix representation of a graph before and after
					permutation by the RCM algorithm.}
					\label{fig:rcm}
				\end{figure}
				
				A number of algorithms have been proposed for automatically selecting
				good vertex and chip iteration orders, typically using a
				graph-traversal based heuristic. An example typical of this type of
				technique is `graph similarity based mapping' described by Hoefler and
				Snir \cite{hoefler11}. This algorithm exploits Reverse
				Cuthill-McKee (RCM) algorithm \cite{cuthill69} to select a linear
				ordering of vertices in which connected vertices tend to appear close
				together. Internally the RCM algorithm uses a breadth-first-search-like
				algorithm to order vertices.
				
				The RCM algorithm was originally designed as a tool for reducing the
				bandwidth of matrices to improve the performance of linear algebra
				techniques. Figure \ref{fig:rcm} illustrates how a sparse matrix is
				permuted by the RCM algorithm to produce a matrix with low bandwidth.
				The matrix dual of a graph is an adjacency matrix, $M$, where $M_{i,j}$
				is $w$ if node $i$ is connected by an edge to node $j$ with weight $w$
				and 0 otherwise. When a graph's adjacency matrix is permuted by the RCM
				algorithm, the connectivity of the graph is unchanged but the mapping
				from vertices to rows and columns in the matrix changes. The ordering
				of vertices in a low-bandwidth adjacency matrix will tend to keep
				connected vertices closer together in the ordering.
				
			\subsubsection{Simulated annealing}
				
				In the academic community, a number of attempts have been made to use
				more sophisticated optimisation algorithms for the placement of
				applications. In 1985, Steele \cite{steele85} proposed the use of
				simulated annealing for placing applications in the 6D torus topology
				of the 64 node `Caltech Cosmic Cube' machine. The proposed algorithm is
				described below.
				
				Initially, the vertices in the communication graph of the application
				to be placed are assigned randomly to nodes in the machine's
				connectivity graph. Two vertices are then chosen at random and swapped.
				If this swap reduces the \emph{cost} of the placement (as defined
				later), the vertices are kept in their new locations. If the cost
				increases as a result of the swap, the vertices are moved back to their
				original locations with a probability dependent on the cost increase
				incurred and the current \emph{temperature}.
				
				The process of picking and swapping vertices is repeated many times.
				Initially the temperature is set to a large value causing almost all
				swaps to be accepted, regardless of their cost. As the algorithm
				proceeds, the temperature is gradually reduced until eventually only
				swaps resulting in cost reductions are accepted. By initially accepting
				some apparently detrimental swaps, the algorithm is able to avoid
				becoming trapped in local minima. Towards the end of the algorithm's
				execution the solution is presumed to be close to the global
				minimum-cost solution and so few or no detrimental swaps are accepted.
				
				The cost of a particular placement is defined by some expression
				approximating the quality of the placement with better quality
				placements having lower-costs. In Steele's work, this cost is computed
				by finding the shortest path routes between all connected vertices in
				the connectivity graph using Dijkstra's algorithm. Rather than
				repeating this relatively expensive computation after each swap, the
				cost may be updated by only recomputing the routes connecting with the
				swapped vertices.
				
				The number of swaps to attempt and the way the temperature is reduced,
				known as the \emph{annealing schedule}, can have a major impact on the
				eventual quality of the solution. Selection of an annealing schedule is
				discussed in greater depth later in this chapter.
				
				Towards the end of the 1980s, application placement appeared to be
				becoming less important as super computer network architectures
				improved appearing to render computationally intensive placement
				algorithms necessary \cite{dally87}.  In addition, network and problem
				sizes remained small, so small in fact that exact linear-programming
				based placement algorithms remained a meaningful point of comparison
				for simulated annealing based techniques \cite{xu91}.  In this
				environment, simpler sequential placement algorithms gained favour over
				more computationally expensive algorithms such as simulated annealing.
				
				In contemporary application placement algorithms, `low temperature'
				simulated annealing algorithms are occasionally employed as a
				post-processing step for by other placement algorithms \cite{hoefler11}
				but has otherwise fallen out of use.
				
				% Outside of badly attempting 90s scale problems at least... \cite{chen06}
			
			\subsubsection{Recursive partitioning}
				
				As problem and machine sizes have grown and network utilisation has
				once again become an important factor in application performance
				\cite{navaridas09b} more complex optimisation algorithms have
				reappeared in the literature. One popular approach employs graph
				partitioning algorithms such as METIS \cite{karypis98} to perform
				recursive bipartitioning based placement
				\cite{phillips14,hoefler11,pellegrini96}.  This placement process is
				illustrated in figure \ref{fig:partitioning}.
				
				\begin{figure}
					\center
					\buildfig{figures/partitioning.tex}
					
					\caption{Illustration of application placement by recursive
					partitioning. Each node has the capacity to hold two vertices.}
					\label{fig:partitioning}
				\end{figure}
				
				In the first step, the application communication graph and machine
				connectivity graph are bipartitioned such that the number of edges
				between partitions is minimised. Each half of the communication graph
				is associated with one of the halves of the machine connectivity graph.
				The partitioning process is then repeated recursively on each of the
				two communication and connectivity graph pairs. The process halts when
				either of the graphs can no longer be partitioned. The vertices in the
				communication graph are then placed on their associated node in the
				connectivity graph.
				
				Unfortunately recursive partitioning-based techniques are known to
				perform sub-optimally when the number of nodes is not a power of two
				\cite{simon97}. In addition, placement problems are often subject to
				multiple orthogonal constraints, for example processor and memory
				requirements. Since the balance of a partition with multiple orthogonal
				resources depends on the eventual packing employed by a placement,
				partitioning-based algorithms may inadvertently become trapped by local
				minima.
				
				Other approaches employ other optimisation techniques but designed for
				use with networks particular connectivity patterns, for example trees
				\cite{jeannot14,traff02}. Since SpiNNaker's network structure does not
				match these architectures, these approaches are unsuitable.
	
		\subsection{Circuit placement algorithms}
			
			The circuit placement problem in chip and FPGA design is very similar to
			that of application placement, requiring communicating components to be
			assigned positions in some topology such that congestion is controlled.
			Modern CPUs have millions or billions of components and thus present a
			considerable placement problem often exceeding even the largest
			application problems. As such, chip placement algorithms may be well
			suited to tackling modern large-scale problems.
			
			A key difference between the circuit and application placement problems
			is that while super computer network topologies come in many shapes and
			forms, chips are generally strictly 2D and often constrained to Manhattan
			geometry (analogous to a 2D mesh topology) to simplify routing. As a
			result of this circuit placement algorithms often directly exploit
			geometric propoerties of the problem which may make them difficult to
			adapt to certain network topologies.
			
			Circuit placement techniques can be grouped into three broad categories
			\cite{kahng11}:
			
			\begin{description}
				
				\item[Partitioning-based] These algorithms function by recursively
				partitioning the circuit in order to group together related components.
				
				\item[Simulated annealing]
				
				\item[Analytical] These algorithms approximate the placement problem as
				another problem to which exact, or good, solutions can be found
				efficiently.
				
			\end{description}
			
			I assess the suitability of each of these techniques for use in
			application placement below. For a more detailed survey of circuit
			placement techniques the reader is referred to `VLSI physical design:
			from graph partitioning to timing closure' by Kahng \emph{et al.}
			\cite{kahng11}.
			
			\subsubsection{Partitioning-based placement}
				
				Partitioning based placement has a long history with early examples
				dating back to the 1970s \cite{breuer77}. As in recursive partitioning
				based application placement algorithms, the input circuit is
				partitioned alongside the target topology. Since the target topology is
				the 2D surface of a chip only simple geometric partitioning is
				required.
				
				Modern partitioning based placers such as Capo \cite{roy05} employ
				additional heuristics such as terminal propagation where connections
				between adjacent partitions are are considered to attempt to align
				components on either side of the cut.
				
				As in application placement, however, difficulties in generating good
				quality many-way partitions can negatively impact the performance of
				these algorithms. These approaches have since been largely superseded
				\cite{markov15}.
			
			\subsubsection{Analytical placement}
				
				The most recent generation of placement algorithms attempt to transform
				placement problems into approximately equivalent problems with
				closed-form solutions. One of the common trade-offs of these approaches
				is modelling components as point-like objects. As circuit designs have
				grown, this approximation has grown more accurate resulting in a boom
				in popularity \cite{markov15}.
				
				A popular analytical method employed by a number of circuit placement
				algorithms is `quadratic placement' \cite{kahng11,spindler08}. In this
				algorithm the components in a circuit are modelled as infinitesimal
				objects with connections between components modelled as spring-like
				forces. Unlike real springs which obey Hooke's law, the force exerted
				by a connection is proportional to the \emph{square} of its distance.
				
				\begin{figure}
					\center
					\buildfig{figures/quadratic-placement.tex}
					
					\caption{An optimal 1D quadratic placement solution for two fixed
					vertices ($f_1$ and $f_2$), two movable vertices ($m_1$ and $m_2$)
					and three weighted edges.}
					\label{fig:quadratic-placement}
				\end{figure}
				
				To illustrate this approach, consider the simple one-dimensional
				placement problem presented in figure \ref{fig:quadratic-placement}.
				In the quadratic placement technique, the cost of a placement is the
				weighted sum of the squares of the edge distances. In this case this
				yields:
				%
				\begin{equation*}
					\textrm{Cost} = 1(f_1 - m_1)^2 + 2(m_1 - m_2)^2 + 1(m_2 - f_2)^2
				\end{equation*}
				%
				An optimal placement is one which minimises this function. To find
				values which minimise a quadratic, the equation is differentiated
				yielding a system of linear equations. The linear equations are then
				solved to find the positions of the movable vertices which minimise the
				quadratic cost function.
				
				Quadratic placement is extended or 2 (or more dimensions) by simply
				applying the process for each dimension separately. Unfortunately, a
				straight-forward extension to non-euclidean geometries such as trees or
				even toruses is non trivial. For example, quadratic placement relies on
				cost increasing monotonically as two points are moved in opposite
				directions while in a torus this is not the case. Likewise in tree
				topologies, `movement' is not well defined.
				
				An additional challenge in quadratic placement is that some vertices'
				locations must be fixed in order to prevent a degenerate solution where
				all vertices have the same coordinate. In circuit placement, the
				locations of `pads' used to connect components in a circuit to the
				outside world are typically fixed and provide convenient anchors during
				placement. Algorithms such as SimPL \cite{kim12b} employ a combination
				of analytical techniques in order to add virtual `anchor' components
				which prevent degenerate solutions from forming. Unfortunately this
				approach further adds to the difficulty of porting the technique to
				non-euclidean geometries.
				
				% TODO: mention viswanathan07 FastPlace 3.0 -- multilevel technique?
			
			\subsubsection{Simulated annealing}
				
				Simulated annealing was originally developed specifically to be applied
				to the problem of circuit placement \cite{kirkpatrick83}. Throughout
				the late 1980s and early 1990s simulated annealing became the dominant
				circuit placement technique when the first chips breaching one-million
				components appeared \cite{betz97,sechen85}.  Due to difficulties in
				scaling beyond tens-of-millions of components, simulated-annealing has
				become less popular large placement problems.  Various efforts continue
				to be made to speed up the simulated annealing process by attempting to
				partition the input problem into several independent problems
				\cite{choong10,haldar00} and parallelise the search process
				\cite{ludwin08}.
				
				Simulated annealing notably remains popular as a stand-alone placement
				tool in niche applications where flexibility is important. For example,
				the highly configurable Verilog-To-Routing (VTR) software suite
				\cite{luu14} and Open Source Arachne-PNR \cite{cseed} use simulated
				annealing to produce placements for a broad spectrum of FPGA
				architectures.
	
	\section{Application placement by simulated annealing}
		
		\label{sec:placement-by-annealing}	
		
		As we have seen, existing software placement techniques are either overly
		reliant on a specific network topology or ill suited to placing the scale
		applications containing up to the low-millions of communicating processes.
		
		In the field of circuit placement, various algorithms have been proposed
		which are explicitly designed to cope with placement problems of similar or
		larger sizes. Though analytical placement techniques now dominate the
		field, these techniques rely on the properties of 2D Euclidean geometry
		which do not apply to most torus or tree-based super computer network
		topologies. In addition, while analytical placement algorithms can produce
		high quality placement solutions for extremely large problems, the
		modelling assumptions used fit smaller problems poorly resulting in lower
		quality placements.
		
		At the scales present in world-leading current super computers over the
		next few years (see again figure \ref{fig:top500-num-processors} on page
		\pageref{fig:top500-num-processors}), simulated annealing is known to be an
		effective technique. In addition, the flexibility of this algorithm makes
		it well suited to coping with the diversity of network topologies in
		existence. This suggests that simulated annealing would be a good choice as
		a modern application placement algorithm.
		
		To determine the suitability of simulated annealing on modern applications
		I have implemented a simulated annealing based placer for the SpiNNaker
		architecture. Compared with previous simulated annealing algorithms for
		application placement, this algorithm is intended to cope with much larger
		problem instances and thus trades off accuracy in its cost estimation
		function for execution speed. By contrast with circuit placement algorithms
		built on simulated annealing, the algorithm represents the problem a little
		differently. In circuit simulation, for example, exactly one component may
		be placed in a given space while in application placement, several
		applications may be placed on the same node (e.g. on different cores within
		the node). These differences, and the specific annealing schedule used, are
		described in detail in the remainder of this section.
		
		\subsection{Representation}
			
			A typical SpiNNaker application is made up of a number of processes
			running on individual cores and communicating via multicast messages sent
			through the network. This is described as a graph where the
			\emph{vertices} represent individual processes and the (multi-)edges,
			referred to here as \emph{nets}, represent directed flows of multicast
			traffic from one process to many other processes. This part of the
			description is analogous to that of a circuit placement problem where
			vertices represent components and nets represent wires from outputs to
			the inputs of other components.
			
			Each vertex in the application graph must be placed onto a specific
			SpiNNaker chip subject to certain hard and soft constraints. A hard
			constraint must be met by the placement algorithm for a placement
			solution to be legal. A soft constraint, however, may be relaxed or
			ignored at the cost of reduced placement quality without invalidating the
			result.
			
			In circuit placement and application placement alike, there is typically
			a soft constraint that vertices connected by a net should be placed close
			to each other. In my placement algorithm, nets are assigned
			\emph{weights} which provide an additional soft constraint that more
			heavily weighted nets should be given priority over nets with lower
			weights.
			
			A hard constraint on the placement solution is that vertices assigned to
			a single chip must not consume more on-chip resources than are available.
			For example, in a typical application a vertex (i.e. a process) may use
			one or more cores and a block of shared on-chip SDRAM for data storage or
			shared-memory communication. This is similar to the hard constraint in
			circuit placement that a given physical space may be occupied by only one
			component. Unfortunately, the presence of multiple resource types to
			consider in application placement means that application placement
			actually encompasses a bin packing problem \cite{korte06}.
			
			The SpiNNaker topology is represented as a 2D hexagonal torus topology
			with homogeneous inter-chip links. Each chip is defined to have a certain
			quantity of each resource type, for example cores and SDRAM. Different
			chips may have different quantities of each resource for example due to
			the presence of defective CPU cores. Individual links and chips may also
			be flagged as dead imposing a hard constraint that the placement
			algorithm must not assign vertices to dead chips.
		
		\subsection{Generating candidate swaps}
			
			In conventional simulated annealing based placement schemes, vertices are
			moved by picking random pairs of vertices and swapping them. In an
			application placement problem as described above it is not, in general,
			possible to swap any pair of vertices. For example, if a small vertex is
			swapped with a large one, the large vertex may not fit in the space left
			behind by the smaller vertex.
				
			\begin{figure}
				\center
				\begin{subfigure}{\linewidth}
					\center
					\buildfig{figures/sa-swap-select.tex}
					\caption{A random vertex, $v_a$ is selected along with a random
					target chip (right).}
					\label{fig:sa-swap-select}
				\end{subfigure}
				
				\vspace*{1em}
				
				\begin{subfigure}{\linewidth}
					\center
					\buildfig{figures/sa-swap-remove.tex}
					\caption{Vertex $v_a$ is removed from its chip. Vertices are removed
					from the target chip until enough space is available to fit $v_a$.
					In this example, vertices $v_b$ and $v_c$ are removed.}
					\label{fig:sa-swap-remove}
				\end{subfigure}
				
				\vspace*{1em}
				
				\begin{subfigure}{\linewidth}
					\center
					\buildfig{figures/sa-swap-commit.tex}
					\caption{Vertex $v_a$ is swapped with vertices $v_b$ and $v_c$, if
					there is room. If there is not room $v_a$, $v_b$ and $v_c$ are
					returned to their original locations and the swap is aborted.}
					\label{fig:sa-swap-commit}
				\end{subfigure}
				
				\caption{Selecting vertices to swap.}
				\label{fig:sa-swap}
			\end{figure}
			
			\begin{figure}
				\begin{minted}[autogobble,tabsize=4]{python}
					def generate_swap(vertices, chips, distance):
						# Remove a random vertex from its chip
						vertex_a = pick_random(vertices)
						chip_a = get_vertex_chip(vertex_a)
						remove_from_chip(vertex_a, chip_a)
						
						# Pick different, nearby chip at random
						chip_b = pick_random_chip(chips, not=chip_a, within=distance)
						
						# Remove vertices from chip_b until there is enough space for
						# vertex_a.
						vertices_b = []
						while (not space_available_for(vertex_a, chip_b)
						       and has_vertices(chip_b)):
							vertices_b.append(remove_random_vertex(chip_b)))
						
						can_swap = (space_available_for(vertex_a, chip_b) and
						            space_available_for(vertices_b, chip_a))
						if can_swap:
							# Make the swap
							add_to_chip(vertex_a, chip_b)
							add_to_chip(vertices_b_b, chip_a)
							return (vertex_a, chip_a, vertices_b, chip_b)
						else:
							# Swap could not be made, restore vertices to original chips
							add_to_chip(vertex_a, chip_a)
							add_to_chip(vertices_b, chip_b)
							raise CouldNotSwap()
						
				\end{minted}
			
				\caption{Pseudocode for selecting vertices to swap.}
				\label{fig:swap-pseudocode}
			\end{figure}
			
			Rather than picking pairs of vertices to swap, the process illustrated in
			figure \ref{fig:sa-swap} (and precisely described by the pseudocode in
			figure \ref{fig:swap-pseudocode}) is used. This swapping process avoids
			swaps involving larger vertices being aborted because no single vertex on
			a target chip frees up enough room to complete a swap. In practice, when
			using this algorithm few swaps to be aborted.
			
			The distance parameter may be used to cause this algorithm to only
			consider swaps which move vertices small distances. This parameter is
			controlled such that initially very long-distance swaps are allowed but
			towards the end of the annealing process, only localised swaps are made
			which are less likely to have produce a negative change. This parameter's
			use is explained later in \S\ref{sec:placement-schedule-acceptance}.
		
		\subsection{Cost function}
			
			For each swap made by the algorithm, the change in the placement's `cost'
			must also be determined. In principle the ideal `cost' function would
			measure network congestion by performing routing to determine the
			resources used. This approach was used by early application placement
			algorithms \cite{steele85} but incurs a very large performance overhead
			since simulated annealing relies on being able to test vast numbers of
			random swaps.
			
			A common alternative to directly measuring congestion used by many
			circuit placement algorithms is to measure the length of the routes
			between connected vertices. Rather than actually computing routes,
			approximate metrics such as the Half-Perimeter Wire Length (HPWL) and
			star model are used \cite{kahng11}.
			
			\begin{figure}
				\center
				\begin{subfigure}[b]{\linewidth}
					\center
					\buildfig{figures/cost-function-star.tex}
					
					\caption{Star $= a_x + a_y + b_x + b_z + c_x + c_y + d_x + d_y$}
					\label{fig:cost-function-star}
				\end{subfigure}
				
				\vspace*{1.5em}
				
				\begin{subfigure}[b]{0.32\linewidth}
					\center
					\buildfig{figures/cost-function-hpwl-xy.tex}
					
					\caption{HPWL$_{xy}$ $= w_{xy} + h_{xy}$}
					\label{fig:cost-function-hpwl-xy}
				\end{subfigure}
				\begin{subfigure}[b]{0.32\linewidth}
					\center
					\buildfig{figures/cost-function-hpwl-xz.tex}
					
					\caption{HPWL$_{xz}$ $= w_{xz} + h_{xz}$}
					\label{fig:cost-function-hpwl-xz}
				\end{subfigure}
				\begin{subfigure}[b]{0.32\linewidth}
					\center
					\buildfig{figures/cost-function-hpwl-yz.tex}
					
					\caption{HPWL$_{yz}$ $= w_{yz} + h_{yz}$}
					\label{fig:cost-function-hpwl-yz}
				\end{subfigure}
				
				\caption{Net cost functions for hexagonal toruses and meshes.}
				\label{fig:cost-function}
			\end{figure}
			
			\begin{figure}
				\center
				\buildrplot{figures/cost-function-error.R}
				
				\caption{Error of net cost functions compared with actual routing
				solutions. Y-axis limit clipped for clarity. The error for the star
				cost function's error grows linearly beyond the axis limit.}
				\label{fig:cost-function-error}
			\end{figure}
			
			In the star model, the cost of a net is found by summing the distances
			from the net's source vertex to each sink vertex in turn as illustrated
			in figure \ref{fig:cost-function-star}. In a hexagonal mesh and torus
			topology, the distance between two points can be cheaply computed (see
			chapter \ref{sec:shortestPaths}) making this cost function appropriate
			for SpiNNaker's network topology. This model will over-estimate network
			resource usage for nets with three or more vertices since in practice
			routes often overlap.
			
			In the HPWL model, a bounding box is drawn around all the vertices
			connected by the net and the cost of the net is calculated as half of the
			perimeter of this bounding box. In a non-hexagonal torus or mesh
			topology, the HPWL model exactly matches the length of an optimal
			multicast route for nets with two or three vertices. In nets with a
			greater number of vertices, HPWL under-estimates the length of an optimal
			route by a factor of $\sqrt{n}$ for nets with $n$ vertices
			\cite{chung79}.
			
			The three non-orthogonal axes of hexagonal mesh and torus topologies mean
			that there are several ways in which a bounding box may be defined.
			Specifically, the bounding box may extend along any pair of axes
			resulting in three possible bounding boxes, and thus three HPWL values
			for any net as illustrated in \ref{fig:cost-function-hpwl-xy},
			\ref{fig:cost-function-hpwl-xz} and \ref{fig:cost-function-hpwl-yz}. The
			relative sizes of these three bounding boxes varies depending on specific
			arrangement of vertices in the net meaning that no single bounding box is
			appropriate for all situations.
			
			A single HPWL-like metric can be produced by taking the minimum of the
			half-perimeters of the three bounding boxes. Like the HPWL function for
			non-hexagonal toruses, this `hexagonal HPWL' cost function exactly
			predicts the length of optimal routes for nets with two or three
			vertices.
			
			To determine select the most appropriate cost function for use in a
			hexagonal torus topology, the costs estimated for \num{100000} randomly
			generated nets. Each net connects between two and thirty randomly chosen
			vertices in a $30\times30$ hexagonal torus topology. The cost calculated
			for each net is then compared with the costs of routes generated by the
			routing algorithm described in chapter \ref{sec:routing} and the error is
			plotted in figure \ref{fig:cost-function-error}. A 99\% confidence
			interval is too small to see when plotted and so has been omitted from
			the plot.
			
			These plots confirm that the error introduced is consistently
			over-estimated by the star model and under-estimated by both the
			Hexagonal HPWL and HPWL${_{xy}}$ cost models. In addition, the Hexagonal
			HPWL and HPWL$_{xy}$ scaled by $\sqrt{n}$ are shown. While this
			correction does appear to reduce the magnitude of the error for these
			functions, it does not eliminate it. This is likely due to the fact that
			the error only tends towards $\sqrt{n}$ for very large $n$ \cite{chung79}
			and also because the error is being computed against a realistic but non
			optimal placement algorithm.
			
			On the basis of these results, the star cost function is rejected on the
			grounds of poor accuracy. Though the $\sqrt{n}$-scaled Hexagonal HPWL
			cost function produces the smallest error in general, its runtime is
			three times greater than the $\sqrt{n}$-scaled HPWL$_{xy}$ whose accuracy
			is very similar. Since the execution speed of the cost function largely
			dominates the runtime of the simulated annealing algorithm, I use the
			$\sqrt{n}$-scaled HPWL$_{xy}$ net cost function as a reasonable trade-off
			between accuracy and performance.
			
		\subsection{Annealing schedule and acceptance function}
			
			\label{sec:placement-schedule-acceptance}
			
			The annealing schedule  and acceptance function (including all formulae
			below) is taken directly from that used by the VPR FPGA placement
			software \cite{betz97}. Due to a combination of time constraints and this
			schedule and acceptance function being known to work well, alternatives
			are not considered in this thesis.
			
			Starting with an intial random (but valid) placement, $N$ swaps with no
			distance limit are performed, where $N$ is the number of vertices in the
			graph. The initial annealing temperature, $T$, is then set to 20 times
			the standard deviation of the cost changes produced by the $N$ swaps.
			
			The annealing algorithm now performs a round of $E \times N^{1.33}$
			random swaps where $E$ is a tunable parameter controling `effort'. By
			default $E$ is set to 1. Increasing $E$ roughly linearly increases
			execution time for diminishing returns in improved placement quality.
			Each random swap is retained with a probabiltiy:
			%
			\begin{align*}
				P_{\textrm{accept}}=
					\begin{cases}
						1 & \textrm{if } \Delta{}C \le 0 \\
						e^{\nicefrac{-\Delta{}C}{T}} & \textrm{otherwise} \\
					\end{cases}
			\end{align*}
			%
			Where $\Delta{}C$ is the change in cost resulting from the swap and $e$
			is Euler's number. This probability is 1 for any beneficial or neutral
			change. Small cost increases accepted with a greater probability than
			large cost increases. At higher temperatures, larger cost increases are
			accepted with a greater probability.
			
			After a round of swaps has been performed, the temperature is updated
			according to $T_\textrm{new} = \alpha T_\textrm{old}$ where $\alpha$ is
			determined according to
			%
			\begin{align*}
				\alpha=
					\begin{cases}
						0.50 & \textrm{if } R_\textrm{accept} > 0.96 \\
						0.90 & \textrm{if } 0.80 < R_\textrm{accept} \le 0.96 \\
						0.95 & \textrm{if } 0.15 < R_\textrm{accept} \le 0.80 \\
						0.80 & \textrm{if }        R_\textrm{accept} \le 0.80 \\
					\end{cases}
			\end{align*}
			%
			Where $R_\textrm{accept}$ is the proportion of swaps which were accepted
			by the algorithm in that round. This approach attempts to maximise the
			time spent at temperatures in which some but not all swaps are accepted
			since this is likely when the greatest improvement in placement quality
			is occuring. The next round of annealing is then performed as described
			above using the newly calculated temperature.
			
			The algorithm terminates when $T < \nicefrac{0.005 C}{N_\textrm{nets}}$
			where $C$ is the estimated current cost of the placement solution and
			$N_\textrm{nets}$ is the number of nets in the problem.
			
			Towards the end of placement the solution will tend to be close to ideal.
			As a result, it is likely that swapping vertices over long distances will
			result in large cost increases. As a result towards the end of the anneal
			reducing the swap distance limit, $D_\textrm{limit}$, tends to increase
			the proportion of swaps which are accepted. As stated above,
			$D_\textrm{limit}$ is initially set such that all possible swaps are
			considered. It is then updated according to the following formula at the
			end of each round of swaps:
			%
			\[
				D_\textrm{limit}^\textrm{new} =
					\textrm{max}(1,\quad D_\textrm{limit}^\textrm{old}(1-0.44 + R_\textrm{accept}))
			\]
			%
			This rule attempts to limit swap distance such that the proportion of
			accepted swaps does not drop below 0.44 for as long as possible.
	
		\subsection{Additional constraints}
			
			XXX: TODO: TALK ABOUT PLACEMENT/SAME CHIP CONSTRAINTS
	
	\section{Evaluation}
		
		\label{sec:placement-results}
		
		To be useful, the simulated annealing based placer must be able to produce
		placements whose quality matches or exceeds those of conventional
		techniques and do so in a reasonable amount of time.
		
		TODO: INTRODUCE SECTION
		
		\subsection{Baseline algorithms}
			
			To provide a benchline to compare my simulated annealing based placement
			algorithm, the following three placement algorithms will be included in
			experiments:
			
			\begin{description}
				
				\item[Hilbert] The placement algorithm used by existing SpiNNaker
				software and provided as a Cray extension to SLURM. Places vertices
				along a hilbert curve path in breadth-first traversal order of the
				graph.
				
				\item[RCM] The `graph similarity based mapping' algorithm described by
				Hoefler and Snir \cite{hoefler11}. Assigns vertices to chips in Reverse
				Cuthill McKee order.
				
				\item[Random] A random placement algorithm which places vertices
				uniform-randomly with no regard for connectivity.
				
			\end{description}
			
		\subsection{Existing applications}
			
			To verify the suitability of simulated annelaing based placement for
			existing SpiNNaker applications a representaive selection of these
			applications' communication graphs have been collected as benchmarks.
			Table \ref{tab:real-benchmarks} describes each of the benchmarks.  While
			these applications are much smaller than those expected to run on new,
			large-scale SpiNNaker systems, they provide a realistic model of the
			patterns of connectivity found in these types of applications.
			
			\begin{table}
				\center
				\begin{tabular}{lrrr p{0.4\linewidth}}
					\toprule
					Name & Vertices & Nets & Fan-out & Notes \\
					\midrule
					Microcircuit   & \num{1338} & \num{760} & \num{94.50} &
						Spiking neural model of a cortical microcircuit \cite{potjans14}
						implemented in `PyNN SpiNNaker' \cite{knight16}. \\
					\addlinespace
					Sudoku         & \num{299} & \num{109} & \num{15.86} &
						Spiking neural model which solves Sudoku puzzles implemented in `PyNN
						SpiNNaker' \cite{knight16}. \\
					\addlinespace
					Card-Sorting   & \num{469} & \num{919} & \num{1.06} & 
						Spiking neural model which solves the Wisconsin card sorting test
						\cite{aubin15} implemented in `Nengo SpiNNaker' \cite{mundy15}. \\
					\addlinespace
					CConv          & \num{2560} & \num{12020} & \num{3.48} &
						Spiking neural model which performs 512D circular convolution
						\cite{eliasmith13} implemented in `Nengo SpiNNaker' \cite{mundy15}. A
						key component in the SPAUN neural model \cite{eliasmith12}. \\
					\addlinespace
					Parse          & \num{855} & \num{3046} & \num{3.26} &
						Spiking neural model which performs simple sentence parsing
						implemented in `Nengo SpiNNaker' \cite{mundy15}. \\
					\addlinespace
					MU0            & \num{1084} & \num{1084} & \num{1.38} &
						A digital circuit simulation of a simple computer processor
						\cite{nutter16} implemented using the digital circuit simulator
						example program included with `Rig' \cite{rig15}. \\
					\bottomrule
				\end{tabular}
				
				\caption{SpiNNaker application benchmarks. `Fan-out' refers to the mean
				fan out of the nets in the communication graph.}
				\label{tab:real-benchmarks}
			\end{table}
			
			\begin{figure}
				\center
				\begin{subfigure}{\linewidth}
					\center
					\buildrplot{figures/application-benchmarks-quality.R}
					
					\caption{Placement quality (routed net length, normalised)}
					\label{fig:application-benchmarks-quality}
				\end{subfigure}
				
				\vspace*{1em}
				
				\begin{subfigure}{\linewidth}
					\center
					\buildrplot{figures/application-benchmarks-tables.R}
					
					\caption{Routing table entries (red line at \num{1024} entries)}
					\label{fig:application-benchmarks-tables}
				\end{subfigure}
				
				\vspace*{1em}
				
				\begin{subfigure}{\linewidth}
					\center
					\buildrplot{figures/application-benchmarks-runtime.R}
					
					\caption{Placement algorithm runtime}
					\label{fig:application-benchmarks-runtime}
				\end{subfigure}
				
				\caption{Application benchmark placement performance.}
				\label{fig:application-benchmarks}
			\end{figure}
			
			Each benchmark's connectivity graph is placed on an idealised $13 \times
			13$ hexagonal torus topology and the routed using the router described in
			chapter \ref{sec:routing}. For each benchmark the placement and routing
			processes is repeated \num{1000} times and varoius metrics recorded which
			are aggregated in figure \ref{fig:application-benchmarks}. All
			experiments were conducted on a cluster of idle workstations with
			3.10~GHz Intel Core-i5-2400 CPUs and 8~GB of RAM.
			
			The first metric attempts to measure the `quality' of the placement
			solution in terms of the total number of hops taken by the generated
			routes. A better placement solution should require fewer hops and as a
			result encounter lower network congestion. Figure
			\ref{fig:application-benchmarks-quality} shows how the hop counts vary
			for each placement algorithm and benchmark. Since the actual hop counts
			vary greatly between benchmarks, these results are normalised by the
			number of hops produced by the Hilbert placer.
			
			In the majority of benchmarks, the simulated annealing based placement
			algorithm achieves the greatest reduction in hop counts, reducing the
			total number of hops required to $\frac{2}{7}^\textrm{ths}$ of those
			required by the Hilbert placer in the `CConv' benchmark. The RCM and
			Random placement algorithms both consistently produce results similar or
			worse than those of the Hilbert placement algorithm.
			
			Unlike the other benchmarks, the `Microcircuit' benchmark notably shows
			little difference in placement quality between the four algorithms. This
			may be explained by the fact that this network is largley all-to-all
			connected meaning that a placement function can do little more than place
			the vertices tightly together to improve quality.
			
			The second metric considered is the maximum number of routing table
			entries required on a single chip after placement. Figure
			\ref{fig:application-benchmarks-tables} shows the size of the largest
			routing table for each benchmark and placement algorithm. Though many of
			the benchmarks do not run out of routing tables at any scale, the `CConv'
			network frequently runs out of routing table entries for all bu the
			simulated annealing based placer.
			
			XXX TODO: ADD FULLY MINIMISED TABLES FOR BENCHMARK RESULTS AND COMMENT ON
			TABLE USAGE
			
			The third metric considered is placement algorithm runtime, shown in
			figure \ref{fig:application-benchmarks-tables}. The simulated annealing
			algorithm requires much longer runtimes than the baseline placement
			algorithms considered, in several cases an order of magnitude longer.
			This is unsurprising since the baseline algorithms consist of simple
			greedy approaches.
			
			The `Microcircuit' benchmark is notable in that all of the placement
			algorithms take a long time to execute compared with the other
			benchmarks. This longer runtime can be attributed to a preprocessing step
			common to every placement algorithm implementation which handles `same
			chip' constraints. This pre-processing stage merges together vertices in
			the communication graph which are labelled as requiring placement on the
			same chip. Unlike the other application graphs, the `Microcircuit'
			communication graph is annotated with a large number of these constraints
			and, due to the na\"ive implementation of the preprocessing step, all of
			the placement algorithms are slowed down by a constant amount.
			
			\subsubsection{Impact of placement on `PyNN SpiNNaker'}
				
				The `PyNN SpiNNaker' based neural models behind the `Microcircuit' and
				`Sudoku' benchmarks are compute bound and produce relatively small
				quantities of network traffic. Configuring `PyNN SpiNNaker' to use each
				of the four placement algorithms under consideration did not result in
				any changes in performance.
				
				It is anticipated that future models built on PyNN SpiNNaker will grow
				in size and, unlike the Microcircuit model, have less dense
				connectivity. With larger, more structured models, placement is
				anticipated to be of greater significance.
			
			\subsubsection{Impact of placement on `Nengo SpiNNaker'}
				
				The `Nengo SpiNNaker' based neural simulations behind the `CConv' and
				`Parse' benchmark were each run \num{100} times with each of the
				placement algorithms under test. For each run, the number of packets
				dropped by the SpiNNaker routers was recorded and displayed in figure
				\ref{fig:nengo-dropped-packets}.
				
				\begin{figure}
					\center
					\begin{subfigure}{0.45\linewidth}
						\buildrplot{figures/cconv-dropped-packets.R}
						
						\caption{CConv}
						\label{fig:cconv-dropped-packets}
					\end{subfigure}
					\begin{subfigure}{0.45\linewidth}
						\buildrplot{figures/parse-dropped-packets.R}
						
						\caption{Parse}
						\label{fig:parse-dropped-packets}
					\end{subfigure}
					
					\caption{Rates of packet dropping in `Nengo SpiNNaker' simulations
					with different placements. Results shown jittered in the `X'
					direction for clarity.}
					\label{fig:nengo-dropped-packets}
				\end{figure}
				
				In the `CConv' neural model when placed using the simulated annealing
				algortihm, 70\% of simulations resulted in fewer than 100 packets being
				dropped per second with 41\% dropping no packets at all. In 95 out of
				the 100 runs of the `CConv' model was placed and routed without
				exhausting any of SpiNNaker's routing tables. In the `Parse' benchmark
				when placed by simulated annealing, 93\% of simulations did not drop
				any packets and all 100 of the 100 simulation runs did not overflow
				SpiNNaker's routing tables.
				
				In both neural models, when placed using each of the baseline placement
				algorithms significant numbers of packets are dropped in most runs. In
				the case of the `CConv' model, all of the baseline placement algorithms
				produce placements which result in packets being dropped. In addition,
				for the ``CConv`` model only 58\%, 7\% and 9\% of placement solutions
				resulted in routes which fit in SpiNNaker's routing tables, for the
				Hilbert, RCM and Random placers respectively.
				
				Dropping 6 million packets per second corresponds with roughly 40\% of
				all packets being dropped. Since packet dropping rates beyond 1\% are
				fatal to the correctness of most simulations.
				
				Unfortunately the `Card Soring' neural simulation was not available for
				use in these experiments.
				
				TODO: MENTION WIGGLY EDGE EXPERIMENT?
				
			\subsubsection{Impact of placement on the `Circuit Simulator'}
				
				The `MU0' CPU simulation built using the `circuit simulator' SpiNNaker
				application produces very little traffic in its default mode of
				operation. The application functions correctly when placed with all
				four placement algorithms.
				
				\begin{figure}
					\center
					\buildrplot{figures/mu0-saturation.R}
					
					\caption{Simulation speeds at which the circuit simulator becomes
					network bound when placed by the baseline placers.}
					\label{fig:mu0-saturation}
				\end{figure}
				
				The circuit simulator may be configured to run at higher speeds,
				directly increasing the quantity of network traffic generated. The
				`MU0' simulation was executed at a range of different execution speeds
				after being placed by each of the four placement algorithms. When
				placed by the simulated annealing algorithm, at 400$\times$ the default
				simulation speed, the simulator becomes CPU bound but does not drop any
				packets. By contrast, when placed by the baseline algorithms, the
				simulator begins to drop packets at lower simulation speeds. Figure
				\ref{fig:mu0-saturation} shows the distribution of maximum simulation
				speeds achieved for each of the 100 runs of the simulator with each
				placer. As in many of the other benchmarks, the hilbert placer performs
				better than the Random and RCM placers with the RCM placer producing
				the placements with the most limited speed.
			
		\subsection{Scalability}
			
			Unfortunately, due to the limited scale of existing SpiNNaker
			applications it is necessary to turn to a synthetic benchmark to
			demonstrate the scaling properties of the simulated annealing placement
			algorithm.
			
			A synthetic application graph consisting of a 2D grid of vertices in
			which each vertex is connected with its neighbours following a 2D
			gaussian probability distribution. This application graph is then placed
			into a hexagonal mesh topology whose aspect ratio matches the application
			graph and which is exactly the right size to contain the whole graph.
			This placement problem may be scaled up by increasing the number of
			vertices or by increasing the number of connections made by each vertex.
			
			Placement quality is judged by comparing the total number of hops
			required to route all nets in the placed application graph against the
			number of hops needed when the vertices are placed `manually' according
			to their location in the 2D grid their nets are defined with. Better
			placements should require a similar number of hops to the manual
			placement while worse placements will require more implying greater
			network congestion.
			
			All experiments were conducted on a cluster of idle workstations with
			3.10~GHz Intel Core-i5-2400 CPUs and 8~GB of RAM.
			
			\subsubsection{Scaling with application graph size}
				
				\begin{figure}
					\center
					\begin{subfigure}{\linewidth}
						\center
						\buildrplot{figures/placement-scalability-size-quality.R}
						
						\caption{Placement overhead (vs. manual placement)}
						\label{fig:placement-scalability-size-quality}
					\end{subfigure}
					
					\vspace*{1em}
					
					\begin{subfigure}{\linewidth}
						\center
						\buildrplot{figures/placement-scalability-size-entries.R}
						
						\caption{Maximum routing table size}
						\label{fig:placement-scalability-size-entries}
					\end{subfigure}
					
					\vspace*{1em}
					
					\begin{subfigure}{\linewidth}
						\center
						\buildrplot{figures/placement-scalability-size-runtime.R}
						
						\caption{Placement algorithm runtime}
						\label{fig:placement-scalability-size-runtime}
					\end{subfigure}
					
					\caption{Scalability with respect to graph size. Lines give mean
					values, bands give range of results.}
					\label{fig:placement-scalability-size}
				\end{figure}
				
				This experiment intends to determine how the runtime and placement
				quality of each algorithm changes as the size of the application graph
				is increased. In this experiment, each vertex is connected to four
				randomly picked neighbours whose distances have a standard-deviation of
				three vertices.  The experiment was repeated ten times for each
				combination of graph size and placement algorithm. The results are
				shown in figure \ref{fig:placement-scalability-size}.
				
				In figure \ref{fig:placement-scalability-size-quality} we can see that
				as the placement problem size grows, existing placement techniques
				quickly begin to produce solutions requiring many more hops to route
				than a `manual' solution. While the quality of the simulated annealing
				approach is reduced for larger graphs, for graphs with \num{1048576}
				vertices, approximately the number in the largest planned SpiNNaker
				machine, only twice as many hops are required to route the placement
				compared with the manual placement.
				
				Figure \ref{fig:placement-scalability-size-runtime} shows the runtime
				of the routing algorithms on each problem size. Once again the
				simulated annealing algorithm's runtime consistantly dwarves the other
				algorithms. Notably, however, the annealing algorithm's runtime grows
				approximately linearly with problem size.
				
				Beyond around quarter of a million vertices, the quality of the
				placements produced by the baseline placement algorithms has dropped
				significantly. Due to the increased length of the routes required,
				routing these placed graphs requires more memory than was available in
				the cluster machines and these results have been omitted.
				
				For the largest graph in the experiment, the runtime of the annealing
				based placer reaches approximately 12~hours. While such long runtimes
				may not be ideal, for very large placement problems, simulated
				annealing produces placements with substantially reduced overhead.
			
			
			\subsubsection{Scaling with application graph size}
			
				\begin{figure}
					\center
					\begin{subfigure}{\linewidth}
						\center
						\buildrplot{figures/placement-scalability-fanout-quality.R}
						
						\caption{Placement overhead (vs. manual placement)}
						\label{fig:placement-scalability-fanout-quality}
					\end{subfigure}
					
					\vspace*{1em}
					
					\begin{subfigure}{\linewidth}
						\center
						\buildrplot{figures/placement-scalability-fanout-entries.R}
						
						\caption{Maximum routing table size}
						\label{fig:placement-scalability-fanout-entries}
					\end{subfigure}
					
					\vspace*{1em}
					
					\begin{subfigure}{\linewidth}
						\center
						\buildrplot{figures/placement-scalability-fanout-runtime.R}
						
						\caption{Placement algorithm runtime}
						\label{fig:placement-scalability-fanout-runtime}
					\end{subfigure}
					
						\caption{Scalability with respect to fan out. Lines give mean values,
						bands give range of results.}
					\label{fig:placement-scalability-fanout}
				\end{figure}
				
				Another factor which can have a significant impact on placement runtime
				is the fan out of the vertices in an application graph. In a second
				experiment, the size of the synthetic application graph is fixed at
				\num{16384} and the fan out of the nets connecting these vertices is
				increased. Again, combination of placement algorithm and fan out was
				tested ten times and the results are shown in figure
				\ref{fig:placement-scalability-fanout}.
				
				As shown in figure \ref{fig:placement-scalability-fanout-quality}, the
				quality of the placements produced does not change dramatically as the
				fan out is increased with the exception of the Hilbert placer which
				appears to perform better for very small or very large fan outs.
				
				In all but the random placer (which does not consider connectivity),
				all of the placement algorithms' runtimes grow in proportion to the fan
				out (figure \ref{fig:placement-scalability-fanout-quality}). Once
				again, simulated annealing is significantly slower than the other
				algorithms but the runtime continues to grow approximately linearly
				with problem size.
