\chapter{Placing applications in large SpiNNaker machines}
	
	In the previous chapter I tackled the problem of scale in generating routes
	for very large networks such as SpiNNaker. In this work the centroid traffic
	pattern was used as an approximation of the expected network traffic
	generated by `well behaved' neural network simulation software running on
	SpiNNaker. The traffic produced largely exhibits strong locality, that is
	most communication occurs between either nearby nodes or clusters of nodes.
	In reality, neural simulation applications are not specified geometrically
	but rather as abstract graphs of communicating neurons
	\cite{davison08,eliasmith13}. Applications must then \emph{place} these
	neurons onto nodes in a SpiNNaker system, attempting maximise communication
	locality.
	
	In this chapter I re-evaluate the suitability of simulated annealing as a
	technique for finding high quality placements for large parallel
	applications. Though this technique had fallen out of fashion in the field of
	application placement by the early 1990s, it has found wide use for placing
	components in computer chip and FPGA designs. In the intervening years,
	placement problems in super computers have grown in size from tens or
	hundreds of nodes to millions, a scale at which chip placement techniques
	were operating in the mid 1990s. I adapt the simulated annealing algorithm
	used by the VPR academic circuit placement software to produce placements for
	applications running on SpiNNaker. In that in a range of real and synthetic
	benchmarks simulated annealing produces high quality placements enabling
	efficient use of SpiNNaker's network resources.
	
	
	%In the field of chip design, Moore's `Law' \cite{moore65,moore75} observes a
	%similar exponential growth in the number of components within a single chip.
	%Today modern processors contain billions of components and an analagous
	%placement problem exists in attempting to place interconnected components
	%near to eachother. In this chapter I explore the techniques used for circuit
	%placement and adapt one such technique, Simulated Annealing (SA)
	%\cite{kirkpatrick83}, for use in application placement. Despite some early
	%interest in SA for application placement in the 1980s and early 1990s, the
	%technique has since fallen out of favour. I find that at the scales of modern
	%placement problems SA-based placement is able to produce solutions of
	%superiour quality to contemporary methods.
	%
	%TODO: SUMMARISE RESULTS...
	
	\section{Related work}
		
		The placement problem has been tackled independently in the literature by
		researchers in both the application and chip placement communities. In this
		survey I cover application and chip placement separately as these two
		communities have remained largely isolated from one another. First I
		explore the techniques applied to application placement before moving on to
		contrast this with the techniques used in circuit placement.
		
		In the application placement literature, the placement problem is often
		referred under the umbrella term `mapping'. Unfortunately term is often
		used more broadly to include other tasks such as routing and application
		partitioning. To avoid ambiguity I use the term `placement', as preferred
		by the chip and FPGA design communities, to refer specifically to the
		problem of assigning nodes in an application's communication graph to nodes
		in a machine's connectivity graph.
		
		\subsection{Application placement algorithms}
			
			TODO: GENERAL INTRO
			
			\subsubsection{Application-specific approaches (manual placement)}
				
				In the case of some applications such as finite element modelling
				\cite{bermejo13}, the structure of the problem itself leads to a
				natural placement of the computation on nodes in a machine. For example
				when simulating a 3D volume in an node super computer with a $3 \times
				4 \times 2$ 3D torus or mesh topology network, the modelled volume
				might be divided into as in figure \ref{fig:fem-partitioning}. Each
				cuboid in the model is then assigned to the corresponding node in the
				network topology.
				
				\begin{figure}
					\center
					\buildfig{figures/fem-partitioning.tex}
					
					\caption{Example partitioning of a 3D space to fit into a super
					computer with a $3\times4\times2$ torus or mesh topology.}
					\label{fig:fem-partitioning}
				\end{figure}
				
				When the number of dimensions in a problem do not match that of the
				underlying network architecture, the common solution is to either
				divide only along a subset of the axes or to divide into additional
				pieces on the existing axes \cite{gilge14}.
			
			\subsubsection{Sequential placement}
				
				In the case where a placement solution is non-obvious one of the
				simplest and most popular strategies is to apply a simple sequential
				placement algorithm. Sequential placement algorithms function by
				iterating over the vertices in the application's communication graph
				and assigning them to a free node in the target machine. Sequential
				placement algorithms are differentiated by the order in which they
				iterate over vertices in the communication graph and fill nodes in the
				target machine. A number of widely used orderings are described below.
				
				\begin{figure}
					\center
					\begin{subfigure}{0.32\linewidth}
						\center
						\buildfig{figures/sequential-row-order.tex}
						\caption{Row-order}
						\label{fig:sequential-row-order}
					\end{subfigure}
					\begin{subfigure}{0.32\linewidth}
						\center
						\buildfig{figures/sequential-alternating.tex}
						\caption{Alternating}
						\label{fig:sequential-alternating}
					\end{subfigure}
					\begin{subfigure}{0.32\linewidth}
						\center
						\buildfig{figures/sequential-hilbert.tex}
						\caption{Hilbert curve}
						\label{fig:sequential-hilbert}
					\end{subfigure}
					
					\caption{Space-filling curves in 2D mesh and torus topologies.}
					\label{fig:sequential}
				\end{figure}
				
				Super computer management software such as SLURM \cite{yoo03} and Blue
				Gene's system software \cite{gilge14} by default na\"ively iterate over
				vertices in an application communication graph in the order they are
				provided. The nodes in the target machine are then iterated over in a
				simple space-filling curve through the network topology. Figure
				\ref{fig:hilbert-placement} illustrates the default patterns available
				in these software packages. The row-order (figure
				\ref{fig:sequential-row-order}) and alternating (figure
				\ref{fig:sequential-alternating}) curves correspond with 2D versions of
				the default node assignment orders used in SLURM and BlueGene systems.
				
				\begin{figure}
					\center
					\buildfig{figures/hilbert-placement.tex}
					
					\caption{A Hilbert curve, coloured from blue to red.}
					\label{fig:hilbert-placement}
				\end{figure}
				
				The Cray extensions to SLURM software provide a Hilbert curve
				\cite{hilbert91} (figure \ref{fig:sequential-hilbert}) node assignment
				order. Unlike the row-order and alternating space filling curves the
				Hilbert curve ensures that pairs of vertices close together in the node
				iteration order are also close together in the target machine's network
				\cite{moon01, zumbusch99}. Figure \ref{fig:hilbert-placement} shows a
				5$^\textrm{th}$-order Hilbert curve where each point in the curve is
				coloured according to its position along the curve. In this figure it
				is possible to see that nearby positions in the curve (which share
				similar colours) are also close in 2D space.
				
				When the proximity of vertices in the vertex-ordering supplied by an
				application is a good estimator of those vertices communication
				requirements, the sequential assignment schemes discussed above can be
				very effective. These techniques have also proven adequate in
				small-scale and densely connected applications such as early neural
				simulations running on prototype SpiNNaker machines with tens of nodes
				\cite{galluppi10} but growing beyond this scale has proven problematic.
				
				\begin{figure}
					\center
					\begin{subfigure}{0.45\linewidth}
						\center
						\buildfig{figures/rcm-initial.tex}
						
						\caption{Original permutation}
						\label{fig:rcm-initial}
					\end{subfigure}
					\begin{subfigure}{0.45\linewidth}
						\center
						\buildfig{figures/rcm-sorted.tex}
						
						\caption{RCM permutation}
						\label{fig:rcm-sorted}
					\end{subfigure}
					
					\caption{Adjacency matrix representation of a graph before and after
					permutation by the RCM algorithm.}
					\label{fig:rcm}
				\end{figure}
				
				A number of algorithms have been proposed for automatically selecting
				good vertex iteration orders, typically using a graph-traversal based
				heuristic. A typical method, described by Hoefler \emph{et al.}
				\cite{hoefler11} exploits the Reverse-Cuthill-McKee (RCM) algorithm
				\cite{cuthill69}. An application's communication matrix is represented
				as an adjacency matrix, $M$, where $M_{i,j}$ is 1 if node $i$ is
				connected by an edge to node $j$ and 0 otherwise. An example matrix is
				illustrated in figure \ref{fig:rcm-initial}. The RCM algorithm uses a
				simple heuristic to permute the matrix (i.e. renumber the nodes in the
				graph) in order to reduce the bandwidth of the matrix. Figure
				\ref{fig:rcm-sorted} shows the RCM-permuted version of the example
				adjacency matrix. When a graph's vertices are ordered as in a
				bandwidth-reduced sparse matrix, vertices close together in the
				ordering are likely to communicate while those further apart tend not
				to communicate.
				
			\subsubsection{Optimisation-based Placement}
				
				% Citations from short report about optimisation in placement...
				% \cite{chen06,jeannot14} and \cite{jeannot10} ("subsets of apps")
				
				In the academic community, a number of attempts have been made to use
				more sophisticated optimisation algorithms for the placement of
				applications. In 1985, Steele \cite{steele85} proposed the use of
				simulated annealing for placing applications in the 6D torus topology
				of the 64 node `Caltech Cosmic Cube' machine. Simulated annealing,
				originally developed by Kirkpatrick \emph{et al.} \cite{kirkpatrick83},
				is a general-purpose optimisation algorithm which works by analogy to
				the physical process of annealing. In brief simulated annealing
				functions by randomly swapping vertices in a candidate placement
				solution, accepting swaps which move connected vertices closer together
				and rejecting some proportion of swaps which move connected vertices
				further apart. The simulated annealing algorithm is described in detail
				later in this chapter.
				
				Towards the end of the 1980s, application placement appeared to be
				becoming less important as super computer network architectures
				improved:
				%
				\begin{displayquote}
					``Careful placement was necessary because of the slow communication
					and non-uniform addressing of early concurrent computers. However,
					the development of message passing machines with fast communications
					and a uniform global address space  has made placement less of an
					issue. In such machines a random placement performs nearly as well as
					an optimum placement.''
					
					\noindent --- W. Dally, 1987 \cite{dally87}
				\end{displayquote}
				%
				In addition, network and problem sizes remained small, so small in fact
				that linear-programming based optimal placement still appeared in
				benchmarks comparing placement algorithms \cite{xu91}. In this
				environment, simpler sequential placement algorithms gained favour over
				more computationally expensive algorithms such as simulated annealing.
				
				As problem and machine sizes have grown and network utilisation has
				once again become an important factor in application performance
				\cite{navaridas09b} more complex optimisation algorithms have
				reappeared in the literature. One popular approach employs graph
				partitioning algorithms such as METIS \cite{karypis98} to perform
				recursive bipartitioning based placement
				\cite{phillips14,hoefler11,pellegrini96}.  This placement process is
				illustrated in figure \ref{fig:partitioning}.
				
				In the first step, the application communication graph and machine
				connectivity graph are bipartitioned such that the number of edges
				between partitions is minimised. Each half of the communication graph
				is associated with one of the halves of the machine connectivity graph.
				The partitioning process is then repeated recursively on each of the
				two communication and connectivity graph pairs. The process halts when
				the graphs can no longer be partitioned at which point the vertices in
				the communication graph are placed on their associated node.
				
				\begin{figure}
					\center
					\buildfig{figures/partitioning.tex}
					
					\caption{Illustration of application placement by recursive
					partitioning.}
					\label{fig:partitioning}
				\end{figure}
				
				TODO: PARTITIONING IS GREAT AND ALL BUT QUALITY ISN'T ALWAYS GREAT AND
				IT DOESN'T DEAL WELL WITH MULTI-CONSTRAINT SCENARIOS E.G. PROCESSOR AND
				MEMORY RESTRICTIONS.
				
				Unfortunately, many of these simply aren't suited to the scale of
				neural applications running on SpiNNaker (e.g. only cope with tens of
				nodes while SpiNNaker may contain hundreds of thousands).
				
				Additionally, a number of algorithms have been developed which make
				assumptions about the topologies of the problem or network. Tree match
				for example attempts to map tree-shaped problems to tree-shaped
				networks. Such algorithms can be highly effective but again do not
				apply to SpiNNaker or its neural applications.
		
		\subsection{Chip placement algorithms}
			
			The chip-design industry has, for many years, dealt with problems
			analogous to the task of placing super computer jobs in a way suited to
			SpiNNaker. Modern CPUs have millions or billions of components with
			strictly fixed connectivity. CPU designers must place each of these onto
			a chip such that the connection lengths are controlled to reduce
			congestion and increase performance. As such, these algorithms are
			ideally suited to future super computer placement work since they already
			operate at the scales required \cite{nam07}.
			
			\subsubsection{Cost functions}
				
				HPWL is popular but a bit crap for high fan-outs. It is, however, quite
				simple.
				
				TODO: SELECT A BETTER COST FUNCTION...
			
			\subsubsection{Simulated annealing}
				
				One of the oldest techniques used for circuit placement is simulated
				annealing and this remains popular today thanks to its sheer
				versatility (see VPR, other open FPGA tools).
				
				SA works by analogy with the physical process of annealing.
				The simulated annealing algorithm works by selecting random pairs of
				components on a chip, swapping them and evaluating some cost function.
				If the swap reduces the cost function, it is kept, if not, depending on
				a function of the current temperature and the cost introduced by the
				swap.
				
				TODO: ILLUSTRATION OF SIMULATED ANNEALING SWAP OPERATION
				
				By occasionally allowing costly swaps, the annealing algorithm avoids
				becoming trapped in local minima. As the algorithm proceeds, the
				temperature is slowly reduced and with it the proportion of costly
				swaps which are retained. This causes the placement to move from
				exploration early on towards refinement later on.
				
				The temperature schedule of an annealing algorithm is critical to its
				success. In general these schedules are computed based on the
				performance of the algorithm as it runs. In VPR the following schedule
				is used.
				
				TODO: DESCRIBE VPR'S SCHEDULE
				
				TODO: FIND AND DESCRIBE ALTERNATIVE SCHEDULE?
				
				Unfortunately, SA is very difficult to parallelise, especially in the
				case of placement. As a result, its scalability has been limited and
				resulted in significantly reduced usage in recent work.
			
			\subsubsection{Partitioning placement}
				
				Partitioning based placement solves the placement problem using
				graph-partitioning recursively on the problem graph to assign each part
				of the circuit to some area in the super chip. Though a number of
				algorithms have proven successful in academic placement contests over
				the years, they are not popular in industrial settings.
			
			\subsubsection{Analytical placement}
				
				In analytical placement, cost function for the circuit graph is
				approximated in a form which is amenable to solutions with standard
				numerical or symbolic algebraic techniques. Using these techniques,
				exact minimum cost (in terms of the approximation) configurations can
				be obtained.
				
				Quadratic placement is a popular analytical placement technique which
				approximates the cost of a placement as the sum of the squares of the
				distances between connected circuit elements.
				
				TODO: FIGURE EXAMPLE QUADRATIC PLACEMENT PROBLEM AND SOLUTION
				
				As such this gives a quadratic cost function like so which we must
				minimise.
				
				TODO: QUADRATIC COST EQN
				
				To minimise the function we differentiate and solve using simple
				symbolic manipulation.
				
				TODO: QUADRATIC COST SOLUTION
				
				Unfortunately, quadratic placement doesn't contain any congestion
				relief by default so various schemes exist. For example, extra anchor
				nodes are inserted which gently pull the circuit components apart from
				each other. As a result, the algorithm generally proceeds by iterating,
				regenerating anchors each time.
				
				Other non-quadratic analytical methods exist too with numerical
				solutions. The approaches are often similar.
			
			\subsubsection{Hierarchical clustering}
				
				Many placement algorithms scale super-linearly with problem size and so
				larger problems become increasingly problematic to handle. To solve
				this problem clustering techniques are first applied to first simplify
				the placement problem. A solution is then found at the coarse level and
				then hierarchically fleshed out.
				
				Various clustering algorithms are in use.
				
				TODO: TALK ABOUT CLUSTERING IN PLACEMENT...
				
				TODO: DESCRIBE THE ALGORITHM I IMPLEMENTED.
	
	\section{Application placement by simulated annealing}
		
		\label{sec:placement-by-annealing}	
		
		I have implemented a simplified SA based application placement algorithm
		based on the approach used in the popular VPR place and route tool chain.
		The algorithm is written in C and is optimised for experimentation rather
		than performance but is production-ready. It has been integrated into the
		`Rig' SpiNNaker software tools and has been used to place very large
		simulations. More on that later.
		
		\subsection{Representation}
			
			Model each chip as having a quantity of various resources (e.g. Cores,
			SDRAM) available. The application graph consists of vertices which each
			consume some quantity of these resources. Vertices must be placed on a
			single chip such that the resources required on a given chip do not
			exceed those available. Vertices are then interconnected by 1:N nets with
			weights which act as hints. The nets are treated as a soft constraint:
			vertices connected via a net will, ideally, be placed near to each other,
			with priority being given to nets with higher weights. Additionally there
			will be a list of placement constraints (see later).
			
			A key observation is that while vertices in an application may frequently
			have a 1:1 correspondence with application cores, this need-not be the
			case. For example, a vertex may represent a block of SDRAM which is
			shared. A vertex may also represent some other resource, for example,
			external IO availability. By making these resource types user-defined,
			applications programmers can express flexible hard-constraints on their
			application.
			
			Another observation is that generic soft constraints can be expressed may
			be expressed using a net with an appropriate weight.
			
			As a result of these facilities, application programmers can easily
			express their own application-specific hard and soft placement
			constraints without having to modify the algorithm. This representation
			has become a de-facto standard for placement problem interchange for
			SpiNNaker applications.
		
		\subsection{Cost function}
			
			At present I've used HPWL despite this being really bad for high-fan-out
			multicast and totally ignorant to the hexagonal nature of SpiNNaker...
			
			To compute bounding boxes for tori I use the following approach. For each
			dimension, sort the points on that dimension and find the largest gap
			between them on a ring. The bounding box goes the other way.
			
			TODO: FIGURE ILLUSTRATING BOUNDING BOX COMPUTATION FOR TORI.
		
		\subsection{Annealing schedule}
			
			The annealing schedule is that used by VPR. Despite being for circuit
			placement, it seems to work jolly well.
			
			TODO: DESCRIBE AND RATIONALISE THE SCHEDULE
		
		\subsection{Constraint handling}
			
			Various hard and soft constraints may be expressed by software
			approaches. For each we explain how they may be handled by the placement
			algorithm:
			
			\subsubsection{Location Constraint}
				
				The vertex is placed on a chip and removed from the set of movement
				candidates.
			
			\subsubsection{Same-chip constraint}
				
				When two vertices must always be placed on the same chip they are
				simply combined into one vertex which consumes the sum of their
				resources. Placement then treats them as one chip and thus is forced to
				atomically place the vertices.
			
			\subsubsection{Reserve resource constraint}
				
				Simply reduce resource availability on that chip.
			
			\subsubsection{Keep near Ethernet}
				
				Simply add a net.
	
	\section{Evaluation}
		
		\label{sec:placement-results}
		
		Though benchmarks exist for super computer loads and chip placement tasks,
		such things don't exist for neural applications. As a result I use a
		selection of real applications for SpiNNaker along with some synthetic
		benchmarks based on biological data.
		
		\subsection{Benchmark networks}
			
			First some real networks.
			
			Some nengo networks: SPAUN: `The world's largest functional brain model'.
			Word-net network from Jamie: Example of some learning.
			
			TODO: DESCRIBE SHAPE OF NENGO NETWORKS
			
			Some PyNN networks: Microcortical column model from PyNN. Note almost
			broadcast connectivity but varying weights. Try and extract a vision
			netlist from Anna. Maybe try and get a netlist for Tom's barrel cortex.
			
			Now for some artificial networks. Pipeline, noisy pipeline, mesh,
			Gaussian 2D.
		
		\subsection{Experiments}
			
			We compare random, linear, greedy and annealing based placement
			approaches to placement. We compare static metrics (such as mean/max
			congestion, table usage) along with experiments based on simulated
			network traffic in real hardware. Network Tester generates artificial
			traffic in proportion with the weights given for each model. We compare
			the relative level of traffic sustainable. We also consider use of
			machines of various sizes.
		
		\subsection{Results}
			
			SA placement is slow but rather effective, especially for some networks.
			Generally worth doing. Will need to be sped up for very large machines...
			
			TODO: EXPERIMENTS!
	
