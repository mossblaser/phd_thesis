\chapter{Placing applications in large SpiNNaker machines}
	
	\label{sec:placement}
	
	In the previous chapter I tackled the problem of fault tolerant route
	generation for SpiNNaker's network. The centroid traffic pattern was used as
	an approximation of the expected network traffic generated by `well behaved'
	neural network simulation software running on SpiNNaker. This traffic pattern
	exhibits locality with most communication occurring between adjacent nodes.
	In reality, neural simulation applications are not conveniently specified
	geometrically but rather as abstract graphs of communicating
	neurons~\cite{davison08,eliasmith04}. Though neural models often exhibit
	localised connectivity this can only be exploited if they are laid out
	sensibly within a network. Producing optimal placements is an NP-complete
	problem~\cite{hoefler11}.
	
	In this chapter I evaluate the effectiveness of simulated
	annealing~\cite{kirkpatrick83} for placement of SpiNNaker applications.
	Though simulated annealing fell out of favour for use in application
	placement in the 1990s, I argue that it is time to re-evaluate its
	suitability for modern placement problems. Looking to the field of chip
	design, where circuit placement problems have also been growing in scale
	exponentially for the past few decades, simulated annealing was considered
	ideal for placement problems whose sizes are comparable to that of modern
	super computer placement problems.
	
	In experiments on existing SpiNNaker applications I find that simulated
	annealing achieves placement solutions substantially better than contemporary
	application placement techniques. In some cases simulated annealing is shown
	to make the difference between an application functioning correctly and not
	at all. Using larger scale synthetic benchmarks I have also demonstrated that
	my proof-of-concept simulated annealing based placement algorithm scales up
	to placement problems containing over one million vertices while achieving
	placement quality similar to an ideal placement.
	
	I also build upon the work on hexagonal torus topologies earlier in this
	thesis to develop an efficient placement cost estimation function for use in
	the kernel of the annealing algorithm.
	
	\section{Related work}
		
		Attempts to address the problem of placing applications in super computer
		networks have utilised a variety of approaches ranging from simple greedy
		algorithms to sophisticated combinatorial optimisation
		algorithms~\cite{jeannot14}. In the parallel field of digital circuit
		placement for chip and FPGA design, placement algorithms have also received
		intense study.  In this section I survey the work carried out on
		application placement before exploring chip design techniques to see what
		can be learnt from this perhaps more advanced field.
		
		In the application placement literature, the placement problem is often
		referred to by the umbrella term `mapping'. Unfortunately `mapping' is
		often used more broadly to include other tasks such as routing and
		application partitioning. To avoid ambiguity I use the term `placement', as
		preferred by the chip and FPGA design communities, to refer specifically to
		the problem of assigning vertices in an application's communication graph
		to nodes in a machine's connectivity graph: the focus of this chapter.
		
		\subsection{Application placement algorithms}
			
			Below I consider several popular techniques employed by existing super
			computer applications ranging from application-specific manual placement
			to sophisticated general purpose algorithms.
			
			\subsubsection{Application-specific approaches (manual placement)}
				
				\begin{figure}
					\center
					\buildfig{figures/fem-partitioning.tex}
					
					\caption[Partitioning of a 3D volume for a $3\times4\times2$ torus.]%
					{Example partitioning of a 3D volume to fit into a super
					computer with a $3\times4\times2$ torus or mesh topology.}
					\label{fig:fem-partitioning}
				\end{figure}
				
				In certain applications such as finite element
				modelling~\cite{bermejo13}, a problem's structure defines a `natural'
				placement of the problem onto the target machine. For example when
				simulating a 3D volume in a super computer with a $3 \times 4 \times 2$
				3D torus or mesh topology network, the modelled volume might be divided
				as in figure~\ref{fig:fem-partitioning}. Each cuboid in the model is
				then assigned to the corresponding node in the network topology.
				
				Unfortunately, this technique only applies to a very narrow class of
				applications and is not applicable to many of the types of neural
				models SpiNNaker is designed to run.
			
			\subsubsection{Sequential placement}
				
				In the case where a placement solution is non-obvious, one of the
				simplest and most popular strategies is to apply a simple sequential
				placement algorithm. Sequential placement greedily places the vertices
				of an application's communication graph onto nodes. By carefully
				selecting the order in which vertices are picked and nodes assigned in
				the target architecture, this algorithm can produce reasonable quality
				placements.
				
				\begin{figure}
					\center
					\begin{subfigure}{0.32\linewidth}
						\center
						\buildfig{figures/sequential-row-order.tex}
						\caption{Row-order}
						\label{fig:sequential-row-order}
					\end{subfigure}
					\begin{subfigure}{0.32\linewidth}
						\center
						\buildfig{figures/sequential-alternating.tex}
						\caption{Alternating}
						\label{fig:sequential-alternating}
					\end{subfigure}
					\begin{subfigure}{0.32\linewidth}
						\center
						\buildfig{figures/sequential-hilbert.tex}
						\caption{Hilbert curve}
						\label{fig:sequential-hilbert}
					\end{subfigure}
					
					\caption{Space-filling curves in 2D mesh and torus topologies.}
					\label{fig:sequential}
				\end{figure}
				
				Super computer management software such as SLURM~\cite{yoo03} and Blue
				Gene's system software~\cite{gilge14} by default na\"ively iterate over
				vertices in an application communication graph in the order in which
				they are defined by the application. The nodes in the target machine
				are iterated over using a simple space-filling curve.
				Figure~\ref{fig:sequential} illustrates three popular space filling
				curves.
				
				The row-order (figure~\ref{fig:sequential-row-order}) and alternating
				(figure~\ref{fig:sequential-alternating}) curves illustrate 2D versions
				of the default node assignment orders provided by the SLURM and
				BlueGene systems.  The Cray extensions to SLURM software provide a
				Hilbert curve~\cite{hilbert91} node assignment order
				(figure~\ref{fig:sequential-hilbert}). The Hilbert curve specifically
				attempts to maintain the property that points that are close together
				along the length of the curve are also close together in the 2D
				mapping~\cite{moon01, zumbusch99}. Since the vertex iteration order is
				also chosen such that connected vertices tend to be closer together,
				this property may lead to improved placement solutions.
				
				\begin{figure}
					\center
					\buildrplot{figures/space_filling_curves_comparison.R}
					
					\caption[Relationship between space filling curve offset and 2D space.]%
					{Relationship between distance along a space filling curve
					and between the 2D positions assigned. Results shown for
					Manhattan distances in a $32\times32$ 2D mesh.}
					\label{fig:space_filling_curves_comparison}
				\end{figure}
				
				Figure~\ref{fig:space_filling_curves_comparison} illustrates how the
				distance between locations along each space filling curve map to
				distances in the resulting 2D space. Both the row-order and alternating
				orderings result in distances oscillating between being closer and
				spread out in 2D space, just as the space filling curve oscillates from
				side-to-side. For the Hilbert curve, however, distances grow more
				monotonically\footnote{The growth in distance for Hilbert curves may
				appear slightly random but in fact forms a fractal as a result of the
				fractal construction of the Hilbert curve itself.} and at smaller 1D
				distances, 2D distances are often lower than in other space filling
				curves.
				
				A number of algorithms have been proposed for automatically selecting
				good vertex and chip iteration orders, typically using a
				graph-traversal based heuristic. An example typical of this type of
				technique is `graph similarity based mapping' described by Hoefler and
				Snir~\cite{hoefler11}. This algorithm exploits the Reverse
				Cuthill-McKee (RCM) algorithm~\cite{cuthill69} to select a linear
				ordering of vertices in which connected vertices tend to appear close
				together. Internally the RCM algorithm uses a breadth-first-search-like
				algorithm to order vertices.
				
				The RCM algorithm was originally designed as a tool for reducing the
				`bandwidth' of matrices to improve the performance of linear algebra
				techniques. A matrix's bandwidth is the width of strip of non-zero
				elements on its diagonal. Figure~\ref{fig:rcm} illustrates how a sparse
				matrix is permuted by the RCM algorithm to produce a matrix with lower
				bandwidth. This technique may be applied to a graph by representing the
				graph as an adjacency matrix, $M$, where $M_{i,j}$ is $w$ if node $i$
				is connected by an edge to node $j$ with weight $w$ and 0 otherwise.
				When a graph's adjacency matrix is permuted by the RCM algorithm, the
				connectivity of the graph remains unchanged but the vertex ordering
				changes. The ordering of vertices in a low-bandwidth adjacency matrix
				tends to keep connected vertices closer together in the ordering making
				the ordering well suited for use with a sequential placement algorithm.
				
				\begin{figure}
					\center
					\begin{subfigure}{0.45\linewidth}
						\center
						\buildfig{figures/rcm-initial.tex}
						
						\caption{Original permutation}
						\label{fig:rcm-initial}
					\end{subfigure}
					\begin{subfigure}{0.45\linewidth}
						\center
						\buildfig{figures/rcm-sorted.tex}
						
						\caption{RCM permutation}
						\label{fig:rcm-sorted}
					\end{subfigure}
					
					\caption[Graph adjacency matrix before and after RCM permutation.]%
					{Adjacency matrix representation of a graph before and after
					permutation by the RCM algorithm.}
					\label{fig:rcm}
				\end{figure}
				
				In small-scale and densely connected applications such as early neural
				simulations running on prototype SpiNNaker machines~\cite{galluppi10},
				these techniques have proven sufficient.  As my experiments demonstrate
				later, however, sequential placement is insufficient for some SpiNNaker
				applications.
				
			\subsubsection{Simulated annealing}
				
				\label{sec:application-placement-summary}
				
				In the academic community, a number of attempts have been made to use
				sophisticated optimisation algorithms for the placement of
				applications. In 1985, Steele~\cite{steele85} first proposed the use of
				simulated annealing for placing applications in the 6D torus topology
				of the 64 node `Caltech Cosmic Cube'. The proposed algorithm is
				described below.
				
				Initially, the vertices in the application's communication graph are
				assigned randomly to nodes in the machine. Two vertices are then chosen
				at random and swapped.  If this swap reduces the `cost' of the
				placement (as defined later), the vertices are kept in their new
				locations. If the cost increases as a result of the swap, the vertices
				are moved back to their original locations with a probability dependent
				on the magnitude of the cost increase and the current `temperature'.
				
				The process of picking and swapping vertices is repeated many times
				while the temperature is gradually reduced. Initially the temperature
				is set to a large value causing most swaps to be accepted, regardless
				of their cost. As the algorithm proceeds, the temperature is gradually
				reduced until only swaps resulting in cost reductions are accepted. By
				initially accepting some apparently detrimental swaps, the algorithm
				typically avoids becoming trapped in local minima. Towards the end of
				the algorithm's execution the solution is presumed to be close to the
				global minimum-cost solution and so few or no detrimental swaps are
				accepted.
				
				The cost of a particular placement is defined by some expression
				approximating the quality of the placement with better quality
				placements having lower-costs. In Steele's work, the cost of a
				placement is computed by finding the shortest path routes between all
				connected vertices using Dijkstra's algorithm. Rather than repeating
				this relatively expensive computation after each swap, Steele's
				implementation updated the previously calculated cost by recomputing
				only the routes connecting the swapped vertices.
				
				The number of swaps to attempt and the rate at which the temperature is
				changed is dictated by an `annealing schedule' and can have a major
				impact on the execution time of the algorithm and the quality of the
				placement solution. I describe one such schedule in detail later in
				this chapter when describing my own simulated annealing based
				application placement algorithm.
				
				Towards the end of the 1980s, application placement appeared to be
				becoming less important as super computer network architectures
				improved, appearing to render computationally intensive placement
				algorithms unnecessary~\cite{dally87}.  In addition, network and
				problem sizes remained small: so small in fact that exact
				linear-programming based placement algorithms remained a meaningful
				point of comparison in application placement research~\cite{xu91}. In
				this environment, simpler sequential placement algorithms gained favour
				over more computationally expensive algorithms such as simulated
				annealing.
				
				In contemporary application placement algorithms, `low temperature'
				simulated annealing algorithms are occasionally employed as a
				post-processing step for by other placement
				algorithms~\cite{hoefler11}, preferring other heuristics to generate an
				initial placement solution.
				
				% Outside of badly attempting 90s scale problems at least... \cite{chen06}
			
			\subsubsection{Recursive partitioning}
				
				As problem and machine sizes have grown and network utilisation has
				once again become an important factor in application
				performance~\cite{navaridas09b}, more complex optimisation algorithms
				have reappeared in the literature. One popular approach employs graph
				partitioning algorithms such as METIS~\cite{karypis98} to perform
				recursive bipartitioning based
				placement~\cite{phillips14,hoefler11,pellegrini96}.  This placement
				process is illustrated in figure~\ref{fig:partitioning}.
				
				\begin{figure}
					\center
					\buildfig{figures/partitioning.tex}
					
					\caption[Application placement by recursive partitioning.]%
					{Illustration of application placement by recursive
					partitioning. Each node has the capacity to hold two vertices.}
					\label{fig:partitioning}
				\end{figure}
				
				In the first step, the application communication graph and machine
				connectivity graph are each bipartitioned such that the number of edges
				cut by the partitions is minimised while the size of each partition
				remains similar. Each half of the communication graph is associated
				with one of the halves of the machine connectivity graph.  The
				partitioning process is then repeated recursively further partitioning
				the application and machine graphs. The process halts when either graph
				can no longer be partitioned.  The vertices in the communication graph
				are then placed on their associated node in the connectivity graph.
				
				Unfortunately recursive partitioning-based techniques are known to
				perform sub-optimally when the number of nodes is not a power of
				two~\cite{simon97}. In addition, placement problems are often subject
				to multiple orthogonal constraints. For example a vertex in an
				application's communication graph may consume a certain number of
				processors and some quantity of SDRAM. Partitioning algorithms require
				a metric with which the size of each partition may be measured to
				ensure the two halves are balanced. When multiple orthogonal resources
				exist, a single metric is insufficient to express this requirement. As
				a result, partitioning algorithms are of limited use in applications
				whose placement is subject to several orthogonal requirements, such as
				neural modelling software on SpiNNaker.
				
			\subsubsection{Topology-specific approaches}
				
				Other approaches employ other optimisation techniques that are
				specialised for use with applications and networks with specific
				connectivity patterns, for example trees~\cite{jeannot14,traff02}.
				Unfortunately these algorithms cater neither for SpiNNaker's hexagonal
				torus topology nor the range of communication topologies found in its
				target applications.
	
		\subsection{Circuit placement algorithms}
			
			The circuit placement problem in chip and FPGA design is similar to that
			of application placement: communicating components must be assigned
			positions in a chip so that wiring congestion is controlled.
			
			\begin{figure}
				\center
				\buildrplot{figures/top500-num-processors.R}
				
				\caption[Historical core counts for the `Top500' super computers.]%
				{Historical processor (core) counts for the `Top500' super
				computer installations. The line shows the mean number of processsors and
				grey ribbon indicates the range. Data reported by the `Top500
				List'~\cite{meuer16j}.}
				\label{fig:top500-num-processors}
			\end{figure}
			
			Following the march of Moore's `Law'~\cite{moore75}, the number of
			components in modern chips and FPGAs has been increasing at an
			exponential rate similar to that of core counts of super computers (see
			figure \ref{fig:top500-num-processors}). Today, in 2016, the largest
			chips contain billions of transistors, compared with millions of
			processors found in super computers. In terms of the raw number of
			processors, the world's largest super computers are at a scale comparable
			with the number of components in chips designed fifteen years ago. The
			mean size of today's `Top 500' super computers is somewhat smaller still,
			similar in scale to chips designed over 20 years ago.
			
			In view of of the similarity of the chip and application placement
			problems and the relative lead the field of chip placement has in terms
			of placement size, it appears reasonable to look to chip placement to
			suggest future application placement techniques.
			
			The popular approaches to circuit placement can be grouped into three
			broad categories~\cite{kahng11}:
			
			\begin{description}
				
				\item[Partitioning-based] As in application placement, these algorithms
				function by recursively partitioning the circuit to assign
				related components to areas of a chip.
				
				\item[Analytical] These algorithms approximate the placement problem as
				a simpler problem to which exact solutions can be computed
				inexpensively.
				
				\item[Simulated annealing] As in application placement, simulated
				annealing algorithms selectively apply random permutations to a
				placement causing placement quality to improve.
				
			\end{description}
			
			I briefly introduce each technique before assessing its suitability for
			use in application placement in SpiNNaker. The interested reader is
			encouraged to refer to `VLSI physical design: from graph partitioning to
			timing closure' by Kahng \emph{et al.}~\cite{kahng11} for a more thorough
			introduction to the field of circuit placement and the techniques that I
			summarise here.
			
			\subsubsection{Partitioning-based placement}
				
				Partitioning based placement has a long history with early examples
				dating back to the 1970s~\cite{breuer77}. As in recursive partitioning
				based application placement algorithms, the input circuit is
				partitioned alongside the target topology to assign placements. Again,
				graph partitioning software such as METIS~\cite{karypis98} is used to
				partition the circuits but since the target topology is the 2D surface
				of a chip, simple geometric partitioning is used to divide the surface
				of the chip. Modern partitioning based placers such as
				Capo~\cite{roy05} employ additional heuristics such as `terminal
				propagation' where connections between adjacent partitions are
				considered to attempt to align components on either side of a cut.
				
				Unfortunately, as with its use in application placement, these
				partitioning techniques are not ideal.  Consequently, partitioning
				based placement techniques have largely been superseded in chip
				design~\cite{markov15}.
			
			\subsubsection{Analytical placement}
				
				The current generation of placement algorithms attempt to approximate
				placement problems as similar problems with cheaply computed,
				closed-form solutions. One of the common trade-offs of these approaches
				is modelling the components to be placed as point-like objects. As the
				number of components in digital circuits has grown, this approximation
				improved, resulting in analytical placement becoming the most popular
				approach in use today~\cite{markov15}.
				
				One popular analytical technique is `quadratic
				placement'~\cite{kahng11,spindler08}. In this algorithm the components
				in a circuit are approximated as infinitesimal objects with wires
				between them modelled as spring-like forces. Unlike physical springs,
				which obey Hooke's law, the force exerted by a connection is
				proportional to the \emph{square} of the distance between its
				terminals.
				
				\begin{figure}
					\center
					\buildfig{figures/quadratic-placement.tex}
					
					\caption[An optimal 1D quadratic placement solution.]%
					{An optimal 1D quadratic placement solution for two fixed
					vertices ($f_1$ and $f_2$), two movable vertices ($m_1$ and $m_2$)
					and three weighted edges.}
					\label{fig:quadratic-placement}
				\end{figure}
				
				To illustrate this approach, consider the simple one-dimensional
				placement problem presented in figure~\ref{fig:quadratic-placement}.
				In the quadratic placement technique, the cost of a placement is
				modelled as the weighted sum of the squares of the distances between
				connected components. In this case this yields the expression:
				%
				\begin{equation*}
					%
					\textrm{Cost} = 1(f_1 - m_1)^2 + 2(m_1 - m_2)^2 + 1(m_2 - f_2)^2
					%
				\end{equation*}
				%
				Where $f_1$, $f_2$, $m_1$ and $m_2$ represent the positions of the four
				components. An optimal placement is one which minimises this function.
				To find values which minimise a quadratic, the equation is
				differentiated yielding a system of linear equations. This system of
				equations is then solved to find the positions of the movable vertices
				which minimise the quadratic cost function.
				
				Quadratic placement is extended to two or more dimensions by simply
				applying the process for each dimension separately. Unfortunately, a
				straight-forward extension to non-euclidean geometries such as
				hexagonal toruses is non-trivial. For example, quadratic placement
				relies on cost increasing monotonically as two points are moved in
				opposite directions, a property which does not apply to torus
				topologies. This same problem also applies to the approximations used
				by other analytical techniques.
				
				An additional challenge posed by quadratic placement is that some
				vertices' locations must be fixed to provide a `spreading'
				force preventing degenerate solutions where all vertices are given the
				same coordinates. In circuit placement, the locations of `pads' used to
				connect components in a circuit to the outside world are typically
				fixed and provide convenient anchors during placement. Algorithms such
				as SimPL~\cite{kim12b} employ a combination of analytical techniques
				to add virtual `anchor' components which prevent degenerate
				solutions from forming. Unfortunately this approach further adds to the
				difficulty of porting the technique to non-euclidean geometries. These
				techniques also work better for larger circuit sizes making analytical
				techniques non-ideal for today's relatively small (by chip placement
				standards) super computer placement problems.
				
				% TODO: mention viswanathan07 FastPlace 3.0 -- multilevel technique?
			
			\subsubsection{Simulated annealing}
				
				Simulated annealing was originally developed specifically to be applied
				to the problem of circuit placement~\cite{kirkpatrick83}. Throughout
				the late 1980s and early 1990s simulated annealing was the dominant
				circuit placement technique when the first chips breaching one million
				components appeared~\cite{betz97,sechen85}. Due to the increased
				effectiveness of quadratic placement as problems grew beyond tens of
				millions of nodes, simulated annealing has fallen out of fashion as a
				circuit placement technique. Despite this, various efforts continue to
				be made to enhance the simulated annealing process' performance by
				attempting to partition the input problem into several independent
				problems~\cite{choong10,haldar00} and parallelise the
				algorithm~\cite{ludwin08}.
				
				Simulated annealing notably remains popular as a stand-alone placement
				tool in niche applications where flexibility is important. For example,
				the flexible, multi-FPGA, Verilog-To-Routing (VTR) software
				suite~\cite{luu14} and Open Source Arachne-PNR~\cite{cseed} use
				simulated annealing to produce placements within multiple FPGA
				architectures.
				
				Since application placement problems are only now beginning to reach
				the scales at which simulated annealing became popular in circuit
				design, it is likely to be a good choice for application placement over
				the next few years. In addition, simulated annealing's flexibility
				potentially makes it a good choice to handle the wide variety of
				architectures in use by modern super computers.
	
	\section{Placement by simulated annealing}
		
		\label{sec:placement-by-annealing}	
		
		As we have seen, existing software placement techniques are not well suited
		to the large-scale applications projected for the SpiNNaker architecture.
		In the field of circuit placement, various algorithms have been proposed
		which are explicitly designed to cope with placement problems of similar or
		larger sizes. While analytical placement algorithms can produce high
		quality placement solutions for extremely large problems, the current --
		and near future -- generations of application placement problems are still
		too small. Simulated annealing, however, has a proven record in circuit
		design at the scale of modern application placement problems. In addition,
		the flexibility of simulated annealing makes it easy to adapt to new
		network architectures and, in this case, SpiNNaker's hexagonal torus
		topology.
		
		To demonstrate the suitability of simulated annealing for modern
		applications I have implemented a minimal simulated annealing based placer
		for the SpiNNaker architecture. Compared with previous simulated annealing
		algorithms for application placement, this algorithm is intended to cope
		with much larger problem instances and draws inspiration from the
		techniques used by circuit placement annealing algorithms.
		
		In contrast with circuit placement algorithms built on simulated annealing,
		my implementation represents the problem differently to handle the
		nuances of application placement. In circuit simulation, for example,
		exactly one component may be placed in a given space while in application
		placement, several applications may be placed on the same node (e.g. on
		different cores, or as separate processes). These differences, along with
		details of the cost functions and annealing schedule used are described in
		the remainder of this section.
		
		\subsection{Representation}
			
			A typical SpiNNaker application is made up of a number of processes which
			are executed by individual processor cores, consuming some quantity of
			on-chip resources such as shared SDRAM. These processes communicate via
			multicast packets which are sent via SpiNNaker's network.
			
			In the proposed placement algorithm, an application is defined as a graph
			where the `vertices' represent the processes as defined above and the
			multiedges, referred to here as `nets', represent directed flows of
			multicast packets from one vertex several others. Each vertex consumes
			some quantity of on-chip resources (typically one core and a number of
			bytes of on-chip RAM). The placement algorithm must assign each vertex to
			a specific SpiNNaker chip (i.e. a node in SpiNNaker's network) such that
			the resources available on that chip are not exhausted.
			
			% TODO: Include remark like the following?
			% Unfortunately, since multiple resource types must be considered, that
			% application placement additionally encompasses a bin packing problem
			% \cite{korte06}.
			
			An additional soft constraint is imposed: the placement algorithm should
			attempt to place connected vertices as close together as possible. Nets
			may be annotated with a `weight' which provides a hint to the placer
			indicating the relative importance of a particular net. A larger weight
			indicates the placer should try harder to keep the vertices it connects
			closer together. Because this constraint is soft, the placer should make
			a `best effort' attempt to obey the constraint but is not obliged to
			achieve a perfect placement solution.
			
			The SpiNNaker topology is represented as a 2D hexagonal torus topology
			with homogeneous inter-chip links. Each chip is defined to have a certain
			quantity of each resource consumed by vertices. For example, SpiNNaker
			chips nominally contain 17 application processors and 128~MB of on-chip
			SDRAM. Some chips, however, may have fewer working cores while others may
			be non-functional, thus effectively have no resources.
			
			\subsubsection{Additional constraints}
				
				Real-world applications may also apply some additional constraints on
				placement solutions. Two such constraints are considered here.
				
				Some applications require that certain vertices are always placed on a
				specific chip, for example to enable communication with an external
				device attached directly to a SpiNNaker chip. The placement algorithm
				must obey this requirement, treating constrained vertices as
				special-cases.
				
				In other applications, in-memory communication may be required between
				certain vertices and therefore these vertices must always be placed on
				the same chip. This constraint may be implemented by pre-processing the
				application graph such that constrained vertices are merged into a
				single vertex whose resource requirements are the sum of the original
				vertices.
		
		\subsection{Algorithm description}
			
			As outlined earlier, simulated annealing swaps pairs of vertices keeping
			beneficial swaps as well as some proportion of detrimental swaps. In the
			remainder of this section I describe the exact processes by which
			vertices are selected and swapped. I also consider the choice of cost
			function for use with a hexagonal torus topology before specifying the
			annealing schedule used.
		
			\subsubsection{Generating candidate swaps}
				
				In conventional simulated annealing based placement schemes, vertices
				are moved by picking random pairs of vertices and swapping them. In
				application placement problems where vertices may differ in resource
				requirements it is not possible, in general, to swap arbitrary pairs of
				vertices. For example, if a `small' vertex is swapped with a `larger'
				one, the larger vertex may not fit in the space left behind by the
				small vertex.
				
				\begin{figure}
					\center
					\begin{subfigure}{\linewidth}
						\center
						\buildfig{figures/sa-swap-select.tex}
						\caption{A random vertex, $v_a$ is selected along with a random
						target chip (right).}
						\label{fig:sa-swap-select}
					\end{subfigure}
					
					\vspace*{1em}
					
					\begin{subfigure}{\linewidth}
						\center
						\buildfig{figures/sa-swap-remove.tex}
						\caption{Vertex $v_a$ is removed from its chip. Vertices are removed
						from the target chip until enough space is available to fit $v_a$.
						In this example, vertices $v_b$ and $v_c$ are removed.}
						\label{fig:sa-swap-remove}
					\end{subfigure}
					
					\vspace*{1em}
					
					\begin{subfigure}{\linewidth}
						\center
						\buildfig{figures/sa-swap-commit.tex}
						\caption{Vertex $v_a$ is swapped with vertices $v_b$ and $v_c$ if
						there is room. If there isn't enough room $v_a$, $v_b$ and $v_c$
						are returned to their original locations and the swap is aborted.}
						\label{fig:sa-swap-commit}
					\end{subfigure}
					
					\caption{Selecting vertices to swap.}
					\label{fig:sa-swap}
				\end{figure}
				
				Rather than blindly picking pairs of vertices to swap and abandoning
				those which don't fit, the process illustrated in
				figure~\ref{fig:sa-swap} is used. This swapping process reduces the
				likelihood that swaps involving larger vertices are aborted when the
				vertices on the target chip are individually too small to free up
				enough room to complete a swap.
			
			\subsubsection{Cost function}
				
				For each swap made by the algorithm, the change in the placement's
				`cost' must also be determined. In principle, the ideal `cost' function
				should accurately predict network congestion by performing routing and
				modelling the resulting network traffic. This approach was used by
				early application placement algorithms~\cite{steele85} but incurs a
				large performance overhead. Since the simulated annealing algorithm
				must re-evaluate the cost function before and after every swap, a cost
				function which is slow to evaluate can result in unacceptable execution
				time for the algorithm as a whole.
				
				A common alternative to measuring congestion directly used by many
				circuit placement algorithms is to approximate the network resources
				consumed. In the case of multicast nets, rather than computing
				efficient multicast routes, an estimate is made based on the relative
				locations of the vertices. Two popular techniques are described below
				\cite[\S4.2]{kahng11}.
				
				In the star model, the cost of a net is found by summing the distances
				from the net's source vertex to each sink vertex in turn as illustrated
				in figure~\ref{fig:cost-function-star}. In a hexagonal mesh and torus
				topology, the distance between two points can be cheaply determined (as
				described in chapter~\ref{sec:shortestPaths}) making this cost function
				inexpensive to compute. This model will over-estimate network resource
				usage for nets with more than two vertices since, in practice,
				multicast routing algorithms will tend to generate routes which share
				some hops.
				
				In the HPWL model, a bounding box is drawn around all the vertices
				connected by the net and the cost of the net is estimated as half of
				the perimeter of this bounding box. In a non-hexagonal torus or mesh
				topology, the HPWL model exactly matches the length of an optimal
				multicast route for nets with two or three vertices. In nets with a
				greater number of vertices, HPWL under-estimates the length of an
				optimal route by a factor of $\sqrt{n}$ for nets with $n$
				vertices~\cite{chung79}.
				
				The three non-orthogonal axes of hexagonal mesh and torus networks mean
				that there are several ways in which a bounding box may be defined.
				Specifically, the bounding box may extend along any pair of axes
				resulting in three possible bounding boxes and, thus, three HPWL values
				for any net as illustrated in~\ref{fig:cost-function-hpwl-xy},
				\ref{fig:cost-function-hpwl-xz} and~\ref{fig:cost-function-hpwl-yz}.
				The relative sizes of these three bounding boxes varies depending on
				specific arrangement of vertices in the net, meaning that no single
				bounding box is appropriate for all situations.
				
				A single HPWL-like metric for hexagonal toruses can be produced by
				taking the minimum of the half-perimeters of the three different
				bounding boxes defined for a hexagonal toruses. Like the HPWL function
				for non-hexagonal toruses, this `hexagonal HPWL' cost function exactly
				predicts the length of optimal routes for all nets with two or three
				vertices.
				
				\begin{figure}
					\center
					\begin{subfigure}[b]{\linewidth}
						\center
						\buildfig{figures/cost-function-star.tex}
						
						\caption{Star $= a_x + a_y + b_x + b_z + c_x + c_y + d_x + d_y$}
						\label{fig:cost-function-star}
					\end{subfigure}
					
					\vspace*{1.5em}
					
					\begin{subfigure}[b]{0.32\linewidth}
						\center
						\buildfig{figures/cost-function-hpwl-xy.tex}
						
						\caption{HPWL$_{xy}$ $= w_{xy} + h_{xy}$}
						\label{fig:cost-function-hpwl-xy}
					\end{subfigure}
					\begin{subfigure}[b]{0.32\linewidth}
						\center
						\buildfig{figures/cost-function-hpwl-xz.tex}
						
						\caption{HPWL$_{xz}$ $= w_{xz} + h_{xz}$}
						\label{fig:cost-function-hpwl-xz}
					\end{subfigure}
					\begin{subfigure}[b]{0.32\linewidth}
						\center
						\buildfig{figures/cost-function-hpwl-yz.tex}
						
						\caption{HPWL$_{yz}$ $= w_{yz} + h_{yz}$}
						\label{fig:cost-function-hpwl-yz}
					\end{subfigure}
					
					\caption[Net cost estimation functions for hexagonal toruses.]%
					{Net cost estimation functions for hexagonal toruses and meshes.}
					\label{fig:cost-function}
				\end{figure}
				
				To determine which cost function provides the most accurate results an
				experiment was performed. \num{50000} nets connecting between two and
				thirty uniform-randomly selected vertices were generated within a
				$30\times30$ hexagonal torus topology. The difference between the cost
				estimated by each function and the cost of a route generated by the NER
				routing algorithm is plotted figure~\ref{fig:cost-function-error}.
				
				\begin{figure}
					\center
					\buildrplot{figures/cost-function-error.R}
					
					\caption[Estimation error of various cost functions.]%
					{Estimation error of various cost functions compared with
					routing solutions. Positive and negative errors are equally bad.
					Y-axis limit clipped for clarity. The error of the star cost function
					grows linearly with fan out beyond the axis limit.}
					\label{fig:cost-function-error}
				\end{figure}
				
				These plots confirm that the error introduced is consistently
				over-estimated by the star model and under-estimated by both the
				hexagonal HPWL and HPWL${_{xy}}$ cost models. The error in cost
				estimates produced by the star model grew more quickly than both HPWL
				variants, Both hexagonal HPWL and HPWL$_{xy}$ perform similarly though,
				as expected, hexagonal HPWL is more accurate for low fan out nets.
				
				To attempt to compensate for the theoretical $\sqrt{n}$ error in the
				HPWL cost, the Hexagonal HPWL and HPWL$_{xy}$ scaled by $\sqrt{n}$ are
				also shown. While this correction does not eliminate the error of these
				functions, it does reduce it. This is probably because the error only
				tends towards $\sqrt{n}$ for very large $n$ \cite{chung79} and also
				because the error is being computed against a realistic, non optimal
				routing algorithm.
				
				On the basis of these results, the star cost function is rejected on
				the grounds that it produces the least accurate cost estimates. Though
				the $\sqrt{n}$-scaled Hexagonal HPWL cost function produces the
				smallest error in general, its execution time is three times greater
				than the $\sqrt{n}$-scaled HPWL$_{xy}$ whose accuracy is very similar.
				Since the execution speed of the cost function has a significant impact
				on the execution time of the simulated annealing algorithm, I have used
				the $\sqrt{n}$-scaled HPWL$_{xy}$ net cost function as a trade-off
				between accuracy and performance in this work.
				
			\subsubsection{Annealing schedule and acceptance function}
				
				\label{sec:placement-schedule-acceptance}
				
				The annealing schedule and acceptance function described below are
				taken directly from that used by the VPR FPGA placement
				software~\cite{betz97}. Due to a combination of time constraints and
				this schedule being known to work well, alternatives have not been
				considered in this thesis.
				
				Starting with an initial random (but valid) placement, $N$ swaps
				are performed with no distance limit to gather statistics about the
				problem, where $N$ is the number of vertices in the graph. The initial
				annealing temperature, $T$, is then set to 20 times the standard
				deviation of the cost changes produced by the $N$ swaps.
				
				The annealing algorithm now performs a round of $E \times N^{1.33}$
				random swaps where $E$ is a tunable parameter controlling `effort'. In
				all experiments reported in this thesis, $E$ is set to 1. Increasing
				$E$ roughly linearly increases execution time for diminishing returns
				in placement quality improvements.  Each random swap is accepted with a
				probability:
				%
				\begin{align*}
					P_{\textrm{accept}}=
						\begin{cases}
							1 & \textrm{if } \Delta{}C \le 0 \\
							e^{\nicefrac{-\Delta{}C}{T}} & \textrm{otherwise} \\
						\end{cases}
				\end{align*}
				%
				Where $\Delta{}C$ is the change in cost resulting from the swap and $e$
				is Euler's number. This probability is 1 for any beneficial or neutral
				change. Small cost increases are accepted with a greater probability
				than large cost increases. At higher temperatures, larger cost
				increases are accepted with a greater probability than at lower
				temperatures.
				
				After a round $E \times N^{1.33}$ of swaps has been performed, the
				temperature is updated according to $T_\textrm{new} = \alpha
				T_\textrm{old}$ where $\alpha$ is determined thus:
				%
				\begin{align*}
					\alpha=
						\begin{cases}
							0.50 & \textrm{if } R_\textrm{accept} > 0.96 \\
							0.90 & \textrm{if } 0.80 < R_\textrm{accept} \le 0.96 \\
							0.95 & \textrm{if } 0.15 < R_\textrm{accept} \le 0.80 \\
							0.80 & \textrm{if }        R_\textrm{accept} \le 0.15 \\
						\end{cases}
				\end{align*}
				%
				Where $R_\textrm{accept}$ is the proportion of swaps which were accepted
				by the algorithm in the previous round. This approach attempts to maximise the
				time spent at temperatures in which some but not all swaps are accepted
				since this is likely when the greatest true improvement in placement
				quality is occurring. The next round of annealing is then performed
				using the newly calculated temperature.
				
				The algorithm terminates when $T < \nicefrac{0.005 C}{N_\textrm{nets}}$
				where $C$ is the estimated current cost of the placement solution and
				$N_\textrm{nets}$ is the number of nets in the problem.
				
				Towards the end of placement the solution is likely to be close to the
				(ideally global) minimum cost. In already-good solutions it is likely
				that swapping vertices over long distances will result in large cost
				increases. As a result towards the end of the anneal, swaps are only
				considered over a limited distance, $D_\textrm{limit}$. By limiting the
				distance over which swaps occur, the proportion of swaps which are
				likely to be accepted is also increased. $D_\textrm{limit}$ is
				initially set such that all possible swaps are considered. It is then
				updated according to the following formula at the end of each round of
				swaps:
				%
				\[
					D_\textrm{limit}^\textrm{new} =
						\textrm{max}(1,\quad D_\textrm{limit}^\textrm{old}(1-0.44 + R_\textrm{accept}))
				\]
				%
				This rule attempts to limit swap distance such that the proportion of
				accepted swaps does not drop below 0.44 for as long as possible, i.e.
				such that some, but not all, swaps are accepted for as long as
				possible.
	
	\section{Evaluation}
		
		\label{sec:placement-results}
		
		A good placement application should produce placements which result in
		efficient use of network resources. Secondly, in SpiNNaker, routing table
		usage is an important concern since routing table entries are a finite
		resource. Finally, the execution time of the placement algorithm itself is
		relevant since, especially in applications where experiments are of short
		duration, placement execution time should not dominate application
		execution time.
		
		In this evaluation, the simulated annealing based placement algorithm
		introduced in this chapter is compared with a number of baseline algorithms
		representative of existing application placement techniques. I consider a
		combination of existing SpiNNaker applications and larger-scale synthetic
		benchmarks representative of possible future applications. I compare the
		performance of the placement algorithms under consideration using a
		combination of software models and confirm these results using experiments
		running on SpiNNaker hardware.
		
		\subsection{Baseline algorithms}
			
			To provide a benchmark to compare my simulated annealing based placement
			algorithm against, the following three placement algorithms are
			considered:
			
			\begin{description}
				
				\item[Hilbert] The sequential placement algorithm used by existing
				SpiNNaker software and similar to the algorithm provided by Cray's
				extensions to SLURM. This places vertices along a Hilbert curve path in
				breadth-first traversal order of the application communication graph.
				
				\item[RCM] The `graph similarity based mapping' algorithm described by
				Hoefler and Snir~\cite{hoefler11}. This assigns vertices and fills
				chips in Reverse Cuthill McKee order~\cite{cuthill69}.
				
				\item[Random] A random placement algorithm which places vertices
				uniform-randomly with no regard for connectivity. This algorithm serves
				as an experimental control.
				
			\end{description}
			
		\subsection{Existing applications}
			
			\label{sec:existing-applications}
			
			To verify the suitability of simulated annealing based placement for
			existing SpiNNaker software a representative selection of benchmark
			applications has been collected. The communication graphs for these
			benchmark applications are then used to compare the performance of each
			placement algorithm against a set of simple performance metrics. In
			addition, a subset of the applications were modified to use each of the
			routing algorithms under test and changes in their performance are
			reported.
			
			Table~\ref{tab:real-benchmarks} summarises each of the benchmarks. While
			these applications are much smaller than those expected to run on new,
			large-scale SpiNNaker systems, these benchmarks provide a meaningful
			indicator of the types of connectivity which might be present in future
			networks.
			
			\begin{table}
				\center
				\begin{tabular}{lrr p{1.45cm} p{0.4\linewidth}}
					\toprule
					Name & Vertices & Nets & Fan out & Notes \\
					\midrule
					Microcircuit   & \num{1338} & \num{760} & 94.50 (98) &
						Spiking neural model of a cortical microcircuit~\cite{potjans14}
						implemented in `PyNN SpiNNaker'~\cite{knight16}. \\
					\addlinespace
					Sudoku         & \num{299} & \num{109} & 15.86 (21) &
						Spiking neural model which solves Sudoku puzzles implemented in `PyNN
						SpiNNaker'~\cite{knight16}. \\
					\addlinespace
					Card-Sorting   & \num{469} & \num{919} & 1.06\newline (3) & 
						Spiking neural model which solves the Wisconsin card sorting
						test~\cite{aubin15} implemented in `Nengo
						SpiNNaker'~\cite{mundy15}. \\
					\addlinespace
					CConv          & \num{2560} & \num{12020} & 3.48 (17) &
						Spiking neural model which performs 512D circular
						convolution~\cite{eliasmith13} implemented in `Nengo
						SpiNNaker'~\cite{mundy15}. A key component in the Spaun neural
						model~\cite{eliasmith12}. \\
					\addlinespace
					Parse          & \num{855} & \num{3046} & 3.26 (64) &
						Spiking neural model which performs simple sentence parsing
						implemented in `Nengo SpiNNaker'~\cite{mundy15}. \\
					\addlinespace
					MU0            & \num{1084} & \num{1084} & 1.38\newline (3) &
						A digital circuit simulation of the data path of a simple computer
						processor~\cite{nutter16} implemented using the digital circuit
						simulator example program included with `Rig'~\cite{rig15}. \\
					\bottomrule
				\end{tabular}
				
				\caption[SpiNNaker application benchmarks.]%
				{SpiNNaker application benchmarks. `Fan out' refers to the fan
				out of the nets in the communication graph and not necessarily the
				total fan outs of individual vertices. Both the mean and (maxium) fan
				out are shown.}
				\label{tab:real-benchmarks}
			\end{table}
			
			\subsubsection{Communication graph placement performance}
			
				Each benchmark's connectivity graph was placed on into an idealised $13
				\times 13$ hexagonal torus topology and the routed using the NER
				algorithm. This process was repeated \num{1000} times with various
				performance metrics recorded which are analysed below. Experiments were
				conducted on a cluster of idle workstations with 3.10~GHz Intel
				Core-i5-2400 CPUs and 8~GB of RAM.
			
				\begin{figure}
					\center
					\begin{subfigure}{\linewidth}
						\center
						\buildrplot{figures/application-benchmarks-quality.R}
						
						\caption{Placement quality (routed net length reduction vs. Hilbert
						placer)}
						\label{fig:application-benchmarks-quality}
					\end{subfigure}
					
					\vspace*{1em}
					
					\begin{subfigure}{\linewidth}
						\center
						\buildrplot{figures/application-benchmarks-tables.R}
						
						\caption{Maximum routing table entries (red line at \num{1024} entries)}
						\label{fig:application-benchmarks-tables}
					\end{subfigure}
					
					\vspace*{1em}
					
					\begin{subfigure}{\linewidth}
						\center
						\buildrplot{figures/application-benchmarks-runtime.R}
						
						\caption{Placement algorithm execution time}
						\label{fig:application-benchmarks-runtime}
					\end{subfigure}
					
					\caption{Application benchmark placement performance.}
					\label{fig:application-benchmarks}
				\end{figure}
				
				To measure the `quality' of the placement solutions generated, the
				total number of hops taken by the generated routes is compared in
				figure~\ref{fig:application-benchmarks-quality}. A better placement
				solution should require fewer hops and, as an indirect result, achieve
				lower network congestion.  Since the hop counts vary greatly between
				benchmarks, these results are normalised against the number of hops
				produced by the Hilbert placer, the placement algorithm used by default
				in pre-existing SpiNNaker software.
				
				In the majority of benchmarks, the simulated annealing based placement
				algorithm achieves the greatest reduction in hop counts. The RCM and
				Random placement algorithms both consistently produce results similar
				or worse than those of the Hilbert placement algorithm.
				
				Unlike the other benchmarks, the `Microcircuit' benchmark notably shows
				little difference in placement quality amongst the four algorithms.
				This may be explained by the fact that this network is largely
				all-to-all connected meaning that a placement function can do little
				more than place the vertices tightly together to improve quality.
				
				The second metric considered is the number of routing table entries
				required in the worst case after placement and is shown in
				figure~\ref{fig:application-benchmarks-tables}. As is the practice of
				the applications modelled by these benchmarks, routing tables are
				compressed by the `Ordered Covering' algorithm~\cite{mundy16} and
				default-routing is used when possible.
				
				Though many of the benchmarks do not run out of routing tables when
				placed with any of the algorithms, the `CConv' network notably runs out
				of table entries when placed by all but the simulated annealing and
				random placement algorithms. This can be attributed to the large number
				of nets the `CConv' network uses by comparison with other models.
				
				The random placement algorithm can often achieve competitive routing
				table sizes because it tends to spread out vertices over the entire
				machine. Because longer routes don't always result in more routing
				entries being required (thanks to default routing), this also results
				in the routing entries being spread more thinly across the whole
				system. In these experiments the majority of benchmarks do not fill the
				$13\times13$ hexagonal torus topology supplied meaning that some
				networks can become spread out by the random placer. Though routing
				table usage drops, network utilisation, and potentially congestion,
				increases. This is because the number of routing entries required by a
				route is usually independent of length due to default routing.
				
				The third metric considered is the execution time of the placement
				algorithm, shown in figure~\ref{fig:application-benchmarks-runtime}.
				The simulated annealing algorithm requires much longer execution times
				than the baseline placement algorithms considered; in several cases an
				order of magnitude longer. This is unsurprising since the baseline
				algorithms are comparatively simplistic greedy algorithms. In all
				applications considered, however, the execution time of the simulated
				annealing placement algorithm is multiple orders of magnitude shorter
				than other processes involved in configuring and loading the simulation
				onto SpiNNaker.
				
				The `Microcircuit' benchmark is notable in that all of the placement
				algorithms take a long time to execute compared with the other
				benchmarks. This longer execution time can be attributed to a
				preprocessing step shared by every placement algorithm's implementation
				which handles `same chip' constraints. Unlike the other application
				graphs, the `Microcircuit' communication graph makes liberal use of
				these constraints and, due to the na\"ive implementation of the
				preprocessing step, all of the placement algorithms are slowed down by
				a constant amount.
			
			\subsubsection{Impact of placement on `PyNN SpiNNaker'}
				
				The `PyNN SpiNNaker' based neural models behind the `Microcircuit' and
				`Sudoku' benchmarks are compute bound and produce relatively small
				quantities of network traffic. Configuring `PyNN SpiNNaker' to use each
				of the four placement algorithms under consideration did not result in
				any changes in application performance.
				
				It is anticipated that future models built on PyNN SpiNNaker will grow
				in size and, unlike the Microcircuit model, not feature all-to-all
				connectivity and continue to benefit from good quality placement.
			
			\subsubsection{Impact of placement on `Nengo SpiNNaker'}
				
				The `Nengo SpiNNaker' based neural simulations behind the `CConv' and
				`Parse' benchmark were each run \num{100} times using each of the
				placement algorithms under test. For each run, the number of packets
				dropped by SpiNNaker during the application's execution was recorded
				and the results are presented in
				figure~\ref{fig:nengo-dropped-packets}. Unfortunately the `Card
				Sorting' neural model was not available for comparison in these
				experiments.
				
				\begin{figure}
					\center
					\begin{subfigure}{0.45\linewidth}
						\buildrplot{figures/cconv-dropped-packets.R}
						
						\caption{CConv}
						\label{fig:cconv-dropped-packets}
					\end{subfigure}
					\begin{subfigure}{0.45\linewidth}
						\buildrplot{figures/parse-dropped-packets.R}
						
						\caption{Parse}
						\label{fig:parse-dropped-packets}
					\end{subfigure}
					
					\caption[Packet dropping in `Nengo SpiNNaker' simulations]%
					{Rates of packet dropping in `Nengo SpiNNaker' simulations
					with different placements. Results shown jittered in the `X'
					direction for clarity.}
					\label{fig:nengo-dropped-packets}
				\end{figure}
				
				In the `CConv' neural model, when placed using the simulated annealing
				algorithm, 70\% of simulations resulted in fewer than 100 packets being
				dropped per second and 41\% of runs dropping no packets at all.  In 95
				out of the 100 runs SpiNNaker's routing tables were not exhausted. In
				the `Parse' benchmark, when placed by simulated annealing, 93\% of
				simulations did not drop any packets and all 100 simulation runs did
				not overflow SpiNNaker's routing tables.
				
				In both neural models, placement by the baseline placement algorithms
				result in fatal quantities of packets being dropped in most runs
				rendering the baseline placement algorithms unsuitable for use with
				these models. In addition, for the `CConv' model only 58\%, 7\% and 9\%
				of placement solutions resulted in routes which fit in SpiNNaker's
				routing tables, for the Hilbert, RCM and Random placers respectively.
				
				In these experiments, the simulated annealing algorithm reliably
				produces the best quality placements, often making the difference
				between simulations working or not. Unfortunately the results show that
				the algorithm will, on rare occasions, produce unusable placements in
				terms of either network congestion or routing table use. A possible
				explanation for this behaviour is explored in
				\S\ref{sec:wiggly-board-allocations} and possible solutions discussed.
				The experiments performed make use of irregularly-shaped hexagonal mesh
				topologies cut from larger SpiNNaker machines. The irregularities in
				the edges of the topology are hypothesised to be producing the same
				negative effect on route quality as high rates of HSS link faults
				explored in \S\ref{sec:routing-evaluation}. The placement algorithm
				occasionally places some parts of the network around obstructions at
				the edges the network which may explain these infrequent failures.
				
			\subsubsection{Impact of placement on the `Circuit Simulator'}
				
				The `MU0' CPU simulation built using the `circuit simulator' SpiNNaker
				application produces very little traffic in its default mode of
				operation and functions correctly when placed with all four placement
				algorithms.
				
				\begin{figure}
					\center
					\buildrplot{figures/mu0-saturation.R}
					
					\caption[Packet dropping in `circuit simulator'.]%
					{Simulation speeds at which the circuit simulator begins to
					drop packets.}
					\label{fig:mu0-saturation}
				\end{figure}
				
				The circuit simulator may be configured to run at higher speeds,
				directly increasing the density of network traffic generated. To judge
				the relative quality of all four placement algorithms the simulation
				was executed at a range of different speeds. By determining when or if
				the network becomes saturated, the relative placement quality may be
				compared.
				
				When placed by the simulated annealing algorithm, at 400$\times$ the
				default simulation speed, the simulator becomes CPU bound but
				SpiNNaker's network does not become saturated and drop packets. By
				contrast, when placed by the baseline algorithms, the application
				saturates the network and drops packets before becoming CPU bound.
				Figure~\ref{fig:mu0-saturation} shows the distribution of simulation
				speeds achieved for 100 runs of the simulator placed by each baseline
				placer. As in many of the other benchmarks, the Hilbert placer performs
				better than the Random and RCM placers with the RCM placer producing
				the placements which induced the greatest congestion.
			
		\subsection{Scalability}
			
			Unfortunately, due to the limited size of existing SpiNNaker applications
			it is necessary to turn to a synthetic benchmark to demonstrate the
			scaling properties of the simulated annealing placement algorithm.
			
			A synthetic application graph is considered consisting of a 2D grid of
			vertices in which each vertex is connected with its neighbours according
			to a 2D Gaussian probability distribution. This application graph is then
			placed into a hexagonal mesh topology whose size and aspect ratio matches
			the application graph. This placement problem may be scaled up by
			increasing the number of vertices or by increasing the number of
			connections made by each vertex. The graph also has a natural manual
			placement solution which is used as a gold standard in these experiments.
			
			Placement quality is judged by comparing the total number of hops
			required to route all nets in the placed application graph against the
			number of hops needed when the vertices are placed `manually' using their
			natural placement. Better placements should require a similar number of
			hops to the manual placement while worse placements will require more,
			implying greater network congestion. Worst-case routing table usage is
			calculated based on the use of routes which exploit default routing.
			
			For each combination of placement algorithm and graph size, the
			experiment was repeated 20 times and the aggregated results are presented
			below. All experiments were conducted on a cluster of idle workstations
			with 3.10~GHz Intel Core-i5-2400 CPUs and 8~GB of RAM.
			
			\subsubsection{Graph size}
				
				\begin{figure}
					\center
					\begin{subfigure}{\linewidth}
						\center
						\buildrplot{figures/placement-scalability-size-quality.R}
						
						\caption{Placement overhead (vs. manual placement)}
						\label{fig:placement-scalability-size-quality}
					\end{subfigure}
					
					\vspace*{1em}
					
					\begin{subfigure}{\linewidth}
						\center
						\buildrplot{figures/placement-scalability-size-entries.R}
						
						\caption{Maximum routing table size}
						\label{fig:placement-scalability-size-entries}
					\end{subfigure}
					
					\vspace*{1em}
					
					\begin{subfigure}{\linewidth}
						\center
						\buildrplot{figures/placement-scalability-size-runtime.R}
						
						\caption{Placement algorithm execution time}
						\label{fig:placement-scalability-size-runtime}
					\end{subfigure}
					
					\caption[Placer scalability with respect to graph size.]%
					{Scalability with respect to graph size. Lines give mean
					values, bands give range of results. Some results are absent due to
					out-of-memory errors during routing following placement of some
					larger graphs.}
					\label{fig:placement-scalability-size}
				\end{figure}
				
				This experiment intends to determine how the execution time and
				placement quality of each algorithm changes as the size of the
				application graph is increased. In this experiment, each vertex is
				connected to four randomly picked neighbours whose distribution of
				distances have a standard-deviation of three hops. The results are
				shown in figure~\ref{fig:placement-scalability-size}.
				
				In figure~\ref{fig:placement-scalability-size-quality} we can see that,
				as the placement problem size grows, existing placement techniques
				quickly begin to produce solutions requiring many more hops to route
				than a `manual' solution. While the quality of the simulated annealing
				approach falls for larger graphs, at \num{1048576} vertices,
				approximately the number in the largest planned SpiNNaker machine, only
				twice as many hops are required to route the placement compared with
				the ideal manual placement.
				
				Beyond around a quarter of a million vertices, the quality of the
				placements produced by the baseline placement algorithms has dropped
				significantly. Due to the increased length of the routes required,
				routing these placed graphs requires more memory than was available in
				the cluster machines and so no results are reported.
				
				Figure~\ref{fig:placement-scalability-size-runtime} shows the execution
				time of the routing algorithms for each problem size. Once again the
				simulated annealing algorithm's execution time consistently dwarfs the
				other algorithms but grows approximately linearly with the problem
				size. For the largest graph in the experiment, the execution time of
				the annealing based placer reaches approximately 12~hours. While such
				long execution times may not be ideal, for large placement problems,
				simulated annealing produces placements with substantially reduced
				overhead.
			
			\subsubsection{Fan out}
			
				\begin{figure}
					\center
					\begin{subfigure}{\linewidth}
						\center
						\buildrplot{figures/placement-scalability-fanout-quality.R}
						
						\caption{Placement overhead (vs. manual placement)}
						\label{fig:placement-scalability-fanout-quality}
					\end{subfigure}
					
					\vspace*{1em}
					
					\begin{subfigure}{\linewidth}
						\center
						\buildrplot{figures/placement-scalability-fanout-entries.R}
						
						\caption{Maximum routing table size}
						\label{fig:placement-scalability-fanout-entries}
					\end{subfigure}
					
					\vspace*{1em}
					
					\begin{subfigure}{\linewidth}
						\center
						\buildrplot{figures/placement-scalability-fanout-runtime.R}
						
						\caption{Placement algorithm execution time}
						\label{fig:placement-scalability-fanout-runtime}
					\end{subfigure}
					
						\caption[Placer scalability with respect to fan out.]%
						{Scalability with respect to fan out. Lines give mean values,
						bands give range of results.}
					\label{fig:placement-scalability-fanout}
				\end{figure}
				
				Another factor which can have a significant impact on placement
				execution time is the fan out of the nets in an application graph. A
				second experiment was conducted to determine the effects of increasing
				the fan out of the synthetic application graph. The number of vertices
				in the graph was fixed at \num{16384} and the fan out of the nets
				connecting these vertices increased. Again, each combination of
				placement algorithm and fan out was tested twenty times and the results
				are shown in figure~\ref{fig:placement-scalability-fanout}.
				
				As shown in figure~\ref{fig:placement-scalability-fanout-quality}, the
				quality of the placements produced does not change dramatically as the
				fan out is increased with the exception of the Hilbert placer which
				appears to perform better for very small and very large fan outs. In
				all cases however, simulated annealing maintains its lead in placement
				quality.
				
				In all but the random placer (which does not consider connectivity),
				the placement algorithms' execution times grow in proportion to the fan
				out (figure~\ref{fig:placement-scalability-fanout-quality}). Once
				again, simulated annealing is significantly slower than the other
				algorithms but scales approximately linearly with problem size.
	
	\section{Conclusions}
		
		In this chapter I re-evaluated the suitability of simulated annealing for
		application placement in super computers. In the field of chip placement,
		simulated annealing has proven highly successful for placement problems
		similar in scale to application placement problems in modern super
		computers. Following this observation I developed a simulated annealing
		based application placement algorithm for SpiNNaker's hexagonal torus
		topology based on techniques used by circuit placement algorithms.
		
		Existing applications containing thousands of vertices have demonstrated
		improved performance when placed by simulated annealing. Included in these
		benchmarks are neural simulations built with `Nengo SpiNNaker' which do not
		function at all when placed by contemporary placement algorithms but run
		reliably when placed by simulated annealing.
		
		Synthetic benchmarks have demonstrated that the proof-of-concept simulated
		annealing based placement algorithm developed as part of this work is able
		to maintain good placement quality in networks containing over one million
		vertices. Though execution times are considerably longer than existing
		application placement algorithms, taking over twelve hours for one million
		vertex networks, at these scales simulated annealing is the only placement
		algorithm able to deliver results within an order of magnitude of an ideal
		placement solution.
		
		Of the baseline placement algorithms considered, the Hilbert placer
		consistently outperforms the other baseline experiments, followed by the
		RCM algorithm which often performed similarly. It is worth highlighting
		that the Hilbert algorithm exploits knowledge of SpiNNaker's network while
		the RCM algorithm is topology agnostic, possibly explaining the discrepancy
		in performance. The random placement algorithm consistently produced the
		worst placement quality confirming that even the simple baseline algorithms
		considered generate some benefit.
		
		These results support the notion that circuit placement techniques may
		provide valuable insights for the development of future application
		placement techniques. As application placement problems continue to grow,
		adapting other placement techniques developed for circuit placement may
		become more challenging. A key difference between the circuit and
		application placement problems is that, while super computer network
		topologies come in many shapes and forms, chips are largely 2D.  Because of
		this modern circuit placement techniques increasingly exploit geometric
		properties of the circuit placement problem which may be more difficult to
		adapt to super computer network topologies.  However, at their current rate
		of growth, placement problems in super computers are likely to remain
		tractable to simulated annealing based placement techniques for a number of
		years to come. 
